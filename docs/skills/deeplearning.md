# Deep learning

[Python Tutorial](https://www.w3schools.com/python/default.asp)

[ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ _é‚±é”¡é¹è‘—_2020å¹´.pdf](https://www.yuque.com/attachments/yuque/0/2024/pdf/34417153/1709725036974-bb82fbd2-62ff-4d31-b38e-890c326c92c8.pdf)<br />[Pytorchå¸¸ç”¨APIæ±‡æ€»(æŒç»­æ›´æ–°)_pytorchçš„api-CSDNåšå®¢](https://blog.csdn.net/qq_49134563/article/details/108200828)<br />[PyTorch | å¹¿æ’­æœºåˆ¶ï¼ˆbroadcastï¼‰_pytorch broadcast-CSDNåšå®¢](https://blog.csdn.net/m0_52650517/article/details/119913625)
<a name="Ap48w"></a>
## Deep learning
data<br />ä¸‰è¦ç´ ï¼šæ¨¡å‹ modelã€å­¦ä¹ å‡†åˆ™ criteriaã€ä¼˜åŒ–ç®—æ³•
<a name="tFbBb"></a>
### æ¨¡å‹
æ¨¡å‹$f(x;Î¸)$<br />æœºå™¨å­¦ä¹ çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªæ¨¡å‹æ¥è¿‘ä¼¼çœŸæ˜¯æ˜ å°„å‡½æ•°$g(x)$æˆ–çœŸæ˜¯æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ$p_r(y|x)$<br />éçº¿æ€§æ¨¡å‹<br />KL æ•£åº¦ã€äº¤å‰ç†µ
<a name="I5ZUE"></a>
### å­¦ä¹ å‡†åˆ™
æœŸæœ›é£é™©<br />æŸå¤±å‡½æ•°<br />å¹³æ–¹ã€äº¤å‰ç†µã€hinge<br />ç»éªŒé£é™©æœ€å°åŒ– ERM<br />ç»“æ„é£é™©æœ€å°åŒ– SRMï¼šæ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
<a name="JNx6f"></a>
### ä¼˜åŒ–ç®—æ³•
ï¼ˆè¶…ï¼‰å‚æ•°ä¼˜åŒ–<br />ï¼ˆæ‰¹é‡ï¼‰æ¢¯åº¦ä¸‹é™æ³• BGDï¼šæ¯æ¬¡è¿­ä»£æ—¶è®¡ç®—æ¯ä¸ªæ ·æœ¬æŸå¤±å‡½æ•°çš„æ¢¯åº¦å¹¶æ±‚å’Œ<br />æå‰åœæ­¢ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ<br />éšæœºï¼ˆå¢é‡ï¼‰æ¢¯åº¦ä¸‹é™æ³• SGDï¼šæŠ½ N ä¸ªæ ·æœ¬ï¼Œç”±å®ƒä»¬è®¡ç®—å‡ºæ¥çš„ç»éªŒé£é™©çš„æ¢¯åº¦æ¥è¿‘ä¼¼æœŸæœ›é£é™©çš„æ¢¯åº¦<br />å°æ‰¹é‡æ¢¯åº¦ä¸‹é™æ³•ï¼šä»‹äºæ¢¯åº¦ä¸‹é™æ³•å’Œéšæœºæ¢¯åº¦ä¸‹é™æ³•ä¹‹é—´

<a name="PNMXd"></a>
### ç®—æ³•
å‰å‘ä¼ æ’­ï¼šè¾“å…¥è¿›å» ç½‘ç»œåšäº†ä»€ä¹ˆè®¡ç®—
![](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709724929106-958fec79-3810-4abc-8ea5-7d799905dafe.png)
<a name="FEcw3"></a>
#### å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°


1. **Sigmoid**ï¼Œä¸¤ç«¯é¥±å’Œå‡½æ•°
   1. **Logistic** <img src="https://cdn.nlark.com/yuque/0/2024/png/34417153/1709725299105-4c06743a-67ff-4933-8295-213c27a6ef32.png#averageHue=%23fcfbfa&clientId=u66705bcf-f251-4&from=paste&height=45&id=uf102fe18&originHeight=67&originWidth=222&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=7501&status=done&style=none&taskId=u6f8926d0-699d-4a35-898f-00eba4cd52d&title=&width=148" alt="Alt text">

   2. **Tanh** ![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709725314557-8451b1a3-64ae-4126-b6ee-c57d37eae75c.png#averageHue=%23dcd3ca&clientId=u66705bcf-f251-4&from=paste&height=48&id=u4360faec&originHeight=72&originWidth=292&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=16035&status=done&style=none&taskId=ud3b1fb52-4edc-4946-b705-f4af05fed80&title=&width=194.66666666666666)![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709725383719-55775481-905c-4583-a225-8823a6827840.png#averageHue=%23f8f7f5&clientId=u66705bcf-f251-4&from=paste&height=27&id=u6cacfb91&originHeight=40&originWidth=222&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=7148&status=done&style=none&taskId=ucd715632-e4ac-4f17-ba45-a382eb0df47&title=&width=148)
é¥±å’Œå®šä¹‰å¯¹äºå‡½æ•° ğ‘“(ğ‘¥)ï¼Œè‹¥ ğ‘¥ â†’ âˆ’âˆ æ—¶ï¼Œå…¶å¯¼æ•° ğ‘“â€²(ğ‘¥) â†’ 0ï¼Œåˆ™ç§°å…¶ä¸ºå·¦é¥±å’Œï¼è‹¥ ğ‘¥ â†’ +âˆ æ—¶ï¼Œå…¶å¯¼æ•° ğ‘“â€²(ğ‘¥) â†’ 0ï¼Œåˆ™ç§°å…¶ä¸ºå³é¥±å’Œï¼å½“åŒæ—¶æ»¡è¶³å·¦ã€å³é¥±å’Œæ—¶ï¼Œå°±ç§°ä¸ºä¸¤ç«¯é¥±å’Œ

1. Hard-Logistic & hard-Tanh ä¸€é˜¶æ³°å‹’å±•å¼€çš„ç›´çº¿è¿‘ä¼¼
2. **ReLU**(PReLU)

![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709725612997-13ad4348-a7c1-45ae-b72e-f13cbb5ae07e.png#averageHue=%23fbfaf9&clientId=u66705bcf-f251-4&from=paste&height=59&id=u8c2f6cff&originHeight=88&originWidth=237&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=10381&status=done&style=none&taskId=u6b09a76c-ec08-436b-a3cd-adb470da8b3&title=&width=158)![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709815006287-233d495c-ffe3-4c14-a9e2-7c101d7239e0.png#averageHue=%23fbfbfa&clientId=u84e4090c-43d5-4&from=paste&height=92&id=u39688cc8&originHeight=138&originWidth=381&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=23005&status=done&style=none&taskId=uce4efad2-7d34-483a-9c66-bbbfad8d4a1&title=&width=254)<br />å…¶ä¸­ ğ›¾ğ‘– ä¸º ğ‘¥ â‰¤ 0 æ—¶å‡½æ•°çš„æ–œç‡ï¼å› æ­¤ï¼ŒPReLU æ˜¯éé¥±å’Œå‡½æ•°ï¼å¦‚æœ ğ›¾ğ‘– = 0ï¼Œé‚£ä¹ˆPReLU å°±é€€åŒ–ä¸º ReLU

4. **ELU**

![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709815094072-4b37f2f5-6453-4da4-acf4-188bfc33f1b9.png#averageHue=%23fbfafa&clientId=u84e4090c-43d5-4&from=paste&height=91&id=u7148bb36&originHeight=136&originWidth=441&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=29726&status=done&style=none&taskId=u40b939fb-3b26-4e61-99bd-dcdcd3460a3&title=&width=294)

5. **Softplus**

![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709815132024-7f02bb15-94eb-48c2-ad24-cc57bfe537ea.png#averageHue=%23ddd6ce&clientId=u84e4090c-43d5-4&from=paste&height=23&id=uf27f199f&originHeight=34&originWidth=306&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=11347&status=done&style=none&taskId=uddcab670-6e76-4666-8f38-b8807f17fd8&title=&width=204)<br />Softplus å‡½æ•°å…¶å¯¼æ•°åˆšå¥½æ˜¯ Logistic å‡½æ•°ï¼Softplus å‡½æ•°è™½ç„¶ä¹Ÿå…·æœ‰å•ä¾§æŠ‘åˆ¶ã€å®½å…´å¥‹è¾¹ç•Œçš„ç‰¹æ€§ï¼Œå´æ²¡æœ‰ç¨€ç–æ¿€æ´»æ€§

6. **Swish**

![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709815228781-0b3008bd-cee3-48d9-a3e8-8a81f47337d4.png#averageHue=%23f9f8f7&clientId=u84e4090c-43d5-4&from=paste&height=133&id=u0ddc9319&originHeight=199&originWidth=769&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=102308&status=done&style=none&taskId=uc515cbfc-80f0-42d8-a624-3e4b4e22bff&title=&width=512.6666666666666)

7. **GELU**

![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709815285967-80ed2bf3-8b53-4f0f-b19a-2f96562e3a04.png#averageHue=%23dfdad2&clientId=u84e4090c-43d5-4&from=paste&height=24&id=ud352bb2c&originHeight=36&originWidth=240&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=8528&status=done&style=none&taskId=udf1b892e-5ac4-4917-b35f-699b65c4c86&title=&width=160)![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709815302136-66c55af4-61bc-48be-b58a-d92a63e77aa7.png#averageHue=%23ded6ce&clientId=u84e4090c-43d5-4&from=paste&height=79&id=u7282e022&originHeight=118&originWidth=777&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=80124&status=done&style=none&taskId=u2af67573-5093-4a9b-a65f-f9548d968a2&title=&width=518)<br />![image.png](https://cdn.nlark.com/yuque/0/2024/png/34417153/1709815329086-b72081ea-416c-4fda-adb4-73d1db78e8df.png#averageHue=%23fbfaf9&clientId=u84e4090c-43d5-4&from=paste&height=70&id=u4136e230&originHeight=105&originWidth=579&originalType=binary&ratio=1.5&rotation=0&showTitle=false&size=30243&status=done&style=none&taskId=u86831f01-b63a-4e76-92a3-b7952b3ad74&title=&width=386)
<a name="njnWz"></a>
#### MLPï¼ˆFNNï¼‰
å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLP ï¼‰æ˜¯ç°ä»£å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰ çš„åç§°ï¼Œç”±å…·æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°çš„**å®Œå…¨è¿æ¥**çš„ç¥ç»å…ƒç»„æˆï¼ŒåŒ…å«è‡³å°‘ä¸‰å±‚èŠ‚ç‚¹ï¼šè¾“å…¥å±‚ã€ä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚å’Œè¾“å‡ºå±‚ã€‚ ä¸€å±‚ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æˆ–ç¥ç»å…ƒä»¥ä¸€å®šçš„æƒé‡è¿æ¥åˆ°ä¸‹ä¸€å±‚ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ï¼Œä½¿ç½‘ç»œå®Œå…¨è¿æ¥ã€‚ MLP ä½¿ç”¨ç§°ä¸º**åå‘ä¼ æ’­çš„ç›‘ç£å­¦ä¹ **æŠ€æœ¯è¿›è¡Œè®­ç»ƒã€‚ ç¥ç»ç½‘ç»œä¸­çš„èŠ‚ç‚¹å°†**éçº¿æ€§æ¿€æ´»å‡½æ•°**åº”ç”¨äºä»å‰ä¸€å±‚æ¥æ”¶çš„åŠ æƒè¾“å…¥ï¼Œç„¶åå°†ç»“æœä¼ é€’åˆ°ä¸‹ä¸€å±‚ã€‚ è¿™ç§éçº¿æ€§ä½¿å¾— MLP èƒ½å¤Ÿå¯¹è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„å¤æ‚å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼Œè€Œçº¿æ€§æ¨¡å‹æ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚ <br />MLP å¯ç”¨äºä»è®¡ç®—æœºè§†è§‰åˆ°è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸçš„å„ç§ä»»åŠ¡ï¼Œä¾‹å¦‚åˆ†ç±»ã€å›å½’å’Œç‰¹å¾å­¦ä¹ ã€‚ MLP çš„å…³é”®ç‰¹å¾åŒ…æ‹¬å…¶æ·±åº¦ï¼ˆå±‚æ•°ï¼‰ã€å®½åº¦ï¼ˆæ¯å±‚ä¸­çš„èŠ‚ç‚¹æ•°ï¼‰ã€æ¿€æ´»å‡½æ•°ï¼ˆä¾‹å¦‚ sigmoidã€tanh æˆ– ReLUï¼‰ä»¥åŠç”¨äºè®­ç»ƒçš„ä¼˜åŒ–ç®—æ³•ï¼ˆé€šå¸¸æ˜¯æŸç§å½¢å¼çš„æ¢¯åº¦ä¸‹é™ï¼‰ã€‚ MLP è¢«è®¤ä¸ºæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¶æ„ï¼Œå°½ç®¡å·ç§¯ç¥ç»ç½‘ç»œ (CNN) å’Œå¾ªç¯ç¥ç»ç½‘ç»œ (RNN) ç­‰æ›´å¤æ‚çš„ç½‘ç»œåˆ†åˆ«æ›´å¸¸ç”¨äºæ¶‰åŠå›¾åƒå’Œåºåˆ—æ•°æ®çš„ä»»åŠ¡ã€‚
<a name="t62Qq"></a>
#### CNN
æ»¤æ³¢å™¨ Filterï¼Œåˆå«å·ç§¯æ ¸ Kernal<br />å·ç§¯ç¥ç»ç½‘ç»œæ˜¯å¤šå±‚æ„ŸçŸ¥å™¨çš„å˜ä½“<br />CNN çš„å±‚å…·æœ‰æŒ‰3 ä¸ªç»´åº¦æ’åˆ—çš„ç¥ç»å…ƒï¼šå®½åº¦ã€é«˜åº¦å’Œæ·±åº¦ã€‚[71]å·ç§¯å±‚å†…çš„æ¯ä¸ªç¥ç»å…ƒä»…ä¸å…¶ä¹‹å‰å±‚çš„ä¸€å°éƒ¨åˆ†åŒºåŸŸç›¸è¿ï¼Œç§°ä¸ºæ„Ÿå—é‡ã€‚ä¸åŒç±»å‹çš„å±‚ï¼ˆå±€éƒ¨è¿æ¥å’Œå®Œå…¨è¿æ¥ï¼‰å †å åœ¨ä¸€èµ·å½¢æˆ CNN æ¶æ„ã€‚<br />å±€éƒ¨è¿æ¥ï¼šéµå¾ªæ„Ÿå—é‡çš„æ¦‚å¿µï¼ŒCNN é€šè¿‡åœ¨ç›¸é‚»å±‚çš„ç¥ç»å…ƒä¹‹é—´å¼ºåˆ¶æ‰§è¡Œ**å±€éƒ¨è¿æ¥**æ¨¡å¼æ¥åˆ©ç”¨ç©ºé—´å±€éƒ¨æ€§ã€‚å› æ­¤ï¼Œè¯¥æ¶æ„ç¡®ä¿å­¦ä¹ åˆ°çš„â€œè¿‡æ»¤å™¨â€å¯¹ç©ºé—´å±€éƒ¨è¾“å…¥æ¨¡å¼äº§ç”Ÿæœ€å¼ºçš„å“åº”ã€‚å †å è®¸å¤šè¿™æ ·çš„å±‚ä¼šå¯¼è‡´éçº¿æ€§æ»¤æ³¢å™¨å˜å¾—è¶Šæ¥è¶Šå…¨å±€ï¼ˆå³å“åº”åƒç´ ç©ºé—´çš„æ›´å¤§åŒºåŸŸï¼‰ï¼Œä»¥ä¾¿ç½‘ç»œé¦–å…ˆåˆ›å»ºè¾“å…¥çš„å°éƒ¨åˆ†çš„è¡¨ç¤ºï¼Œç„¶åä»å®ƒä»¬ç»„è£…æ›´å¤§åŒºåŸŸçš„è¡¨ç¤ºã€‚<br />å…±äº«æƒé‡ï¼šåœ¨ CNN ä¸­ï¼Œæ¯ä¸ªè¿‡æ»¤å™¨éƒ½ä¼šåœ¨æ•´ä¸ªè§†é‡ä¸­å¤åˆ¶ã€‚è¿™äº›å¤åˆ¶çš„å•å…ƒå…±äº«ç›¸åŒçš„å‚æ•°åŒ–ï¼ˆæƒé‡å‘é‡å’Œåå·®ï¼‰å¹¶å½¢æˆç‰¹å¾å›¾ã€‚è¿™æ„å‘³ç€ç»™å®šå·ç§¯å±‚ä¸­çš„æ‰€æœ‰ç¥ç»å…ƒåœ¨å…¶ç‰¹å®šå“åº”åœºå†…å“åº”ç›¸åŒçš„ç‰¹å¾ã€‚ä»¥è¿™ç§æ–¹å¼å¤åˆ¶å•å…ƒå…è®¸æ‰€å¾—åˆ°çš„æ¿€æ´»å›¾åœ¨è§†é‡ä¸­è¾“å…¥ç‰¹å¾çš„ä½ç½®ç§»åŠ¨çš„æƒ…å†µä¸‹æ˜¯ç­‰å˜çš„ï¼Œå³å®ƒä»¬æˆäºˆå¹³ç§»ç­‰å˜æ€§â€”â€”å‡è®¾è¯¥å±‚çš„æ­¥å¹…ä¸ºä¸€ã€‚[72]<br />æ± åŒ–ï¼šåœ¨ CNN çš„æ± åŒ–å±‚ä¸­ï¼Œç‰¹å¾å›¾è¢«åˆ’åˆ†ä¸ºçŸ©å½¢å­åŒºåŸŸï¼Œæ¯ä¸ªçŸ©å½¢ä¸­çš„ç‰¹å¾è¢«ç‹¬ç«‹ä¸‹é‡‡æ ·ä¸ºå•ä¸ªå€¼ï¼Œé€šå¸¸é‡‡ç”¨å¹³å‡å€¼æˆ–æœ€å¤§å€¼ã€‚é™¤äº†å‡å°ç‰¹å¾å›¾çš„å¤§å°ä¹‹å¤–ï¼Œæ± åŒ–æ“ä½œè¿˜ä¸ºå…¶ä¸­åŒ…å«çš„ç‰¹å¾èµ‹äºˆä¸€å®šç¨‹åº¦çš„å±€éƒ¨å¹³ç§»ä¸å˜æ€§ï¼Œä»è€Œä½¿ CNN å¯¹äºå…¶ä½ç½®çš„å˜åŒ–æ›´åŠ  ROBUSTã€‚


## Classification

**Example**
```python  title="MNIST" linenums="1"
import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
from tqdm import tqdm  # visual bar
import matplotlib.pyplot as plt

class Mymodel(nn.Module):
    def __init__(self, in_channels, classnumber):
        super(Mymodel, self).__init__()
        self.mymodel = nn.Sequential(
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=32,
                kernel_size=3,
                # stride=1,
                padding=1,
            ),
            nn.BatchNorm2d(32),  # æ­£åˆ™åŒ–
            nn.ReLU(),
            nn.MaxPool2d(2,2),  # 28/2=14
            nn.Conv2d(
                in_channels=32,
                out_channels=32,
                kernel_size=3,
                # stride=1,
                # padding=0,
            ),
            # 14-2=12
            nn.BatchNorm2d(32),  # æ­£åˆ™åŒ–
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 12/2=6
            nn.Conv2d(
                in_channels=32,
                out_channels=32,
                kernel_size=3,
                # stride=1,
                # padding=0
            ),
            # 6-2=4
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),  # 4/2=2
        )

        self.filter = nn.Sequential(
            nn.Linear(32*2*2, 64),
            nn.ReLU(),
            nn.Linear(64, classnumber)
        )
    def forward(self,inputs):
        inputs = self.mymodel(inputs)
        inputs = inputs.view(-1, 32*2*2)
        inputs = self.classifier(inputs)
        return inputs


if __name__ == '__main__':
    train_dataset = MNIST(root="D:\File\CODE\python\dataset")
    train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle= True)

    test_dataset = MNIST(root= "D:\File\CODE\python\dataset", train=False, transform=ToTensor())
    test_dataloader = DataLoader(dataset=test_dataset, batch_size=10)

    device = "cuda"
    model = Mymodel(in_channels=1, classnumber=10).to(device)

    epochs = 100
    optimizer = optim.Adam(model.parameters()) # lr=1e-3 by default
    loss_fn = nn.CrossEntropyLoss()  # äº¤å‰ç†µ

    for i in range(epochs):
        model.train()
        train_data = tqdm(train_dataloader)

        mean_loss, acc = 0.0, 0.0  # æŸå¤±ã€ç²¾ç¡®åº¦
        data_num = 0

        for x,y in train_data:  # x is input, y is label or target
            x = x.to(device)  # Move input and target to GPU if available
            y = y.to(device)

            predicts = model(x)  # å…ˆé¢„æµ‹
            # predicts: This is the output of the model for the current batch of inputs x.
            # [batch_size, n_classes], where n_classes is the number of classes in the classification problem.
            loss = loss_fn(predicts, y)  # å†ç®—æŸå¤±
            optimizer.zero_grad() # ä¼˜åŒ–å™¨æ¢¯åº¦æ¸…é›¶
            loss.backward()  # åå‘ä¼ æ’­
            optimizer.step()  # æ›´æ–°æ¨¡å‹å‚æ•°

            acc += torch.sum(torch.argmax(predicts,dim=1) == y).item()
            # torch.argmax(predicts, dim=1): This function finds the indices (i.e., the classes)
            # of the maximum values along dimension 1 (the class dimension) of predicts.
            # This effectively performs a prediction by selecting the class with the highest score for each input in the batch.
            # The output shape matches the batch size, containing the predicted class for each input.

            # torch.argmax(predicts, dim=1) == y: This compares the predicted classes to the true labels y.
            # If a prediction matches the true label, the comparison returns True; otherwise, it returns False.
            # this comparison effectively counts the correct predictions by returning a tensor of 1s and 0s.

            # torch.sum(...): This function sums up the values in the tensor of 1s (correct predictions) and 0s (incorrect predictions),
            # giving the total number of correct predictions in the batch.

            # item(): This method converts a PyTorch scalar tensor (a tensor with a single value) to a Python number.
            # It's used here to extract the number of correct predictions as a Python integer or float, which can then be accumulated in the acc variable.

            # acc += ...: This accumulates the number of correct predictions over all batches processed in the epoch.
            # By adding up the number of correct predictions after processing each batch, you keep a running total of how many samples have been correctly classified so far during the epoch.
            mean_loss /= loss.item() * x.size(0)
            data_num += x.size(0)
            train_data.set_description(f"Training..Epoach:{i+1}/{epochs},Loss:{loss.item(* x.size(0)):.4f}")

        mean_loss /=data_num
        acc /= data_num
        print(f"Training acc:{acc:.4f},Loss:{mean_loss:.4f}")

        model.eval()  # Set the model to evaluation modeè¯„ä¼°æ¨¡å¼
        test_data = tqdm(test_dataloader)  # Wrap dataloader with tqdm for a progress bar
        mean_loss, acc = 0.0, 0.0
        data_num = 0
        with torch.no_grad():  # Disable gradient calculationæ²¡æœ‰æ¢¯åº¦è®¡ç®—å’Œæ›´æ–°
            for x, y in test_data:
                x = x.to(device)
                y = y.to(device)
                predicts = model(x)
                loss = loss_fn(predicts, y)
                # ä¸éœ€è¦ä¼˜åŒ–å’Œæ›´æ–°
                acc += torch.sum(torch.argmax(predicts, dim=1) == y).item()
                mean_loss += loss.item() * x.size(0)
                data_num += x.size(0)
                test_data.set_description(
                    f"Evaluation ... Epoch: {i + 1}/{epochs}, Loss: {loss.item() * x.size(0):.4f}")
            mean_loss /= data_num
            acc /= data_num
            print(f"\nTraining Acc: {acc:.4f}, Loss: {mean_loss:.4f}")

        torch.save(model.state_dict(), "./model.pth")

```