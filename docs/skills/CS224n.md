# Stanford CS224N: NLP with Deep Learning

[Stanford CS224N: NLP with Deep Learning | Spring 2024 | Lecture 1 - Intro and Word Vectors](https://www.youtube.com/watch?v=DzpHeXVSC5I&list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D)

## Lecture 1 - Intro and Word Vectors

WordNet, a thesaurus containing lists of **synonym sets** and **hypernyms** (â€œis aâ€ relationships)

è¯å…¸åŒ…å«åŒä¹‰è¯é›†å’Œä¸Šä½è¯åˆ—è¡¨

-   It canâ€™t be used to compute word similarity accurately 

-   So we need to **learn to encode similarity in the vectors themselves**

Represent words by their **context**

>   [!NOTE]
>
>   Word vectors are also called (word) embeddings or (neural) word representations
>
>   They are a distributed representation



### word2vec

#### objective function

![image-20251226165150389](./assets/image-20251226165150389.png)

-   To  calculate $ğ‘ƒ(ğ‘¤_{t+j}|ğ‘¤_t;\theta)$, we will *use two* vectors per word *w*: $v_w$ context and $u_w$ center.

    <img src="./assets/image-20251226170325631.png" alt="image-20251226170325631" style="zoom:30%;" />

#### Prediction fucntion

![image-20251226170523402](./assets/image-20251226170523402.png)

  Interactive Session

<img src="./assets/image-20251226180649642.png" alt="image-20251226180649642" style="zoom: 33%;" />

Model: the softmax function

Objective: Maximize the Log-Likelihood



## Lecture 2: Word Vectors, Word Senses, and Neural Classifiers

### Optimization: Gradient Descent

![image-20251228012002377](./assets/image-20251228012002377.png)

<img src="./assets/image-20251228012313105.png" alt="image-20251228012313105" style="zoom: 33%;" />

Actually never use because  $ğ½(ğœƒ)$ is a function of **all** windows in the corpus (potentially billions!)  very expensive to compute

Use **Stochastic Gradient Descent**

-   Repeatedly sample windows, and update after each one
-   <img src="./assets/image-20251228012329674.png" alt="image-20251228012329674" style="zoom:33%;" />



### Main idea of word2vac

![image-20251228014452101](./assets/image-20251228014452101.png)

-   Word2vec maximizes objective by putting similar words nearby in space
-   

Skip-gram

### GloVe



 **How to evaluate word vectors**

-   intrinsic and extrinsic





**A neural network = running several logistic regressions at the same time**



