# ECE448/CS440 Artificial Intelligence

[CS 440 Artificial Intelligence](https://courses.grainger.illinois.edu/cs440/fa2025/)

https://github.com/illinois-cs-coursework

https://courses.grainger.illinois.edu/cs440/fa2025/readings.html

> [!IMPORTANT]
>
> https://courses.grainger.illinois.edu/cs440/fa2025/lectures/probability-review.html
>

## Introduction

> https://courses.grainger.illinois.edu/cs440/fa2025/lectures/intro.html

### Historical and other trivia

We've seen a lot of trivia, most of it not worth memorizing. The following items are the exceptions. Be able to explain (very briefly) what they are and (approximately) what time period they come from.

- **McCulloch and Pitts**
    - **Time Period:** 1940s
    - **Contribution:** They introduced the *first mathematical model of a neural network*. Their work was foundational, proposing that networks of simple computational units (neurons) could perform complex logical operations. These were *theoretical models on paper*, as the hardware to implement them didn't exist yet.

- **Fred Jelinek**
    -   **Time Period:** 1980s - 1990s
    -   **Contribution:** A key figure in *speech recognition*. He pioneered the use of statistical models, specifically *n-gram language models* and Hidden Markov Models (HMMs), which dramatically improved the accuracy and utility of speech recognition systems.
- **Pantel and Lin (SpamCop)**
    -   **Time Period:** Late 1990s
    -   **Contribution:** They were pioneers in using *Naive Bayes classifiers for spam detection*. Their work showed that this statistical approach was highly effective for classifying emails, forming the basis of many modern spam filters.
        -   æœ´ç´ è´å¶æ–¯åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨
- **Boulis and Ostendorf**
    -   **Time Period:** Mid 2000s
    -   **Contribution:** They conducted research comparing the performance of Naive Bayes versus Support Vector Machine (SVM) classifiers for gender classification based on transcribed telephone conversations.
    -   [A Quantitative Analysis of Lexical Differences Between Genders in Telephone Conversations](http://www.aclweb.org/anthology/P05-1054), ACL 2005
- **The Plato System**
    -   **Time Period:** Started in the 1960s
    -   **Contribution:** An early and influential *computer-assisted instruction system* developed at the University of Illinois. It was a precursor to modern *e-learning platforms and online communities*.
- **The Golem of Prague**
    -   **Time Period:** 16th-century Jewish folklore
    -   **Contribution:** An early myth or story related to artificial intelligence. It tells of an artificial humanoid creature created from clay to protect the Jewish community. It represents an ancient human desire to create intelligent, autonomous beings.

### Probability

> https://courses.grainger.illinois.edu/cs440/fa2025/lectures/probability-review.html

**Random variables, axioms of probability:**

-   A **random variable** is a variable whose value is a numerical outcome of a random phenomenon.
-   The **axioms of probability** (**Kolmogorov's axioms of probability** æŸ¯å°”è«å“¥æ´›å¤«æ¦‚ç‡å…¬ç†) are fundamental rules:
    1.  The probability of any event is non-negative.
    2.  The probability of the entire sample space (a certain event) is 1.
    3.  The probability of the union of mutually exclusive events is the sum of their individual probabilities. 

$$
0 â‰¤P(A)\\
P(True) = 1\\
P(A|B) = P(A) + P(B), \text{if A and B are mutually exclusive events}
$$
-   **Joint, marginal, conditional probability:**
    -   **Joint Probability** è”åˆæ¦‚ç‡ $P(A,B)$: The probability of two events occurring together
    -   **Marginal Probability** è¾¹é™…æ¦‚ç‡ $P(A)$: The probability of a single event occurring, irrespective of other events. It can be calculated by summing the joint probabilities over all outcomes of the other variable: $P(A)=âˆ‘_BP(A,B)$
    -   **Conditional Probability** æ¡ä»¶æ¦‚ç‡ $P(Aâˆ£B)$: The probability of event A occurring *given* that event B has already occurred. It is calculated as $P(A | B) = \frac{P(A,B)}{P(B)}$

### **Modelling Text Data**

**Word types vs. word tokens:**
-   **Tokens:** The total number of words in a document (e.g., "the cat sat on the mat" has 6 tokens). å•è¯æ€»æ•°
-   **Types:** The number of *unique* words in a document (e.g., "the cat sat on the mat" has 5 types: "the", "cat", "sat", "on", "mat"). è¯å…¸æ¡ç›®ï¼Œå”¯ä¸€çš„å•è¯æ•°

**The Bag of Words model:** We can use the *individual words as features*. A bag-of-words model determines the class of a document based on the *frequency* of occurrence of each word. It ignores the order in which words occur, which ones occur together, etc. So it will miss some details, e.g. the difference between "poisonous" and "not poisonous." å¿½ç•¥è¯­æ³•ç”šè‡³è¯åºä½†ä¿æŒå¤šæ ·æ€§

**Bigrams, ngrams:**
-   **N-grams** are contiguous sequences of *n* items (e.g., words, letters) from a given sample of text.
    -   **N-gram** æ˜¯æ¥è‡ªç»™å®šæ–‡æœ¬æ ·æœ¬çš„ *n ä¸ª*é¡¹ç›®ï¼ˆä¾‹å¦‚å•è¯ã€å­—æ¯ï¼‰çš„è¿ç»­åºåˆ—ã€‚
-   A **bigram** is a specific n-gram where n=2 (a two-word sequence). For example, in "the cat sat", the bigrams are "the cat" and "cat sat".
    -   ç‰¹å®šçš„ n-gramï¼Œå…¶ä¸­ n=2ï¼ˆå³ä¸¤ä¸ªå•è¯çš„åºåˆ—ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨â€œthe cat satâ€ä¸­ï¼ŒäºŒå…ƒè¯­æ³•æ˜¯â€œthe catâ€å’Œâ€œcat satâ€ã€‚

**Data cleaning:**
-   **Tokenization:** The process of splitting a stream of text into words, phrases, symbols, or other meaningful elements called tokens.
    -   **æ ‡è®°åŒ–ï¼š** å°†æ–‡æœ¬æµæ‹†åˆ†ä¸ºå•è¯ã€çŸ­è¯­ã€ç¬¦å·æˆ–å…¶ä»–æœ‰æ„ä¹‰çš„å…ƒç´ ï¼ˆç§°ä¸ºæ ‡è®°ï¼‰çš„è¿‡ç¨‹ã€‚å®šä¹‰å•è¯å¾—åˆ° a clean string of words
    -   divide at whitespace  åœ¨ç©ºç™½å¤„åˆ’åˆ†
    -   normalize punctuation, html tags, capitalization, etc è§„èŒƒæ ‡ç‚¹ç¬¦å·ã€html æ ‡ç­¾ã€å¤§å†™å­—æ¯ç­‰
    -   perhaps use a stemmer to remove word endings ä½¿ç”¨è¯å¹²åˆ†æå™¨æ¥åˆ é™¤å•è¯ç»“å°¾
-   **Stemming åˆ†è¯:** The process of reducing inflected (or sometimes derived) words to their word stem, base or root form. **Julie Lovins** (1968) created one of the first stemming algorithms, and **Martin Porter** (1980) developed the Porter Stemmer, which is one of the most widely used.
    -   **è¯å¹²æå–ï¼š** å°†è¯å½¢å˜åŒ–çš„è¯ç®€åŒ–ä¸ºè¯å¹²ã€åŸºè¯æˆ–è¯æ ¹å½¢å¼çš„è¿‡ç¨‹ã€‚Julie Lovins åˆ›å»ºäº†æœ€æ—©çš„è¯å¹²æå–ç®—æ³•ä¹‹ä¸€ï¼Œ Martin Porter å¼€å‘äº† Porter è¯å¹²æå–å™¨ï¼Œå®ƒæ˜¯ç›®å‰ä½¿ç”¨æœ€å¹¿æ³›çš„ç®—æ³•ä¹‹ä¸€ã€‚
-   **Making units of useful size:** This involves either breaking long words into smaller pieces (common in languages like German) or grouping characters into words (necessary for languages without spaces, like Chinese).
    -   å°†é•¿å•è¯åˆ†æˆæ›´å°çš„éƒ¨åˆ†ï¼Œç‰¹åˆ«æ˜¯ä¸­æ–‡ï¼ˆæ²¡æœ‰ç©ºæ ¼ï¼‰

**Special types of words:**
-   **Stop words:** Very common words (e.g., "the", "a", "is") that are often removed before processing because they carry little semantic weight.
    -   éå¸¸å¸¸è§çš„è¯ï¼šfunction words, fillers, backchannel
-   **Rare words:** Words that appear very infrequently. They can be problematic for statistical models and are sometimes removed or replaced with a generic "UNK" (unknown) token.
    -   ç”Ÿåƒ»è¯ï¼šå‡ºç°é¢‘ç‡æä½çš„è¯ï¼Œåˆ é™¤ä¸€éƒ¨åˆ†æˆ–éƒ½ç”¨UNKæ ‡è®°ï¼ˆè§†ä¸ºä¸€ä¸ªå•ç‹¬çš„é¡¹ç›®ï¼‰
-   **Hapax legomena:** Words that occur only once in a corpus. They are an extreme case of rare words.
    -   ç½•è§è¯çš„æç«¯æƒ…å†µï¼Œåªå‡ºç°ä¸€æ¬¡
-   **Filler:** Words or sounds used to pause in a conversation (e.g., "um," "uh," "like").
    -   å¡«å……è¯
-   **Backchannel:** Signals from a listener that indicate they are paying attention (e.g., "uh-huh," "yeah," "I see").
    -   å¬ä¼—å‘å‡ºçš„ä¿¡å·è¯
-   **Function vs. content words:** *Content words* (nouns, main verbs, adjectives) carry the primary meaning. *Function words* (prepositions, articles, conjunctions) provide grammatical structure.
    -   **å®è¯** ï¼ˆåè¯ã€ä¸»è¦åŠ¨è¯ã€å½¢å®¹è¯ï¼‰æ‰¿è½½ä¸»è¦å«ä¹‰
    -   **åŠŸèƒ½è¯** ï¼ˆä»‹è¯ã€å† è¯ã€è¿è¯ï¼‰æä¾›è¯­æ³•ç»“æ„

### **Testing**

**Roles of training, development, test datasets:**

-   **Training set è®­ç»ƒé›†:** The data used to train the model and learn its parameters.
-   **Development set (or validation set) éªŒè¯é›†:** The data used to tune the model's hyperparameters and make design choices. It helps prevent overfitting to the training set. é˜²æ­¢è¿‡åº¦æ‹Ÿåˆ
-   **Test set æµ‹è¯•é›†:** The data held back until the very end to provide an unbiased, final evaluation of the model's performance. ä¿ç•™åˆ°æœ€åçš„æ•°æ®ï¼Œä»¥å¯¹æ¨¡å‹çš„æ€§èƒ½æä¾›å…¬æ­£çš„æœ€ç»ˆè¯„ä¼°ã€‚

**Evaluation metrics for classification:**

<img src="./assets/image-20250825085728014.png" alt="image-20250825085728014" style="zoom:50%;" />

| Confusion Matrix æ··æ·†çŸ©é˜µ | Labels from Algorithm |                     |
| ------------------------- | --------------------- | ------------------- |
| /                         | happen                | Not happen          |
| Correct = happen          | True Positive (TP)    | False Negative (FN) |
| Correct = not happen      | False Positive (FP)   | True Negative (TN)  |

- **False positive rate** = $FP/(FP+TN)$ [how many wrong things are in the negative outputs]
- **False negative rate** = $FN/(TP+FN)$ [how many wrong things are in the positive outputs]
- **Accuracy** = $(TP+TN)/(TP+TN+FP+FN)$
    - The fraction of predictions the model got righ
- **Error rate** = $1-accuracy$
- **precision (p)** = $TP/(TP+FP)$ [how many of our outputs were correct?]
- **recall (r)** = $TP/(TP+FN)$ 
    - True Positive [how many of the correct answers did we find?]
- **F1** = $2pr/(p+r)$
    - F1 is the harmonic mean of precision and recall. Both recall and precision need to be good to get a high F1 value.

-   **Confusion Matrix:** A table that visualizes the performance of a classifier, showing the counts of true positives, true negatives, false positives, and false negatives.

## Naive Bayes

>   https://courses.grainger.illinois.edu/cs440/fa2025/lectures/bayes.html

### Basic definitions and mathematical model

-   $P(A | C)$ is the probability of A in a context where C is true

    - Definition of conditional probability: $P(A | C) = \frac{P(A,C)}{P(C)}$
        - $P(A)=âˆ‘P(A|Z) p(Z,\theta)$
    - $P(A,C) = P(A) \times P(C | A) = P(C,A)=P(C) \times P(A | C)$
    - **Bayes Rule**: $P(C|A)=\frac{P(A|C)\times P(C)}{P(A)}$
        - $P(cause|evidence)=\frac{P(evidence|cause)P(cause)}{P(evidence)}$
        - posterior likelihood prior normalization

-   **Likelihood:** $P(evidenceâˆ£cause)$ æ¦‚ç‡
-   **Prior:** $P(cause)$ å…ˆéªŒ
-   **Posterior:** $P(causeâˆ£evidence)$ åéªŒ
-   **argmax operator:** Returns the input value that maximizes a function. In classification, we use it to find the class with the highest posterior probability.
    -   è¡¨ç¤ºè¿”å›ä½¿å‡½æ•°æœ€å¤§åŒ–çš„è¾“å…¥å€¼çš„ç¬¦å·ï¼Œç”¨æ¥æ‰¾åˆ°åéªŒæ¦‚ç‡æœ€é«˜çš„ç±»
-   **Independence vs. Conditional Independence:** Naive Bayes makes a "naive" assumption of *conditional independence* of features: features are independent of each other *given the class*. This is a stronger assumption than simple independence.
    -   **Independence** ç‹¬ç«‹æ€§
        - Two events A and B are independent **iff** $P(A,B) = P(A) \times P(B)$
    -   **Conditional Independence** æ¡ä»¶ç‹¬ç«‹æ€§
        -   Definition ï¼š$P(A, B | C) = P(A|C) \times P(B|C)$, ç­‰ä»·äº $P(A | B) = P(A), P(B | A) = P(B)$
    -   ç‹¬ç«‹æ€§å¾ˆå°‘æˆç«‹ï¼›æ¡ä»¶ç‹¬ç«‹æ€§æ˜¯åœ¨ç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¸­ï¼Œä¸¤ä¸ªå˜é‡æ˜¯å¦ç‹¬ç«‹ï¼Œè¿‘ä¼¼åˆç†ã€‚
-   **MAP vs. ML estimate**
    -   **Maximum Likelihood (ML)** chooses the parameters that maximize the likelihood of the data.  æ¦‚ç‡æœ€å¤§åŒ–
    -   **Maximum a Posteriori (MAP)** incorporates a prior probability, choosing parameters that maximize the posterior probability. The prior acts as a regularizer. åéªŒæ¦‚ç‡æœ€å¤§åŒ–
-   **Combining evidence:** Under the conditional independence assumption, the likelihood of all evidence is simply the product of the likelihoods of each individual piece of evidence:
    -    $P(evidence_1,â€¦,evidencenâˆ£cause)=âˆ_iP(evidence_iâˆ£cause)$.
-   **Model size:** Naive Bayes dramatically reduces the number of parameters needed compared to a full joint distribution table, making it computationally feasible and less prone to overfitting on small datasets.

### Applying Naive Bayes to text classification

-   **equations:** You estimate the prior probability of a class by its frequency in the training data, and the likelihood of a word given a class by its frequency within documents of that class.

    -   ä¼°è®¡æ–¹ç¨‹ï¼Œé€šè¿‡è®­ç»ƒæ•°æ®ä¸­æŸä¸ªç±»åˆ«çš„é¢‘ç‡æ¥ä¼°è®¡è¯¥ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡

-   **Avoiding underflow:** Since you are multiplying many small probabilities, the result can become too small for a computer to store (underflow). To fix this, you work with the sum of log probabilities instead: $log(Aâ‹…B)=log(A)+log(B)$.

    -   å¯¹æ•°é˜²æ­¢ä¹˜ä»¥å¤ªå°çš„æ•°å­—å½±å“å‡†ç¡®æ€§ï¼ˆè®¡ç®—ç²¾åº¦ï¼‰ï¼Œå°†æœ´ç´ è´å¶æ–¯ç®—æ³•æœ€å¤§åŒ–

-   **Avoiding overfitting (Smoothing)** å¹³æ»‘å¤„ç†

    -   **Why it's important:** If a word never appears in the training data for a certain class, its probability will be zero, causing the entire posterior probability for that class to become zero. è¿‡åº¦æ‹Ÿåˆï¼Œ0ä¼šç ´åæœ´ç´ è´å¶æ–¯ç®—æ³•

    -   **Laplace smoothing:** Adds a small constant (usually 1) to every count, ensuring no probability is ever zero.

        -   æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼š<img src="./assets/image-20250902212218178.png" alt="image-20250902212218178" style="zoom:50%;" />

        -   $n$ = number of words in our Class C training data
            $count(W)$ = number of times W appeared in Class C training data

            $\alpha$: a constant positive number
            $V$ = number of word **types** seen in training data

    -   **Deleted estimation åˆ é™¤ä¼°è®¡:** A cross-validation technique used to find optimal smoothing parameters.

        -   å¯¹äºæ¯ä¸ªè§‚æµ‹åˆ°çš„è®¡æ•° rï¼Œæˆ‘ä»¬å°†è®¡ç®—ä¸€ä¸ªæ ¡æ­£åçš„è®¡æ•° $Corr(r)$ã€‚å‡è®¾ $W1,...,Wn$æ˜¯åœ¨æ•°æ®é›†å‰åŠéƒ¨åˆ†å‡ºç° r æ¬¡çš„å•è¯ã€‚å¯¹äºæ­¤é›†åˆä¸­çš„æ¯ä¸ªå•è¯ $W_k$ï¼Œæ±‚å‡ºå®ƒåœ¨æ•°æ®é›†ååŠéƒ¨åˆ†å‡ºç°çš„è®¡æ•° $C(W_k)$ ã€‚æˆ‘ä»¬å°†è¿™äº›è®¡æ•°å–å¹³å‡å€¼ï¼Œå¾—åˆ°æ ¡æ­£åçš„è®¡æ•°ï¼š
        -   <img src="./assets/image-20250903094420273.png" alt="image-20250903094420273" style="zoom:40%;" />Corr(r) é¢„æµ‹è®­ç»ƒæ•°æ®ä¸­å‡ºç° r æ¬¡çš„å•è¯çš„æœªæ¥è®¡æ•°
        -   åˆ é™¤ä¼°è®¡å·²è¢«è¯æ˜æ¯”æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘æ›´å‡†ç¡®
        
    -   **N-gram smoothing:** Refers to more advanced techniques (like Good-Turing, Kneser-Ney) used for n-gram models to handle unseen n-grams by redistributing probability mass from seen n-grams. The high-level idea is to "borrow" probability from more frequent events to assign to rare or unseen events.
    
        -   å¯¹äºä¸€å…ƒæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¼°è®¡ n ä¸ªæ¦‚ç‡ï¼Œè€Œå¯¹äºäºŒå…ƒæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¼°è®¡ $n^2$ ä¸ªæ¦‚ç‡ï¼Œä½†æˆ‘ä»¬ä»ç„¶æ‹¥æœ‰ç›¸åŒçš„ m ä¸ªå•è¯ä½œä¸ºè®­ç»ƒæ•°æ®
        -   Idea 1: If we haven't seen an ngram, guess its probability from the probabilites of its *prefix* (e.g. "the angry") and the *last word* ("armadillo").
        -   Idea 2: Guess that an unseen word is more likely in contexts where we've seen many different words.

----

## Search

### Example tasks

-   In some detail: roadmap search, maze search, 8-puzzle, edit distance,
-   Seen briefly: speech recognition, adventure game, [Missionaries and Cannibals puzzle](https://en.wikipedia.org/wiki/Missionaries_and_cannibals_problem)
-   

### Basic search methods

-   Search problem formulation: state space, initial state, actions, transition model, goal state(s), path costs
-   Outline for search algorithms: frontier/queue, best/optimal path, visited states table, backpointers and reconstructing return path
-   Breadth-first, uniform cost search, depth-first search, length-bounded DFS, iterative deepening
-   What information should be stored in each state?



State graph representations çŠ¶æ€å›¾

A state graph has the following key parts:

-   states (graph nodes)  çŠ¶æ€ï¼ˆå›¾èŠ‚ç‚¹ï¼‰
-   actions (graph edges, with costs)
-   start state  èµ·å§‹çŠ¶æ€
-   goal states (explicit list, or a goal condition to test) ç›®æ ‡çŠ¶æ€ï¼ˆæ˜ç¡®åˆ—è¡¨ï¼Œæˆ–è¦æµ‹è¯•çš„ç›®æ ‡æ¡ä»¶ï¼‰

### Basic search outline

Our search will use two main data structures:

-   a table of states that have been visited
-   the frontier: a queue of states whose outgoing edges still need to be explored

Basic outline for search code

-   Loop until you find a goal state or queue is empty
-   pop state S off frontier
-   follow outgoing edges to find S's neighbors
-   for each neighbor X that has not yet been visited, add X to the visited table and to the frontier

Our data structure for storing the frontier determines the type of search

-   breadth-first search (BFS): queue
-   uniform-cost search (UCS): priority queue using cost so far
-   $A^âˆ—$search: priority queue using estimated total cost

#### BFS

-   it returns a path that uses the minimum number of edges

-   The main loop for BFS looks like this:

    ```
    Loop until queue is empty
    -   pop state S off frontier
    -   follow outgoing edges to find S's neighbors
    -   for each neighbor X that has not yet been visited
        -   If X is a goal state, halt and return its path
        -   Otherwise, add X to the visited table and to the frontier
    ```

    

The search can return as soon as a goal state is visited, i.e. when we'd normally put that state into the visited states table. (Note that this is NOT the same as UCS and Aâˆ— search below.) Do not wait until the queue is empty.

Properties:

-   Solution is always optimal if and only if all edges have same cost.
-   Frontier can get very large, often grows linearly with distance outwards from start.

#### UCS

-   finds optimal paths even when edge costs vary
-   

#### A* search

Key idea: combine distance so far with an estimate of the remaining distance to the goal.

Our main search loop for Aâˆ— looks like the one for UCS. However, a state x's queue priority is an estimate f(x) of the final length of a path through that state to the goal. Specifically, f(x) is the sum of

-   $g(x)$ actual cost to reach x from the start state, and
-   $h(x)$ estimated ("heuristic") distance from x to the goal å¯å‘å¼è·ç¦»
    -   Heuristic functions for A* need to have two properties
        -   quick to compute
        -   underestimates of the true distance to the goal.



-   basic definitions and algorithm
-   Manhattan vs. straight-line distance
    -   The **Manhattan distance** between two points is the difference in their x coordinates plus the difference in their y coordinates. Manhattan distance dominates straight-line distance. If there are also diagonal streets, we can't use Manhattan distance because it can overestimate the distance to the goal.
-   definitions of admissible and consistent, how do they relate? why is consistency useful?

In practice, **admissible heuristics are typically consistent**. For example, the straight-line distance is consistent because of the triangle inequality. Uniform cost search is Aâˆ— with a heuristic function that always returns zero. That heuristic is consistnt. However, if you are writing general-purpose search code, it's best to use a method that will still work if the heuristic isn't consistent, such as pushing a duplicate copy of the state onto the queue.



-   returns optimal path if heuristic admissible

    -   Admissibility guarantees that the output solution is optimal. Here's a sketch of the proof:
        å¯æ¥çº³æ€§ä¿è¯äº†è¾“å‡ºè§£æ˜¯æœ€ä¼˜çš„ã€‚ä»¥ä¸‹æ˜¯è¯æ˜çš„æ¦‚è¦ï¼š

    >   Search stops when goal state becomes top option in frontier (not when goal is first discovered). Suppose we have found goal with path P and cost C. Consider a partial path X in the frontier. Its estimated cost must be â‰¥C. Since our heuristic is admissible, the true cost of extending X to reach the goal must also be â‰¥C. So extending X can't give us a better path than P.
    >   å½“ç›®æ ‡çŠ¶æ€æˆä¸ºè¾¹ç•Œä¸­çš„æœ€ä¼˜é€‰é¡¹æ—¶ï¼ˆè€Œä¸æ˜¯ç›®æ ‡é¦–æ¬¡è¢«å‘ç°æ—¶ï¼‰ï¼Œæœç´¢åœæ­¢ã€‚å‡è®¾æˆ‘ä»¬å·²ç»æ‰¾åˆ°äº†è·¯å¾„ä¸º Pã€æˆæœ¬ä¸º C çš„ç›®æ ‡ã€‚è€ƒè™‘è¾¹ç•Œä¸­çš„éƒ¨åˆ†è·¯å¾„ Xã€‚å®ƒçš„ä¼°è®¡æˆæœ¬å¿…å®šä¸º â‰¥C ã€‚ç”±äºæˆ‘ä»¬çš„å¯å‘å¼æ–¹æ³•å¯è¡Œï¼Œå› æ­¤æ‰©å±• X ä»¥è¾¾åˆ°ç›®æ ‡çš„çœŸå®æˆæœ¬ä¹Ÿå¿…å®šä¸º â‰¥C ã€‚å› æ­¤ï¼Œæ‰©å±• X ä¸å¯èƒ½æä¾›æ¯” P æ›´å¥½çš„è·¯å¾„ã€‚

-   what is a good heuristic? what makes one heuristic better than another?

    -   

-   suboptimal search (non-admissible heuristics) and tie-breaking

    -   æ¬¡ä¼˜æœç´¢å’Œæ‰“ç ´å¹³å±€
    -   adjust the queue priorities or multiply our heuristic values by a factor slightly bigger than 1
    -   This can speed up search significantly. The downside is that $A^âˆ—$ may return a path that isn't the shortest possible.

-   Beam search

    -    "Beam search" algorithms prune states with high values of f, frequently restricting the queue to some fixed length N. Like suboptimal search, this sabotages guarantees of optimality.

#### Depth-first search (DFS)

-   In depth-first search, the frontier is stored as a stack (explicitly or implicitly via recursive function calls). So it fully extends the current path, i.e. the one represented by the state at the top of the stack, before moving on to other alternatives.
-   If the search space is infinite (e.g. dynamically constructed), DFS is not guaranteed to reach the goal even if the goal is right near the start. If the frontier is implemented implicitly using recursion, DFS may hit system limits on the size of the recursion stack.

#### Iterative deepening

-   Each iteration starts from scratch, forgetting all previous work.

-   Uses very little memory.

-   Iterative deepening is most useful when the new search area at distance k is large compared to the area search in previous iterations, so that the search at distance k is dominated by the paths of length k.

    ```
    For k = 1 to infinity
    -		Run DFS but stop whenever a path contains at least k states.
    - 	Halt when we find a goal state
    ```

    

Details for each algorithm

-   How does it manage its frontier/queue?
-   Preventing loops and duplicative work
-   What happens if we reach a state via a second, shorter, path?
-   When does the algorithm halt and return its final result?
-   When would each method be an appropiate choice, when would it be inappropriate, and why
-   Compare advantages and disadvantages of each of these search methods

## Robotics and configuration space

Shakey and Blocks world

-   Shakey the robot (1966-72)
    -   had the basic components of a mobile robot

-   Boston Dynamics
    -   Legged robots (Marc Raibert and Boston Dynamics): [early ones](https://www.youtube.com/watch?v=Bd5iEke6UlE) and [newish humanoid one](https://www.youtube.com/watch?v=vjSohj-Iclc) (audio is mechanical noises)
-   Google self-driving bike
    -   Fake
-   [Coning a self-driving car](https://www.youtube.com/watch?v=8MfyIsPWhTk)



The math/geometry turns evil very fast. So expect concrete 2D problems involving relatively simple situations.

-   Example configuration spaces: 

    -   circular robot
    -   n-link arm
        -   [2-link robot simulator](https://www.cs.unc.edu/~jeffi/c-space/robot.xhtml) by Jeffrey Ichnowski
        -   2-link arm's configuration space (from Howie Choset)
        -   

    -   rectangular robot with/without rotation
        -   

-   Sketch, or answer questions about, the configuration space for a (very simple) robot motion planning problem

    -   é…ç½®ç©ºé—´ a configuration space shows all the possible positions for the center of the robot
    -   Normally we convert configuration space into a graph representation containing

        -   waypoints  èˆªç‚¹
        -   edges joining them  è¿æ¥å®ƒä»¬çš„è¾¹ç¼˜

        These graph representations are compact and can be searched efficiently


General knowledge

-   types of robots (e.g. arms, moving carts, snakes, ...)
-   links, joints, degrees of freedom, workspace
-   **ideal robot path**: short but also smooth, safe
-   graph-based planning: visibility graphs, cell decompositions, generalized Voronoi diagrams, probabilistic roadmaps
    -   visibility graphs å¯è§æ€§å›¾è¡¨ï¼šuses vertices of objects as waypoints and edges are straight lines connecting vertices.
        -   It produces paths of minimal length. However, these paths aren't very safe because they skim the edges of obstacles. It's risky to get too close to obstacles. 

    -   Skeletonization éª¨æ¶åŒ–
        -   **RoadMap** converts freespace into a skeleton that goes through the middle of regions. Paths via the skeleton stay far from obstacles, so they tend to be safe but perhaps overly long.
        -   **Generalized Voronoi diagrams** places the skeleton along curves that are equidistant from two or more obstacles. The waypoints are the places where these curves intersect.

-   how can search be complicated by big open areas? by narrow passages? by robots with more than a couple degrees of freedom?















## Readings

### Quiz 1

#### An algorithm for suffix stripping åç¼€å‰¥ç¦»

The main goal of the paper is to introduce a simple, fast, and effective algorithm for **suffix stripping** (also known as **stemming**). This process removes common endings from words to get to their root form, or "stem."

##### Why is Stemming Important? ğŸ¤”

-   The primary use is in **Information Retrieval (IR)**, like search engines. 
-   It groups related words together. For example, **"connect," "connected," "connecting,"** and **"connection"** all get reduced to the single stem **"connect."** 
-   This **improves search results** because a search for "connecting" will also find documents that only mention "connection." 
-   It also reduces the total number of unique words a system has to store, making the system smaller and more efficient.

##### How the Algorithm Works âš™ï¸

The algorithm's cleverness lies in its simplicity. It doesn't use a dictionary. Instead, it uses a set of rules based on the structure of the word itself.

##### The "Measure" of a Word

The core concept is the **measure (m)** of a stem, which roughly corresponds to the number of vowel-consonant sequences it contains. 

A word is first broken down into a sequence of vowel groups (V) and consonant groups (C). The form of any word can be represented as:

```
[C](VC)^m[V]
```

-   `C`: one or more consonants
-   `V`: one or more vowels
-   `m`: the **measure**, or how many times the `(VC)` group repeats.

Here are some examples:

-   **TR, EE, TREE** â†’m=0
-   **TROUBLE, OATS** â†’m=1
-   **TROUBLES, PRIVATE** â†’m=2

This measure (m) is used to prevent the algorithm from stripping suffixes from words that are already very short. 10For example, it will remove `-ATE` from "ACTIVATE" (stem `ACTIV` has m>1) but not from "RELATE" (stem `REL` has m=1).

##### The 5 Steps of the Algorithm

The algorithm removes suffixes sequentially in five steps. A word goes through each step in order. 

-   **Step 1:** Deals with plurals and past participles. It changes suffixes like 

    `-SSES` to `-SS` (e.g., `caresses` â†’ `caress`), `-IES` to `-I` (e.g., `ponies` â†’ `poni`), and removes `-ED` or `-ING` if the stem meets certain conditions.

-   **Step 2:** Handles other common suffixes. If the stem's measure (

    m) is greater than 0, it will change suffixes like `-ATIONAL` to `-ATE` (e.g., `relational` â†’ `relate`) or `-IZATION` to `-IZE` (e.g., `vietnamization` â†’ `vietnamize`).

-   **Step 3:** Continues with another set of suffixes for stems where m>0. It changes 

    `-ICAL` to `-IC` (e.g., `electrical` â†’ `electric`) or removes `-NESS` (e.g., `goodness` â†’ `good`).

-   **Step 4:** Removes a final set of suffixes like `-ANT`, `-ENCE`, `-ER`, and `-IVE`, but only if the stem is long enough (m>1). 16161616This is the step that would turn `GENERALIZE` into `GENERAL`.

-   **Step 5:** This is a final cleanup step. It removes a trailing 

    `-E` if the measure is large enough (e.g., `probate` â†’ `probat`) and reduces a double `L` at the end of a word (e.g., `controll` â†’ `control`).

##### Key Takeaways âœ…

-   **It's Pragmatic, Not Perfect:** The algorithm is designed for IR performance, not perfect linguistics. It will make errors, such as conflating "WAND" and "WANDER," but these errors are acceptable because the overall benefit is positive.
-   **Simple is Better:** Despite its simplicity, it performed slightly *better* in tests than a much more complex stemming system.
-   **Effective:** In a test with 10,000 words, the algorithm reduced the number of unique stems to 6,370, a reduction of about one-third.

#### The Psychological Functions of Function Words

The central argument of the paper is that **function words**â€”small, common words like pronouns (I, you, we), prepositions (to, for), and articles (a, the)â€”are powerful indicators of our psychological state, personality, and social dynamics. Because we use these words unconsciously, they provide an unfiltered look into how we think and relate to others.

##### Function Words vs. Content Words

-   **Content words** are nouns and regular verbs that carry the primary meaning or topic of what we're saying (e.g., "family," "health," "money").
-   **Function words** are the "cement" that holds language together4. They don't have much meaning on their own but show *how* we are expressing ourselves.
-   Though there are fewer than 400 function words in English, they account for over **50% of the words we use** in daily life5.
-   We have very little conscious control over or memory of using them, which makes them a great tool for psychological analysis.

##### Key Findings from Word Analysis ğŸ§‘â€ğŸ”¬

The authors used a computer program called **LIWC (Linguistic Inquiry and Word Count)** to analyze millions of words from blogs, speeches, emails, and experiments7. Here are their most important findings:

1.   Depression and Self-Focus

-   A higher frequency of **first-person singular pronouns** ("I," "me," "my") is strongly linked to depression and negative emotions8. In fact, pronoun use is a better marker of depression than the use of negative emotion words like "sad" or "angry".
-   Poets who committed suicide used "I" at significantly higher rates than non-suicidal poets, suggesting greater self-focus and less social integration.

2. Reactions to Stress

-   **Individual Stress:** When facing personal crises (divorce, cancer diagnosis), Mayor Rudy Giuliani's use of "I" words shot up from about 2% to over 7%. This shows that personal distress often leads to an intense focus on the self.
-   **Shared Stress:** After the 9/11 attacks, the opposite happened. People's use of "I" words *dropped*, while their use of **first-person plural pronouns** ("we," "us") increased. This suggests that a shared tragedy causes people to focus less on themselves and more on their community and social connections.

3. Honesty and Deception

-   When people are **telling the truth**, they tend to use more **first-person singular pronouns** ("I") and more **exclusive words** (e.g., "but," "except," "without").
-   This suggests that truthful accounts are more personal and cognitively complex, while lies are simpler and more detached.

4. Social Status

-   In a conversation, the person with **higher status** consistently uses **fewer "I" words**.
-   For example, in his conversations, President Nixon used "I" far less when speaking to his subordinates John Dean and John Erlichman than they did when speaking to him16. The lower-status person focuses on their own perspective, while the higher-status person focuses on the broader picture.

5. Demographics (Sex and Age)

-   **Sex:** Females tend to use "I" words more than males. Males use more articles ("a," "the") and nouns, which is associated with concrete, categorical thinking18. Females use more verbs, which reflects a more relational focus.
-   **Age:** As people get older, they use "I" less and "we" more20. They also use more future-tense verbs and fewer past-tense verbs, suggesting their focus shifts over the lifespan.

6. Culture

-   Counterintuitively, translated Japanese texts used "I" *more* than American texts. The authors suggest this may be because collectivist values (like harmony and self-criticism) require a high degree of self-focus.
-   American texts used more articles ("a," "the"), which supports the idea that Western thought is more categorical.

##### Conclusion: Words as Reflections

The way we use function words is a **reflection** of our underlying psychological state, not a cause of it25. The authors tried to change people's feelings by forcing them to use different pronouns in experiments, but it didn't work26. This means you can't just say "we" more to feel more connected; rather, feeling connected causes you to naturally say "we" more.

In short, these tiny "junk words" are a window into our minds, revealing everything from our emotional state and social status to how honest we're being.

### Quiz 2

#### "Banned In Germany, Kids' Doll Is Labeled An 'Espionage Device'" (NPR)ew

This article discusses the "My Friend Cayla" doll, which was banned by German regulators. The key issues were:

-   **Espionage é—´è°Concerns:** The doll was classified as an "illegal espionage apparatus" because its insecure Bluetooth connection allowed anyone within range to potentially listen in on and even speak to the child playing with it.
-   **Data Privacy:** The doll recorded children's conversations and sent them to a U.S.-based company that specializes in voice recognition. This was done without clear disclosure or parental consent, raising significant privacy concerns.
-   **Hidden Marketing:** The doll was pre-programmed with phrases that endorsed Disney products, essentially acting as a hidden marketing tool aimed at children.
-   **Lack of Security:** The fundamental design of the toy lacked basic security measures, making it vulnerable to hacking and unauthorized access.



#### "â€˜I love you too!â€™ My familyâ€™s creepy, unsettling week with an AI toy" (The Guardian)

This article provides a more recent, personal perspective on living with a modern AI-powered toy. The important points include:

-   **Emotional Attachment:** The child in the article quickly formed a strong emotional bond with the AI toy, "Grem," telling it "I love you" and treating it as a constant companion. The toy's reciprocal and effusive declarations of love were unsettling for the parents.
-   **Constant Surveillance:** The toy is "always listening" unless manually turned off, creating a sense of constant surveillance within the home and raising privacy concerns for the family.
-   **Influence on Behavior:** The article touches on how the AI toy's personalityâ€”designed to be agreeable and complimentaryâ€”could potentially shape a child's expectations and interactions in the real world.
-   **The Digital Divide and Developmental Impact:** The author and other experts raise broader questions about the impact of such toys on child development, social skills, and the disparity between children who have access to these "educational" AI companions and those who do not.

In summary, both articles highlight the potential dangers and ethical dilemmas presented by internet-connected and AI-powered toys, ranging from concrete security and privacy violations to more subtle concerns about their impact on children's emotional development and family life.









