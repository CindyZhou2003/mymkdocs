# ECE498/598 Deep Generative Models

Normal distribution 正太分布

In probability theory and statistics, a **normal distribution** or Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is
$$
f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\\
$$
$\text{Notation: }{\displaystyle {\mathcal {N}}(\mu ,\sigma ^{2})}\\$
$f(x)=\text{probability density function}\\$
$\sigma	=\text{standard deviation}\\$
$\mu	=\text{mean}$



VAE(Variational Autoencoder)

Latent space models: VAE(Variational Autoencoder)

All images have a latent set of rules $z∈R^K$,$k≤N$ from which they are generated.

Learning the distribution over the rules $q_{\alpha}(z)$ is all we need.

- z is the hidden rule
- We can sample from that distribution $q_{\alpha}(z)$ and generate images based on those sampled rules.

conditional distribution: $q_{\alpha}(z=\text{rules}|x=\text{chess board images})$

- decode rules into images

encoder: rule generator

GAN()

- Learn transformation from white noise to real images.Instead train model A to synthesize real-looking images … Train model B to discriminate that model A is generating fake images.Train both models A and B jointly.

Diffusion

- Let’s transform given data points progressively to Gaussian noise …Then, learn how to remove the noise to get back to the data points. Once trained, this model will be able to transform samples from Gaussian to samples of data.

Flows

- No need to make noise and then denoise … Think of the problem through the lens of ODE / SDEWe want to learn a vector field so that initial points (from a Gaussian) …will follow a trajectory and end up at points on the data distribution.



## Probability Review

**Likelihood**: you know this distribution, so easy to calculate

**Posterior**: the opposite is a known distribution, but this is a bit harder

|            |                                            |                                 |
| ---------- | ------------------------------------------ | ------------------------------- |
| Likelihood | p(data    \| distribution parameters    )  | Tractable, well defined         |
| Posterior  | p (distribution parameters    \| data    ) | Typical goal, tricky, try Bayes |
| Prior      | p( distribution parameters   )             | Domain knowledge                |
| Evidence   | p(data)                                    | Approximate with large data     |

Maximum likelihood Estimation(MLE)

- Slide the bell curve towards the position that maximizes the probability of all data points.
- 为了求解 θargmaxL(θ), 我们需要解方程: δθδL(θ)=0



Expectation Maximization (EM)



