# CS 498 Machine Learning System

>   [!NOTE]
>
>   TA: <img src="./assets/image-20260127153611443.png" alt="image-20260127153611443" style="zoom: 33%;" />

DL computation

<img src="./assets/image-20260122153534052.png" alt="image-20260122153534052" style="zoom:33%;" />

Transformer=attention+ MLPs

Recurrent Neural Networks

-   LSTM

Two Key Problems of RNNs

-   


Attention: 



GNN Architecture

MoE

Stochastic Gradient Descent (SGD)

These slides function as an introductory roadmap for a course on **Machine Learning Systems**. Since you are new to this, think of this field as the intersection of "math" (the models) and "computer engineering" (how to run that math efficiently on hardware).



The content is divided into three main pillars: **Models** (the workloads), **Optimization** (how they learn), and **Frameworks** (the software tools like PyTorch) .



## Deep Learning Workload

### The "Ingredients" of Deep Learning

1.  **Data:** The examples you teach the computer with (Images, Text, Audio, etc.).
2.  **Model:** The mathematical structure that learns patterns (CNNs, Transformers, etc.).
3.  **Compute:** The hardware that does the math (CPUs, GPUs).

**The Learning Process (Propagation 传播)** :

-   **Forward Propagation:** The data goes *into* the model, passes through layers, and the model makes a guess (e.g., "Is this a Cat or a dog?").
-   **Backward Propagation:** The system checks if the guess was wrong (Loss). It then sends a signal *backward* through the model to adjust the internal dials (parameters/weights) so it makes a better guess next time.

### Model Architectures

#### 1. Convolutional Neural Networks (CNNs)

-   **Best for:** Images (Computer Vision).
-   **How they work:** Imagine looking at a picture through a small square window (a filter) and sliding it across the image. This enables the model to identify local features, such as edges, corners, or textures.
-   **Key Models mentioned:**
    -   **ResNet:** Uses "identity connections" (shortcuts) to help train very deep networks without getting stuck.
    -   **U-Net:** Used for "segmentation"分割 (outlining exactly where an object is in an image).

#### 2. Recurrent Neural Networks (RNNs)

-   **Best for:** Sequences 序列 (Text, Audio) where order matters.
-   **How they work:** They process data one step at a time (like reading a sentence word-by-word), keeping an internal "memory" or state of what they have seen so far.
-   **The Problem:** They are *slow* because they can't do everything at once (lack of parallelizability), and they tend to forget things if the sequence is too long.
-   **Key Models:** **LSTM** and **GRU** were invented to help fix the "forgetting" problem. 解决遗忘问题

#### 3. Transformers

-   **Best for:** Modern Text processing 文本处理 (e.g., ChatGPT, BERT).
-   **The Big Idea ("Attention"):** Instead of reading word-by-word like an RNN, Transformers use **Attention**. This allows the model to look at *all* words in a sentence simultaneously and figure out which words relate to each other (e.g., connecting "it" to "the cat").
-   **System Benefit:** Because they look at everything at once, they are "massively parallelizable," meaning they run very fast on modern hardware (GPUs) compared to RNNs.

#### 4. Graph Neural Networks (GNNs)

-   **Best for:** Network data (Social networks, molecular structures).
-   **How they work:** They predict properties of a node (e.g., a person) by aggregating information from their neighbors (e.g., their friends).

#### 5. Mixture-of-Experts (MoE) 专家混合

-   **The Idea:** Instead of one giant model doing everything, you have many smaller "expert" models. For every input, a "Router" decides which experts are best suited to handle the problem.

### Optimization (How the Model Learns)

Optimization is the math used to minimize the model's errors (Loss).

**1. Gradient Descent (The Basic Way)**: Imagine you are standing on a misty mountain and want to get to the bottom. You look at the slope under your feet and take a step downhill. This is **Stochastic Gradient Descent (SGD)**. 随机梯度下降

-   **Problem:** If you use small batches of data, your path is "noisy" (jittery), and you might get stuck in small valleys (local minima).

**2. Momentum** 动量: To fix the jitter, you add "Momentum." Just like a heavy ball rolling down a hill builds up speed and isn't easily deflected by small bumps, this method remembers the direction it was going and keeps moving that way.

SGD+Momentum

**3. Adaptive Methods 自适应方法 (RMSProp & Adam)**

Sometimes you need to take big steps, and sometimes small steps.

-   **RMSProp:** Adapts the step size (learning rate) based on how steep the terrain is.
-   **Adam:** The "gold standard" today. It combines Momentum (velocity) and RMSProp (adaptive steps) to learn very efficiently. Adam is a kind of optimizer.



learning rate: decay over time

Dataflow graph representation

### Frameworks (The Software)

Finally, the slides discuss the tools we use to program these models, like **TensorFlow** and **PyTorch**.

**Computational Graphs**: Deep Learning frameworks represent your code as a "graph" where circles are math operations (add, multiply) and arrows are data flowing between them.

**TensorFlow vs. PyTorch**

-   **Symbolic 符号化 (TensorFlow v1):** You define the *entire* graph structure first, then run it. It's harder to debug but easier for the computer to optimize (make faster) .
-   **Imperative 命令式 (PyTorch):** You run the math line by line, just like Python. It is flexible and easy to debug, but historically harder to optimize automatically.

**The Modern Solution: Just-in-Time (JIT) Compilation,** New tools like `Torch. compile` (PyTorch 2.0), try to give you the best of both worlds: the ease of Python with the speed of optimized graphs.



## Transformers AI

### Architecture

**Tokenization & Embeddings:** The process starts by splitting text into individual pieces (tokens) and converting them into vector representations (embeddings) .

**Positional Encodings 位置编码:** Since Transformers process words in parallel rather than sequentially, they require "positional encodings" to understand word order.

-   The token embeddings and the absolute position embedding are added together element-wise.

-   **Absolute Positional Encoding:** The original method adds a fixed vector to the embedding to represent position.
-   **Relative Position Encoding:** A more modern approach that focuses on the distance between words (e.g., "my dog" matters more than where "dog" appears in the sentence).
-   **Rotary Position Embedding (RoPE):** A standard in recent large language models (LLMs) that uses rotation matrices to encode position.

### Self-Attention

**Query, Key, Value (Q, K, V):** The lecture uses a database analogy where a **Query** searches for information, **Keys** identify objects, and **Values** contain the actual content.

Self-Attention Steps:

-   Step 1: compute “key, value, query” embedding for each input token
-   Step 2: compute scores between pairs of tokens(dot product of Q and K)
-   Step 3: compute normalized attention scores(Softmax)
-   Step 4: get a new representation by the weighted sum of values

**Multi-Head Attention:** Running multiple attention calculations in parallel allows the model to capture different types of relationships between words simultaneously.

###  Encoder vs. Decoder

-   **Encoder:** Its goal is to "add context" to the input embeddings, transforming them into contextually rich representations.
-   **Decoder:** Used for generation (like translation or text completion). It uses masked attention to ensure it generates one token at a time without "cheating" by looking ahead.

### Arithmetic Intensity & Performance

A significant portion of the lecture focuses on analyzing the hardware efficiency of these models.

-   **Arithmetic Intensity (AI):** Defined as the ratio of floating-point operations (FLOPs) to memory operations (Bytes). It measures how much "work" the processor does for every byte of data it moves.
    -   Formula: $AI = \frac{\#ops\text{(compute)}}{\#bytes\text{(memory bandwidth)}}$.
-   **The Roofline Model:** A visual model used to identify if a program is **Compute-bound** (limited by processor speed) or **Memory Bandwidth-bound** (limited by data transfer speed) .
    -   High AI leads to compute-bound performance (good).
    -   Low AI leads to bandwidth-bound performance (bad, processor sits idle waiting for data).
-   **Optimization Example:** The slides demonstrate that "fusing" operations 融合运算 (combining addition and multiplication into one loop 加法乘法合并) increases Arithmetic Intensity by reducing the number of memory reads/writes required for the same amount of math.

### Transformer Performance Analysis

The lecture concludes by applying Arithmetic Intensity calculations specifically to Transformers.

-   **Memory vs. Compute:** Different layers (Linear vs. Attention) have different costs. For example, the Feed-Forward Network involves significantly more FLOPs ($16bsh^2$) compared to other operations.

    **Batch Size Impact:** Increasing batch size generally improves throughput (tokens/second) by moving the workload from being memory-bound to compute-bound . 批处理将内存受限转化为计算受限

