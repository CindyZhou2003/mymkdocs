
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="CindyÁöÑÁ¨îËÆ∞Êú¨">
      
      
      
        <link rel="canonical" href="https://cindyzhou2003.github.io/mymkdocs/UIUC/ECE448.html">
      
      
        <link rel="prev" href="ECE408.html">
      
      
        <link rel="next" href="ECE598.html">
      
      
        
      
      
      <link rel="icon" href="../assets/images/favcion.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>ECE448/CS440 Artificial Intelligence - CindyÁöÑÁ¨îËÆ∞Êú¨</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=LXGW+WenKai:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"LXGW WenKai";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css">
    
      <link rel="stylesheet" href="../css/extra.css">
    
      <link rel="stylesheet" href="../css/neoteroi-mkdocs.css">
    
      <link rel="stylesheet" href="../css/style.css">
    
      <link rel="stylesheet" href="../css/font.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="forest" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ece448cs440-artificial-intelligence" class="md-skip">
          Ë∑≥ËΩ¨Ëá≥
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="È°µÁúâ">
    <a href=".." title="CindyÁöÑÁ¨îËÆ∞Êú¨" class="md-header__button md-logo" aria-label="CindyÁöÑÁ¨îËÆ∞Êú¨" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M384 512H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h304c26.5 0 48 21.5 48 48v288c0 20.9-13.4 38.7-32 45.3V448c17.7 0 32 14.3 32 32s-14.3 32-32 32zM96 384c-17.7 0-32 14.3-32 32s14.3 32 32 32h256v-64zm32-232c0 13.3 10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24H152c-13.3 0-24 10.7-24 24m24 72c-13.3 0-24 10.7-24 24s10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CindyÁöÑÁ¨îËÆ∞Êú¨
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ECE448/CS440 Artificial Intelligence
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="forest" data-md-color-primary="teal" data-md-color-accent="teal"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="forest-dark" data-md-color-primary="Brown" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="ÊêúÁ¥¢" placeholder="ÊêúÁ¥¢" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Êü•Êâæ">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="ÂàÜ‰∫´" aria-label="ÂàÜ‰∫´" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Ê∏ÖÁ©∫ÂΩìÂâçÂÜÖÂÆπ" aria-label="Ê∏ÖÁ©∫ÂΩìÂâçÂÜÖÂÆπ" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Ê≠£Âú®ÂàùÂßãÂåñÊêúÁ¥¢ÂºïÊìé
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/CindyZhou2003/mymkdocs" title="ÂâçÂæÄ‰ªìÂ∫ì" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    Cindy's notebook
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Ê†áÁ≠æ" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../index.html" class="md-tabs__link">
          
  
  
    
  
  ‰∏ªÈ°µü§ñ

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../ZJU/DM.html" class="md-tabs__link">
          
  
  
    
  
  ZJU-SEËØæÁ®ãüìù

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
  
  
    
    
      
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="ECE408.html" class="md-tabs__link">
          
  
  
    
  
  UIUC-MEng coursesüìö

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../skills/Algorithm.html" class="md-tabs__link">
          
  
  
    
  
  ÊäÄËÉΩ‚öôÔ∏è

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../abroad/IELTs_speaking1.html" class="md-tabs__link">
          
  
  
    
  
  Âá∫ÂõΩ‚úàÔ∏è

        </a>
      </li>
    
  

    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../work/Interview.html" class="md-tabs__link">
          
  
  
    
  
  ÊâæÂ∑•

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../other/test.html" class="md-tabs__link">
          
  
  
    
  
  ÂÖ∂‰ªñüéä

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="ÂØºËà™Ê†è" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="CindyÁöÑÁ¨îËÆ∞Êú¨" class="md-nav__button md-logo" aria-label="CindyÁöÑÁ¨îËÆ∞Êú¨" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M384 512H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h304c26.5 0 48 21.5 48 48v288c0 20.9-13.4 38.7-32 45.3V448c17.7 0 32 14.3 32 32s-14.3 32-32 32zM96 384c-17.7 0-32 14.3-32 32s14.3 32 32 32h256v-64zm32-232c0 13.3 10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24H152c-13.3 0-24 10.7-24 24m24 72c-13.3 0-24 10.7-24 24s10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24z"/></svg>

    </a>
    CindyÁöÑÁ¨îËÆ∞Êú¨
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/CindyZhou2003/mymkdocs" title="ÂâçÂæÄ‰ªìÂ∫ì" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    Cindy's notebook
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../index.html" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    ‰∏ªÈ°µü§ñ
  

    
  </span>
  
  

            </a>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    ‰∏ªÈ°µü§ñ
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    ZJU-SEËØæÁ®ãüìù
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    ZJU-SEËØæÁ®ãüìù
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Â§ß‰∏ÄÊò•Â§è
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Â§ß‰∏ÄÊò•Â§è
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/DM.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Á¶ªÊï£Êï∞Â≠¶ÂèäÂÖ∂Â∫îÁî®
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Â§ß‰∫åÁßãÂÜ¨
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Â§ß‰∫åÁßãÂÜ¨
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/OOP.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Èù¢ÂêëÂØπË±°Á®ãÂ∫èËÆæËÆ°
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/FDS.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Êï∞ÊçÆÁªìÊûÑÂü∫Á°Ä
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Â§ß‰∫åÊò•Â§è
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Â§ß‰∫åÊò•Â§è
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/ADS.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    È´òÁ∫ßÊï∞ÊçÆÁªìÊûÑ‰∏éÁÆóÊ≥ïÂàÜÊûê
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/DB.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Êï∞ÊçÆÂ∫ìÁ≥ªÁªü
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/ML.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Êú∫Âô®Â≠¶‰π†ÔºàÂõΩÈôÖÂåñÔºâ
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/PIS.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ‰ø°ÊÅØÂÆâÂÖ®ÂéüÁêÜ
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_4" >
        
          
          <label class="md-nav__link" for="__nav_2_4" id="__nav_2_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Â§ß‰∏âÁßãÂÜ¨
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Â§ß‰∏âÁßãÂÜ¨
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/OS.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Êìç‰ΩúÁ≥ªÁªü
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/CN.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ËÆ°ÁÆóÊú∫ÁΩëÁªú
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/%E6%AF%9B%E6%A6%82.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ÊØõÊ¶Ç
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/IoT.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Áâ©ËÅîÁΩëÊäÄÊúØÂü∫Á°Ä‰∏éÂ∫îÁî®ÂºÄÂèë
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/SQAT.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ËΩØ‰ª∂Ë¥®Èáè‰∏é‰øùËØÅÊµãËØï
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_5" >
        
          
          <label class="md-nav__link" for="__nav_2_5" id="__nav_2_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Â§ß‰∏âÊò•Â§è
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Â§ß‰∏âÊò•Â§è
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/CLDF.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Êï∞Â≠óÈÄªËæëËÆæËÆ°
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ZJU/%E4%B9%A0%E6%A6%82.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ‰π†Ê¶Ç
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    UIUC-MEng coursesüìö
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    UIUC-MEng coursesüìö
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Fall 2025
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    Fall 2025
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="ECE408.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ECE408/CS483 Applied Parallel Programming
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    ECE448/CS440 Artificial Intelligence
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="ECE448.html" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    ECE448/CS440 Artificial Intelligence
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="ÁõÆÂΩï">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      ÁõÆÂΩï
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-and-other-trivia" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical and other trivia
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probability" class="md-nav__link">
    <span class="md-ellipsis">
      
        Probability
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modelling-text-data" class="md-nav__link">
    <span class="md-ellipsis">
      
        Modelling Text Data
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#testing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Testing
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#naive-bayes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Naive Bayes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Naive Bayes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-definitions-and-mathematical-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic definitions and mathematical model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applying-naive-bayes-to-text-classification" class="md-nav__link">
    <span class="md-ellipsis">
      
        Applying Naive Bayes to text classification
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#search" class="md-nav__link">
    <span class="md-ellipsis">
      
        Search
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example tasks
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-search-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic search methods
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#outline-for-search-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Outline for search algorithms
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-search-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Search Algorithms
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        A* Search
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="A* Search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#heuristics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Heuristics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#suboptimal-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        Suboptimal Search Ê¨°‰ºòÊêúÁ¥¢
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-each-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Details for Each Algorithm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#more-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        More Search
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robotics-and-configuration-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        Robotics and Configuration Space
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Robotics and Configuration Space">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#history" class="md-nav__link">
    <span class="md-ellipsis">
      
        History
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-knowledge" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Knowledge
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#natural-language" class="md-nav__link">
    <span class="md-ellipsis">
      
        Natural Language
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Natural Language">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#natural-language-and-hmm-history" class="md-nav__link">
    <span class="md-ellipsis">
      
        Natural Language and HMM History
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#outline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Outline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#types-of-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Types of systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#processing-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Processing pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pos-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      
        POS Tagging
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hmms" class="md-nav__link">
    <span class="md-ellipsis">
      
        HMMs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HMMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#viterbi-trellis-decoding-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Viterbi (trellis Ê£öÊû∂) decoding algorithm Áª¥ÁâπÊØîÁÆóÊ≥ï
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computer-vision" class="md-nav__link">
    <span class="md-ellipsis">
      
        Computer Vision
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Computer Vision">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#applications-of-computer-vision" class="md-nav__link">
    <span class="md-ellipsis">
      
        Applications of Computer Vision
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-formation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Image formation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classifiers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classifiers
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Classifiers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#important-names-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Important Names &amp; Datasets
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-design" class="md-nav__link">
    <span class="md-ellipsis">
      
        General design
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k-nn-and-decision-trees" class="md-nav__link">
    <span class="md-ellipsis">
      
        K-nn and Decision Trees
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="K-nn and Decision Trees">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#k-nearest-neighbors" class="md-nav__link">
    <span class="md-ellipsis">
      
        k-nearest neighbors
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#perceptrons" class="md-nav__link">
    <span class="md-ellipsis">
      
        Perceptrons ÊÑüÁü•Êú∫
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#neural-nets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Neural Nets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Neural Nets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-figures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical Figures
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear-classifiers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Linear Classifiers
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-nets_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Neural Nets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Neural Nets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#three-challenges-in-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Three challenges in training
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convolutional-neural-networks-cnns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Convolutional Neural Networks (CNNs)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generative-adversarial" class="md-nav__link">
    <span class="md-ellipsis">
      
        Generative &amp; Adversarial
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vector-semantics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vector Semantics ÂêëÈáèËØ≠‰πâ
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vector Semantics ÂêëÈáèËØ≠‰πâ">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-figures_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical Figures
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word-meaning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Word meaning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequential-neural-nets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sequential Neural Nets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Sequential Neural Nets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#byte-pair-encoding-bpe" class="md-nav__link">
    <span class="md-ellipsis">
      
        Byte-pair encoding (BPE)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìà Word2vec
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sequential-neural-nets_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sequential Neural Nets
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#historical-names-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical Names (Transformers)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert-bidirectional-encoder-representations-from-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        BERT (Bidirectional Encoder Representations from Transformers)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-autoregressive-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example Autoregressive LLMs
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#language-modeling-fundamentals" class="md-nav__link">
    <span class="md-ellipsis">
      
        Language Modeling Fundamentals
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Language Modeling Fundamentals">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#n-gram-language-model-folks" class="md-nav__link">
    <span class="md-ellipsis">
      
        N-gram Language Model Folks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#input-and-output" class="md-nav__link">
    <span class="md-ellipsis">
      
        Input and Output
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recurrent-neural-networks-rnns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Recurrent Neural Networks (RNNs) Âæ™ÁéØÁ•ûÁªèÁΩëÁªú
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Recurrent Neural Networks (RNNs) Âæ™ÁéØÁ•ûÁªèÁΩëÁªú">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#high-level-view-of-how-they-work" class="md-nav__link">
    <span class="md-ellipsis">
      
        High-Level View of How They Work
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-compute-loss" class="md-nav__link">
    <span class="md-ellipsis">
      
        When to Compute Loss
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bidirectional-rnn-birnn" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bidirectional RNN (BiRNN)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gated-rnn-eg-lstm-gru" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gated RNN (e.g., LSTM, GRU)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#encoder-decoder-and-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Encoder-Decoder and Attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Encoder-Decoder and Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#encoder-decoder-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        Encoder-Decoder Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-input-to-each-step-in-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Input to Each Step in Decoder?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#teacher-forcing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Teacher Forcing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer-blocks-and-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer Blocks and LLMs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transformer Blocks and LLMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformer-blocks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer Blocks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#residual-connections" class="md-nav__link">
    <span class="md-ellipsis">
      
        Residual Connections
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llms-masked-vs-autoregressive" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLMs: Masked vs. Autoregressive
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pre-training-fine-tuning-task-head" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pre-training, Fine-tuning, Task Head
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-training-autoregressive-model-generative-pre-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Self-training Autoregressive Model (Generative Pre-training)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-autoregressive-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Using Autoregressive Model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompt-engineering" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prompt Engineering
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#approximate-scale-of-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Approximate Scale of LLMs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#current-limitations-of-llms-and-testing-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Current Limitations of LLMs and Testing LLMs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-collapse" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model Collapse
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#markov-decision-processes-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Markov Decision Processes (MDPs)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Markov Decision Processes (MDPs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-and-terminology-for-an-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model and Terminology for an MDP
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quantized-representation-of-continuous-state-variables" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quantized Representation of Continuous State Variables
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-equation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bellman Equation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods-of-solving-the-bellman-equation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Methods of Solving the Bellman Equation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-to-choose-a-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        How to Choose a Policy?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-rl" class="md-nav__link">
    <span class="md-ellipsis">
      
        üéØ Reinforcement Learning (RL)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üéØ Reinforcement Learning (RL)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-setup-for-reinforcement-learning-main-loop" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic Setup for Reinforcement Learning (Main Loop)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-based-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model-based Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-free-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model-free Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q-learning-version-of-bellman-equation-q-learning-update-rule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Q-learning Version of Bellman Equation (Q-learning Update Rule)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#td-update-algorithm-temporal-difference-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        TD Update Algorithm (Temporal Difference Learning)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sarsa-update-algorithm-state-action-reward-state-action" class="md-nav__link">
    <span class="md-ellipsis">
      
        SARSA Update Algorithm (State-Action-Reward-State-Action)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-do-td-and-sarsa-differ" class="md-nav__link">
    <span class="md-ellipsis">
      
        How do TD and SARSA Differ?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#selecting-an-action" class="md-nav__link">
    <span class="md-ellipsis">
      
        Selecting an Action
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#incorporating-exploration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Incorporating Exploration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#online-learning-offline-learning-experience-replay" class="md-nav__link">
    <span class="md-ellipsis">
      
        Online Learning, Offline Learning, Experience Replay
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#constraint-satisfaction-problems-csps" class="md-nav__link">
    <span class="md-ellipsis">
      
        üßê Constraint Satisfaction Problems (CSPs)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üßê Constraint Satisfaction Problems (CSPs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#constraint-satisfaction-problems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Constraint Satisfaction Problems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#historical-trivia-and-key-examples" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical Trivia and Key Examples
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#search-algorithms-for-csps" class="md-nav__link">
    <span class="md-ellipsis">
      
        ‚õ∞Ô∏è Search Algorithms for CSPs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="‚õ∞Ô∏è Search Algorithms for CSPs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hill-climbing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hill-climbing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backtracking-search-dfs" class="md-nav__link">
    <span class="md-ellipsis">
      
        Backtracking Search (DFS)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heuristics-for-variable-and-value-selection" class="md-nav__link">
    <span class="md-ellipsis">
      
        Heuristics for Variable and Value Selection
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-checking-constraint-propagation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Forward Checking, Constraint Propagation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ac-3-algorithm-arc-consistency-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        AC-3 Algorithm (Arc Consistency Algorithm)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-to-incorporate-constraint-propagation-into-backtracking-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        How to Incorporate Constraint Propagation into Backtracking Search
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Planning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Planning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-historical-figures-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Historical Figures &amp; Representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-approaches-to-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Approaches to Planning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-objects-environment" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Objects &amp; Environment
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Algorithms
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#game-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        Game Search
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Game Search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-game-tree-basics" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Game Tree Basics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-minimax-alpha-beta" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Minimax &amp; Alpha-Beta
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-optimizations-the-horizon-issues" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Optimizations (The "Horizon" Issues)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-recent-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Recent Algorithms
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayes-nets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayes Nets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayes Nets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-structure" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Structure
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-independence-geometry" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Independence &amp; Geometry
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-size-construction" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Size &amp; Construction
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-inference" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Inference
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#readings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Readings
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Readings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quiz-1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 1
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#an-algorithm-for-suffix-stripping" class="md-nav__link">
    <span class="md-ellipsis">
      
        An algorithm for suffix stripping ÂêéÁºÄÂâ•Á¶ª
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-psychological-functions-of-function-words" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Psychological Functions of Function Words
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 2
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#banned-in-germany-kids-doll-is-labeled-an-espionage-device-nprew" class="md-nav__link">
    <span class="md-ellipsis">
      
        "Banned In Germany, Kids' Doll Is Labeled An 'Espionage Device'" (NPR)ew
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i-love-you-too-my-familys-creepy-unsettling-week-with-an-ai-toy-the-guardian" class="md-nav__link">
    <span class="md-ellipsis">
      
        "‚ÄòI love you too!‚Äô My family‚Äôs creepy, unsettling week with an AI toy" (The Guardian)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 3
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#summary-of-automatic-labeling-of-semantic-roles" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary of "Automatic Labeling of Semantic Roles"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-of-open-source-ai-models-show-gender-bias-in-hiring" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary of "Open source AI models show gender bias in hiring"
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 4
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 4">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#paper-1-recognition-using-visual-phrases" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìÑ Paper 1: "Recognition Using Visual Phrases"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#papers-2-3-ai-usage-in-scientific-publishing" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìÑ Papers 2 &amp; 3: AI Usage in Scientific Publishing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ai-chatbots-have-thoroughly-infiltrated-scientific-publishing" class="md-nav__link">
    <span class="md-ellipsis">
      
        AI Chatbots Have Thoroughly Infiltrated Scientific Publishing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#some-scientists-cant-stop-using-ai-to-write-research-papers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Some scientists can't stop using AI to write research papers
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-5" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 5
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 5">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìÑ"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#important-facts-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìä Important Facts &amp; Statistics:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-2-context-encoders-feature-learning-by-inpainting" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìÑ Summary 2: "Context Encoders: Feature Learning by Inpainting"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#important-facts-statistics_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìä Important Facts &amp; Statistics:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-6" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 6
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 6">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#state-of-what-art-a-call-for-multi-prompt-llm-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        State of What Art? A Call for Multi-Prompt LLM Evaluation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenegen-learning-to-generate-realistic-traffic-scenes" class="md-nav__link">
    <span class="md-ellipsis">
      
        SceneGen: Learning to Generate Realistic Traffic Scenes
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#final" class="md-nav__link">
    <span class="md-ellipsis">
      
        Final
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Final">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-orca-benchmark-evaluating-real-world-calculation-accuracy-in-large-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#faster-sorting-algorithms-discovered-using-deep-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Faster sorting algorithms discovered using deep reinforcement learning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="ECE598.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ECE498/598 Deep Generative Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Spring 2026
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Spring 2026
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="CS498.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CS498 Machine Learning Systems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="ECE478.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CS477/ECE478 Formal Software Development Methods
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="ECE544.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ECE544 Pattern Recognition
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    ÊäÄËÉΩ‚öôÔ∏è
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    ÊäÄËÉΩ‚öôÔ∏è
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../skills/Algorithm.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ÁÆóÊ≥ï
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../skills/deeplearning.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ê∑±Â∫¶Â≠¶‰π†
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Âá∫ÂõΩ‚úàÔ∏è
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Âá∫ÂõΩ‚úàÔ∏è
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5_1" >
        
          
          <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    ÈõÖÊÄùÂè£ËØ≠
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_1">
            <span class="md-nav__icon md-icon"></span>
            
  
    ÈõÖÊÄùÂè£ËØ≠
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../abroad/IELTs_speaking1.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Á¨¨‰∏ÄÊ¨°
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../abroad/IELTs_speaking2.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Á¨¨‰∫åÊ¨°
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../abroad/IELTs_writing.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ÈõÖÊÄù‰ΩúÊñá
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    ÊâæÂ∑•
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    ÊâæÂ∑•
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../work/Interview.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Èù¢ËØï
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    ÂÖ∂‰ªñüéä
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    ÂÖ∂‰ªñüéä
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../other/test.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Test
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../other/Debug_issue.html" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ÈÉ®ÁΩ≤ÁΩëÁ´ôÊó∂ÁöÑÈóÆÈ¢ò
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="ÁõÆÂΩï">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      ÁõÆÂΩï
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-and-other-trivia" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical and other trivia
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#probability" class="md-nav__link">
    <span class="md-ellipsis">
      
        Probability
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modelling-text-data" class="md-nav__link">
    <span class="md-ellipsis">
      
        Modelling Text Data
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#testing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Testing
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#naive-bayes" class="md-nav__link">
    <span class="md-ellipsis">
      
        Naive Bayes
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Naive Bayes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-definitions-and-mathematical-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic definitions and mathematical model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applying-naive-bayes-to-text-classification" class="md-nav__link">
    <span class="md-ellipsis">
      
        Applying Naive Bayes to text classification
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#search" class="md-nav__link">
    <span class="md-ellipsis">
      
        Search
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example tasks
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Example tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-search-methods" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic search methods
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#outline-for-search-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Outline for search algorithms
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-search-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Search Algorithms
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#a-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        A* Search
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="A* Search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#heuristics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Heuristics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#suboptimal-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        Suboptimal Search Ê¨°‰ºòÊêúÁ¥¢
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#details-for-each-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Details for Each Algorithm
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#more-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        More Search
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#robotics-and-configuration-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        Robotics and Configuration Space
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Robotics and Configuration Space">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#history" class="md-nav__link">
    <span class="md-ellipsis">
      
        History
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-knowledge" class="md-nav__link">
    <span class="md-ellipsis">
      
        General Knowledge
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#natural-language" class="md-nav__link">
    <span class="md-ellipsis">
      
        Natural Language
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Natural Language">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#natural-language-and-hmm-history" class="md-nav__link">
    <span class="md-ellipsis">
      
        Natural Language and HMM History
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#outline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Outline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#types-of-systems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Types of systems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#processing-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        Processing pipeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pos-tagging" class="md-nav__link">
    <span class="md-ellipsis">
      
        POS Tagging
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hmms" class="md-nav__link">
    <span class="md-ellipsis">
      
        HMMs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="HMMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#viterbi-trellis-decoding-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        Viterbi (trellis Ê£öÊû∂) decoding algorithm Áª¥ÁâπÊØîÁÆóÊ≥ï
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computer-vision" class="md-nav__link">
    <span class="md-ellipsis">
      
        Computer Vision
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Computer Vision">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#applications-of-computer-vision" class="md-nav__link">
    <span class="md-ellipsis">
      
        Applications of Computer Vision
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-formation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Image formation
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#classifiers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classifiers
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Classifiers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#important-names-datasets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Important Names &amp; Datasets
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#general-design" class="md-nav__link">
    <span class="md-ellipsis">
      
        General design
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#k-nn-and-decision-trees" class="md-nav__link">
    <span class="md-ellipsis">
      
        K-nn and Decision Trees
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="K-nn and Decision Trees">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#k-nearest-neighbors" class="md-nav__link">
    <span class="md-ellipsis">
      
        k-nearest neighbors
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#perceptrons" class="md-nav__link">
    <span class="md-ellipsis">
      
        Perceptrons ÊÑüÁü•Êú∫
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#neural-nets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Neural Nets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Neural Nets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-figures" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical Figures
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear-classifiers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Linear Classifiers
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-nets_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Neural Nets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Neural Nets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#three-challenges-in-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Three challenges in training
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convolutional-neural-networks-cnns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Convolutional Neural Networks (CNNs)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generative-adversarial" class="md-nav__link">
    <span class="md-ellipsis">
      
        Generative &amp; Adversarial
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#vector-semantics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vector Semantics ÂêëÈáèËØ≠‰πâ
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vector Semantics ÂêëÈáèËØ≠‰πâ">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-figures_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical Figures
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word-meaning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Word meaning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sequential-neural-nets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sequential Neural Nets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Sequential Neural Nets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#byte-pair-encoding-bpe" class="md-nav__link">
    <span class="md-ellipsis">
      
        Byte-pair encoding (BPE)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìà Word2vec
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sequential-neural-nets_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Sequential Neural Nets
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#historical-names-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical Names (Transformers)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert-bidirectional-encoder-representations-from-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      
        BERT (Bidirectional Encoder Representations from Transformers)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#example-autoregressive-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Example Autoregressive LLMs
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#language-modeling-fundamentals" class="md-nav__link">
    <span class="md-ellipsis">
      
        Language Modeling Fundamentals
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Language Modeling Fundamentals">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#n-gram-language-model-folks" class="md-nav__link">
    <span class="md-ellipsis">
      
        N-gram Language Model Folks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#input-and-output" class="md-nav__link">
    <span class="md-ellipsis">
      
        Input and Output
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recurrent-neural-networks-rnns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Recurrent Neural Networks (RNNs) Âæ™ÁéØÁ•ûÁªèÁΩëÁªú
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Recurrent Neural Networks (RNNs) Âæ™ÁéØÁ•ûÁªèÁΩëÁªú">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#high-level-view-of-how-they-work" class="md-nav__link">
    <span class="md-ellipsis">
      
        High-Level View of How They Work
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#when-to-compute-loss" class="md-nav__link">
    <span class="md-ellipsis">
      
        When to Compute Loss
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bidirectional-rnn-birnn" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bidirectional RNN (BiRNN)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gated-rnn-eg-lstm-gru" class="md-nav__link">
    <span class="md-ellipsis">
      
        Gated RNN (e.g., LSTM, GRU)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#encoder-decoder-and-attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Encoder-Decoder and Attention
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Encoder-Decoder and Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#encoder-decoder-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      
        Encoder-Decoder Architecture
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-input-to-each-step-in-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Input to Each Step in Decoder?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#teacher-forcing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Teacher Forcing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#attention" class="md-nav__link">
    <span class="md-ellipsis">
      
        Attention
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#transformer-blocks-and-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer Blocks and LLMs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transformer Blocks and LLMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transformer-blocks" class="md-nav__link">
    <span class="md-ellipsis">
      
        Transformer Blocks
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#residual-connections" class="md-nav__link">
    <span class="md-ellipsis">
      
        Residual Connections
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llms-masked-vs-autoregressive" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLMs: Masked vs. Autoregressive
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pre-training-fine-tuning-task-head" class="md-nav__link">
    <span class="md-ellipsis">
      
        Pre-training, Fine-tuning, Task Head
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-training-autoregressive-model-generative-pre-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Self-training Autoregressive Model (Generative Pre-training)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#using-autoregressive-model" class="md-nav__link">
    <span class="md-ellipsis">
      
        Using Autoregressive Model
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#prompt-engineering" class="md-nav__link">
    <span class="md-ellipsis">
      
        Prompt Engineering
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#approximate-scale-of-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Approximate Scale of LLMs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#current-limitations-of-llms-and-testing-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        Current Limitations of LLMs and Testing LLMs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-collapse" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model Collapse
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#markov-decision-processes-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Markov Decision Processes (MDPs)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Markov Decision Processes (MDPs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-and-terminology-for-an-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model and Terminology for an MDP
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quantized-representation-of-continuous-state-variables" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quantized Representation of Continuous State Variables
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-equation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bellman Equation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods-of-solving-the-bellman-equation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Methods of Solving the Bellman Equation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-to-choose-a-policy" class="md-nav__link">
    <span class="md-ellipsis">
      
        How to Choose a Policy?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforcement-learning-rl" class="md-nav__link">
    <span class="md-ellipsis">
      
        üéØ Reinforcement Learning (RL)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üéØ Reinforcement Learning (RL)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#basic-setup-for-reinforcement-learning-main-loop" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic Setup for Reinforcement Learning (Main Loop)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-based-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model-based Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-free-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Model-free Reinforcement Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q-learning-version-of-bellman-equation-q-learning-update-rule" class="md-nav__link">
    <span class="md-ellipsis">
      
        Q-learning Version of Bellman Equation (Q-learning Update Rule)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#td-update-algorithm-temporal-difference-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        TD Update Algorithm (Temporal Difference Learning)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sarsa-update-algorithm-state-action-reward-state-action" class="md-nav__link">
    <span class="md-ellipsis">
      
        SARSA Update Algorithm (State-Action-Reward-State-Action)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-do-td-and-sarsa-differ" class="md-nav__link">
    <span class="md-ellipsis">
      
        How do TD and SARSA Differ?
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#selecting-an-action" class="md-nav__link">
    <span class="md-ellipsis">
      
        Selecting an Action
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#incorporating-exploration" class="md-nav__link">
    <span class="md-ellipsis">
      
        Incorporating Exploration
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#online-learning-offline-learning-experience-replay" class="md-nav__link">
    <span class="md-ellipsis">
      
        Online Learning, Offline Learning, Experience Replay
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#constraint-satisfaction-problems-csps" class="md-nav__link">
    <span class="md-ellipsis">
      
        üßê Constraint Satisfaction Problems (CSPs)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="üßê Constraint Satisfaction Problems (CSPs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#constraint-satisfaction-problems" class="md-nav__link">
    <span class="md-ellipsis">
      
        Constraint Satisfaction Problems
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#historical-trivia-and-key-examples" class="md-nav__link">
    <span class="md-ellipsis">
      
        Historical Trivia and Key Examples
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#search-algorithms-for-csps" class="md-nav__link">
    <span class="md-ellipsis">
      
        ‚õ∞Ô∏è Search Algorithms for CSPs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="‚õ∞Ô∏è Search Algorithms for CSPs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hill-climbing" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hill-climbing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backtracking-search-dfs" class="md-nav__link">
    <span class="md-ellipsis">
      
        Backtracking Search (DFS)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#heuristics-for-variable-and-value-selection" class="md-nav__link">
    <span class="md-ellipsis">
      
        Heuristics for Variable and Value Selection
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#forward-checking-constraint-propagation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Forward Checking, Constraint Propagation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ac-3-algorithm-arc-consistency-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      
        AC-3 Algorithm (Arc Consistency Algorithm)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#how-to-incorporate-constraint-propagation-into-backtracking-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        How to Incorporate Constraint Propagation into Backtracking Search
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Planning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Planning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-historical-figures-representation" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Historical Figures &amp; Representation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-approaches-to-planning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Approaches to Planning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-objects-environment" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Objects &amp; Environment
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Algorithms
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#game-search" class="md-nav__link">
    <span class="md-ellipsis">
      
        Game Search
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Game Search">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-game-tree-basics" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Game Tree Basics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-minimax-alpha-beta" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Minimax &amp; Alpha-Beta
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-optimizations-the-horizon-issues" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Optimizations (The "Horizon" Issues)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-recent-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Recent Algorithms
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayes-nets" class="md-nav__link">
    <span class="md-ellipsis">
      
        Bayes Nets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Bayes Nets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-structure" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Structure
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-independence-geometry" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Independence &amp; Geometry
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-size-construction" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Size &amp; Construction
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-inference" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Inference
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#readings" class="md-nav__link">
    <span class="md-ellipsis">
      
        Readings
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Readings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#quiz-1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 1
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#an-algorithm-for-suffix-stripping" class="md-nav__link">
    <span class="md-ellipsis">
      
        An algorithm for suffix stripping ÂêéÁºÄÂâ•Á¶ª
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-psychological-functions-of-function-words" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Psychological Functions of Function Words
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 2
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#banned-in-germany-kids-doll-is-labeled-an-espionage-device-nprew" class="md-nav__link">
    <span class="md-ellipsis">
      
        "Banned In Germany, Kids' Doll Is Labeled An 'Espionage Device'" (NPR)ew
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#i-love-you-too-my-familys-creepy-unsettling-week-with-an-ai-toy-the-guardian" class="md-nav__link">
    <span class="md-ellipsis">
      
        "‚ÄòI love you too!‚Äô My family‚Äôs creepy, unsettling week with an AI toy" (The Guardian)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 3
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#summary-of-automatic-labeling-of-semantic-roles" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary of "Automatic Labeling of Semantic Roles"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-of-open-source-ai-models-show-gender-bias-in-hiring" class="md-nav__link">
    <span class="md-ellipsis">
      
        Summary of "Open source AI models show gender bias in hiring"
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 4
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 4">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#paper-1-recognition-using-visual-phrases" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìÑ Paper 1: "Recognition Using Visual Phrases"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#papers-2-3-ai-usage-in-scientific-publishing" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìÑ Papers 2 &amp; 3: AI Usage in Scientific Publishing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ai-chatbots-have-thoroughly-infiltrated-scientific-publishing" class="md-nav__link">
    <span class="md-ellipsis">
      
        AI Chatbots Have Thoroughly Infiltrated Scientific Publishing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#some-scientists-cant-stop-using-ai-to-write-research-papers" class="md-nav__link">
    <span class="md-ellipsis">
      
        Some scientists can't stop using AI to write research papers
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-5" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 5
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 5">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìÑ"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#important-facts-statistics" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìä Important Facts &amp; Statistics:
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary-2-context-encoders-feature-learning-by-inpainting" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìÑ Summary 2: "Context Encoders: Feature Learning by Inpainting"
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#important-facts-statistics_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        üìä Important Facts &amp; Statistics:
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quiz-6" class="md-nav__link">
    <span class="md-ellipsis">
      
        Quiz 6
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Quiz 6">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#state-of-what-art-a-call-for-multi-prompt-llm-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      
        State of What Art? A Call for Multi-Prompt LLM Evaluation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenegen-learning-to-generate-realistic-traffic-scenes" class="md-nav__link">
    <span class="md-ellipsis">
      
        SceneGen: Learning to Generate Realistic Traffic Scenes
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#final" class="md-nav__link">
    <span class="md-ellipsis">
      
        Final
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Final">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-orca-benchmark-evaluating-real-world-calculation-accuracy-in-large-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#faster-sorting-algorithms-discovered-using-deep-reinforcement-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Faster sorting algorithms discovered using deep reinforcement learning
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="ece448cs440-artificial-intelligence">ECE448/CS440 Artificial Intelligence<a class="headerlink" href="#ece448cs440-artificial-intelligence" title="Permanent link">&para;</a></h1>
<div style="margin-top: -30px; font-size: 0.75em; opacity: 0.7;">
<p><span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"/></svg></span> Á∫¶ 23590 ‰∏™Â≠ó <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 17H7V3h14m0-2H7a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V3a2 2 0 0 0-2-2M3 5H1v16a2 2 0 0 0 2 2h16v-2H3m12.96-10.71-2.75 3.54-1.96-2.36L8.5 15h11z"/></svg></span> 4 Âº†ÂõæÁâá <span class="twemoji"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"/></svg></span> È¢ÑËÆ°ÈòÖËØªÊó∂Èó¥ 79 ÂàÜÈíü</p>
</div>
<p><a href="https://courses.grainger.illinois.edu/cs440/fa2025/">CS 440 Artificial Intelligence</a></p>
<p><a href="https://github.com/illinois-cs-coursework">https://github.com/illinois-cs-coursework</a></p>
<p><a href="https://courses.grainger.illinois.edu/cs440/fa2025/readings.html">https://courses.grainger.illinois.edu/cs440/fa2025/readings.html</a></p>
<blockquote>
<p>[!IMPORTANT]</p>
<p><a href="https://courses.grainger.illinois.edu/cs440/fa2025/lectures/probability-review.html">https://courses.grainger.illinois.edu/cs440/fa2025/lectures/probability-review.html</a>
</p>
</blockquote>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<blockquote>
<p><a href="https://courses.grainger.illinois.edu/cs440/fa2025/lectures/intro.html">https://courses.grainger.illinois.edu/cs440/fa2025/lectures/intro.html</a></p>
</blockquote>
<h3 id="historical-and-other-trivia">Historical and other trivia<a class="headerlink" href="#historical-and-other-trivia" title="Permanent link">&para;</a></h3>
<p>We've seen a lot of trivia, most of it not worth memorizing. The following items are the exceptions. Be able to explain (very briefly) what they are and (approximately) what time period they come from.</p>
<ul>
<li>
<p><strong>McCulloch and Pitts</strong></p>
<ul>
<li><strong>Time Period:</strong> 1940s</li>
<li><strong>Contribution:</strong> They introduced the <em>first mathematical model of a neural network</em>. Their work was foundational, proposing that networks of simple computational units (neurons) could perform complex logical operations. These were <em>theoretical models on paper</em>, as the hardware to implement them didn't exist yet.</li>
</ul>
</li>
<li>
<p><strong>Fred Jelinek</strong></p>
<ul>
<li><strong>Time Period:</strong> 1980s - 1990s</li>
<li><strong>Contribution:</strong> A key figure in <em>speech recognition</em>. He pioneered the use of statistical models, specifically <em>n-gram language models</em> and Hidden Markov Models (HMMs), which dramatically improved the accuracy and utility of speech recognition systems.</li>
</ul>
</li>
<li><strong>Pantel and Lin (SpamCop)</strong><ul>
<li><strong>Time Period:</strong> Late 1990s</li>
<li><strong>Contribution:</strong> They were pioneers in using <em>Naive Bayes classifiers for spam detection</em>. Their work showed that this statistical approach was highly effective for classifying emails, forming the basis of many modern spam filters.<ul>
<li>Êú¥Á¥†Ë¥ùÂè∂ÊñØÂûÉÂúæÈÇÆ‰ª∂ÂàÜÁ±ªÂô®</li>
</ul>
</li>
</ul>
</li>
<li><strong>Boulis and Ostendorf</strong><ul>
<li><strong>Time Period:</strong> Mid 2000s</li>
<li><strong>Contribution:</strong> They conducted research comparing the performance of Naive Bayes versus Support Vector Machine (SVM) classifiers for gender classification based on transcribed telephone conversations.</li>
<li><a href="http://www.aclweb.org/anthology/P05-1054">A Quantitative Analysis of Lexical Differences Between Genders in Telephone Conversations</a>, ACL 2005</li>
</ul>
</li>
<li><strong>The Plato System</strong><ul>
<li><strong>Time Period:</strong> Started in the 1960s</li>
<li><strong>Contribution:</strong> An early and influential <em>computer-assisted instruction system</em> developed at the University of Illinois. It was a precursor to modern <em>e-learning platforms and online communities</em>.</li>
</ul>
</li>
<li><strong>The Golem of Prague</strong><ul>
<li><strong>Time Period:</strong> 16<sup>th</sup>-century Jewish folklore</li>
<li><strong>Contribution:</strong> An early myth or story related to artificial intelligence. It tells of an artificial humanoid creature created from clay to protect the Jewish community. It represents an ancient human desire to create intelligent, autonomous beings.</li>
</ul>
</li>
</ul>
<h3 id="probability">Probability<a class="headerlink" href="#probability" title="Permanent link">&para;</a></h3>
<blockquote>
<p><a href="https://courses.grainger.illinois.edu/cs440/fa2025/lectures/probability-review.html">https://courses.grainger.illinois.edu/cs440/fa2025/lectures/probability-review.html</a></p>
</blockquote>
<p><strong>Random variables, axioms of probability:</strong></p>
<ul>
<li>A <strong>random variable</strong> is a variable whose value is a numerical outcome of a random phenomenon.</li>
<li>The <strong>axioms of probability</strong> (<strong>Kolmogorov's axioms of probability</strong> ÊüØÂ∞îËé´Âì•Ê¥õÂ§´Ê¶ÇÁéáÂÖ¨ÁêÜ) are fundamental rules:<ol>
<li>The probability of any event is non-negative.</li>
<li>The probability of the entire sample space (a certain event) is 1.</li>
<li>The probability of the union of mutually exclusive events is the sum of their individual probabilities. </li>
</ol>
</li>
</ul>
<p>$$
0 ‚â§P(A)\
P(True) = 1\
P(A|B) = P(A) + P(B), \text{if A and B are mutually exclusive events}
$$
-   <strong>Joint, marginal, conditional probability:</strong>
    -   <strong>Joint Probability</strong> ËÅîÂêàÊ¶ÇÁéá <span class="arithmatex">\(P(A,B)\)</span>: The probability of two events occurring together
    -   <strong>Marginal Probability</strong> ËæπÈôÖÊ¶ÇÁéá <span class="arithmatex">\(P(A)\)</span>: The probability of a single event occurring, irrespective of other events. It can be calculated by summing the joint probabilities over all outcomes of the other variable: <span class="arithmatex">\(P(A)=‚àë_BP(A,B)\)</span>
    -   <strong>Conditional Probability</strong> Êù°‰ª∂Ê¶ÇÁéá <span class="arithmatex">\(P(A‚à£B)\)</span>: The probability of event A occurring <em>given</em> that event B has already occurred. It is calculated as <span class="arithmatex">\(P(A | B) = \frac{P(A,B)}{P(B)}\)</span></p>
<h3 id="modelling-text-data"><strong>Modelling Text Data</strong><a class="headerlink" href="#modelling-text-data" title="Permanent link">&para;</a></h3>
<p><strong>Word types vs. word tokens:</strong>
-   <strong>Tokens:</strong> The total number of words in a document (e.g., "the cat sat on the mat" has 6 tokens). ÂçïËØçÊÄªÊï∞
-   <strong>Types:</strong> The number of <em>unique</em> words in a document (e.g., "the cat sat on the mat" has 5 types: "the", "cat", "sat", "on", "mat"). ËØçÂÖ∏Êù°ÁõÆÔºåÂîØ‰∏ÄÁöÑÂçïËØçÊï∞</p>
<p><strong>The Bag of Words model:</strong> We can use the <em>individual words as features</em>. A bag-of-words model determines the class of a document based on the <em>frequency</em> of occurrence of each word. It ignores the order in which words occur, which ones occur together, etc. So it will miss some details, e.g. the difference between "poisonous" and "not poisonous." ÂøΩÁï•ËØ≠Ê≥ïÁîöËá≥ËØçÂ∫è‰ΩÜ‰øùÊåÅÂ§öÊ†∑ÊÄß</p>
<p><strong>Bigrams, ngrams:</strong>
-   <strong>N-grams</strong> are contiguous sequences of <em>n</em> items (e.g., words, letters) from a given sample of text.
    -   <strong>N-gram</strong> ÊòØÊù•Ëá™ÁªôÂÆöÊñáÊú¨Ê†∑Êú¨ÁöÑ <em>n ‰∏™</em>È°πÁõÆÔºà‰æãÂ¶ÇÂçïËØç„ÄÅÂ≠óÊØçÔºâÁöÑËøûÁª≠Â∫èÂàó„ÄÇ
-   A <strong>bigram</strong> is a specific n-gram where n=2 (a two-word sequence). For example, in "the cat sat", the bigrams are "the cat" and "cat sat".
    -   ÁâπÂÆöÁöÑ n-gramÔºåÂÖ∂‰∏≠ n=2ÔºàÂç≥‰∏§‰∏™ÂçïËØçÁöÑÂ∫èÂàóÔºâ„ÄÇ‰æãÂ¶ÇÔºåÂú®‚Äúthe cat sat‚Äù‰∏≠Ôºå‰∫åÂÖÉËØ≠Ê≥ïÊòØ‚Äúthe cat‚ÄùÂíå‚Äúcat sat‚Äù„ÄÇ</p>
<p><strong>Data cleaning:</strong>
-   <strong>Tokenization:</strong> The process of splitting a stream of text into words, phrases, symbols, or other meaningful elements called tokens.
    -   <strong>Ê†áËÆ∞ÂåñÔºö</strong> Â∞ÜÊñáÊú¨ÊµÅÊãÜÂàÜ‰∏∫ÂçïËØç„ÄÅÁü≠ËØ≠„ÄÅÁ¨¶Âè∑ÊàñÂÖ∂‰ªñÊúâÊÑè‰πâÁöÑÂÖÉÁ¥†ÔºàÁß∞‰∏∫Ê†áËÆ∞ÔºâÁöÑËøáÁ®ã„ÄÇÂÆö‰πâÂçïËØçÂæóÂà∞ a clean string of words
    -   divide at whitespace  Âú®Á©∫ÁôΩÂ§ÑÂàíÂàÜ
    -   normalize punctuation, html tags, capitalization, etc ËßÑËåÉÊ†áÁÇπÁ¨¶Âè∑„ÄÅhtml Ê†áÁ≠æ„ÄÅÂ§ßÂÜôÂ≠óÊØçÁ≠â
    -   perhaps use a stemmer to remove word endings ‰ΩøÁî®ËØçÂπ≤ÂàÜÊûêÂô®Êù•Âà†Èô§ÂçïËØçÁªìÂ∞æ
-   <strong>Stemming ÂàÜËØç:</strong> The process of reducing inflected (or sometimes derived) words to their word stem, base or root form. <strong>Julie Lovins</strong> (1968) created one of the first stemming algorithms, and <strong>Martin Porter</strong> (1980) developed the Porter Stemmer, which is one of the most widely used.
    -   <strong>ËØçÂπ≤ÊèêÂèñÔºö</strong> Â∞ÜËØçÂΩ¢ÂèòÂåñÁöÑËØçÁÆÄÂåñ‰∏∫ËØçÂπ≤„ÄÅÂü∫ËØçÊàñËØçÊ†πÂΩ¢ÂºèÁöÑËøáÁ®ã„ÄÇJulie Lovins ÂàõÂª∫‰∫ÜÊúÄÊó©ÁöÑËØçÂπ≤ÊèêÂèñÁÆóÊ≥ï‰πã‰∏ÄÔºå Martin Porter ÂºÄÂèë‰∫Ü Porter ËØçÂπ≤ÊèêÂèñÂô®ÔºåÂÆÉÊòØÁõÆÂâç‰ΩøÁî®ÊúÄÂπøÊ≥õÁöÑÁÆóÊ≥ï‰πã‰∏Ä„ÄÇ
-   <strong>Making units of useful size:</strong> This involves either breaking long words into smaller pieces (common in languages like German) or grouping characters into words (necessary for languages without spaces, like Chinese).
    -   Â∞ÜÈïøÂçïËØçÂàÜÊàêÊõ¥Â∞èÁöÑÈÉ®ÂàÜÔºåÁâπÂà´ÊòØ‰∏≠ÊñáÔºàÊ≤°ÊúâÁ©∫Ê†ºÔºâ</p>
<p><strong>Special types of words:</strong>
-   <strong>Stop words:</strong> Very common words (e.g., "the", "a", "is") that are often removed before processing because they carry little semantic weight.
    -   ÈùûÂ∏∏Â∏∏ËßÅÁöÑËØçÔºöfunction words, fillers, backchannel
-   <strong>Rare words:</strong> Words that appear very infrequently. They can be problematic for statistical models and are sometimes removed or replaced with a generic "UNK" (unknown) token.
    -   ÁîüÂÉªËØçÔºöÂá∫Áé∞È¢ëÁéáÊûÅ‰ΩéÁöÑËØçÔºåÂà†Èô§‰∏ÄÈÉ®ÂàÜÊàñÈÉΩÁî®UNKÊ†áËÆ∞ÔºàËßÜ‰∏∫‰∏Ä‰∏™ÂçïÁã¨ÁöÑÈ°πÁõÆÔºâ
-   <strong>Hapax legomena:</strong> Words that occur only once in a corpus. They are an extreme case of rare words.
    -   ÁΩïËßÅËØçÁöÑÊûÅÁ´ØÊÉÖÂÜµÔºåÂè™Âá∫Áé∞‰∏ÄÊ¨°
-   <strong>Filler:</strong> Words or sounds used to pause in a conversation (e.g., "um," "uh," "like").
    -   Â°´ÂÖÖËØç
-   <strong>Backchannel:</strong> Signals from a listener that indicate they are paying attention (e.g., "uh-huh," "yeah," "I see").
    -   Âê¨‰ºóÂèëÂá∫ÁöÑ‰ø°Âè∑ËØç
-   <strong>Function vs. content words:</strong> <em>Content words</em> (nouns, main verbs, adjectives) carry the primary meaning. <em>Function words</em> (prepositions, articles, conjunctions) provide grammatical structure.
    -   <strong>ÂÆûËØç</strong> ÔºàÂêçËØç„ÄÅ‰∏ªË¶ÅÂä®ËØç„ÄÅÂΩ¢ÂÆπËØçÔºâÊâøËΩΩ‰∏ªË¶ÅÂê´‰πâ
    -   <strong>ÂäüËÉΩËØç</strong> Ôºà‰ªãËØç„ÄÅÂÜ†ËØç„ÄÅËøûËØçÔºâÊèê‰æõËØ≠Ê≥ïÁªìÊûÑ</p>
<h3 id="testing"><strong>Testing</strong><a class="headerlink" href="#testing" title="Permanent link">&para;</a></h3>
<p><strong>Roles of training, development, test datasets:</strong></p>
<ul>
<li><strong>Training set ËÆ≠ÁªÉÈõÜ:</strong> The data used to train the model and learn its parameters.</li>
<li><strong>Development set (or validation set) È™åËØÅÈõÜ:</strong> The data used to tune the model's hyperparameters and make design choices. It helps prevent overfitting to the training set. Èò≤Ê≠¢ËøáÂ∫¶ÊãüÂêà</li>
<li><strong>Test set ÊµãËØïÈõÜ:</strong> The data held back until the very end to provide an unbiased, final evaluation of the model's performance. ‰øùÁïôÂà∞ÊúÄÂêéÁöÑÊï∞ÊçÆÔºå‰ª•ÂØπÊ®°ÂûãÁöÑÊÄßËÉΩÊèê‰æõÂÖ¨Ê≠£ÁöÑÊúÄÁªàËØÑ‰º∞„ÄÇ</li>
</ul>
<p><strong>Evaluation metrics for classification:</strong></p>
<p><img src="./assets/image-20250825085728014.png" alt="image-20250825085728014" style="zoom:50%;" /></p>
<table>
<thead>
<tr>
<th>Confusion Matrix Ê∑∑Ê∑ÜÁü©Èòµ</th>
<th>Labels from Algorithm</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>/</td>
<td>happen</td>
<td>Not happen</td>
</tr>
<tr>
<td>Correct = happen</td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr>
<td>Correct = not happen</td>
<td>False Positive (FP)</td>
<td>True Negative (TN)</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>False positive rate</strong> = <span class="arithmatex">\(FP/(FP+TN)\)</span> [how many wrong things are in the negative outputs]</li>
<li><strong>False negative rate</strong> = <span class="arithmatex">\(FN/(TP+FN)\)</span> [how many wrong things are in the positive outputs]</li>
<li><strong>Accuracy</strong> = <span class="arithmatex">\((TP+TN)/(TP+TN+FP+FN)\)</span><ul>
<li>The fraction of predictions the model got righ</li>
</ul>
</li>
<li><strong>Error rate</strong> = <span class="arithmatex">\(1-accuracy\)</span></li>
<li><strong>precision (p)</strong> = <span class="arithmatex">\(TP/(TP+FP)\)</span> [how many of our outputs were correct?]</li>
<li><strong>recall &reg;</strong> = <span class="arithmatex">\(TP/(TP+FN)\)</span> <ul>
<li>True Positive [how many of the correct answers did we find?]</li>
</ul>
</li>
<li>
<p><strong>F1</strong> = <span class="arithmatex">\(2pr/(p+r)\)</span></p>
<ul>
<li>F1 is the harmonic mean of precision and recall. Both recall and precision need to be good to get a high F1 value.</li>
</ul>
</li>
<li>
<p><strong>Confusion Matrix:</strong> A table that visualizes the performance of a classifier, showing the counts of true positives, true negatives, false positives, and false negatives.</p>
</li>
</ul>
<h2 id="naive-bayes">Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permanent link">&para;</a></h2>
<blockquote>
<p><a href="https://courses.grainger.illinois.edu/cs440/fa2025/lectures/bayes.html">https://courses.grainger.illinois.edu/cs440/fa2025/lectures/bayes.html</a></p>
</blockquote>
<h3 id="basic-definitions-and-mathematical-model">Basic definitions and mathematical model<a class="headerlink" href="#basic-definitions-and-mathematical-model" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><span class="arithmatex">\(P(A | C)\)</span> is the probability of A in a context where C is true</p>
<ul>
<li>Definition of conditional probability: <span class="arithmatex">\(P(A | C) = \frac{P(A,C)}{P(C)}\)</span><ul>
<li><span class="arithmatex">\(P(A)=‚àëP(A|Z) p(Z,\theta)\)</span></li>
</ul>
</li>
<li><span class="arithmatex">\(P(A,C) = P(A) \times P(C | A) = P(C,A)=P(C) \times P(A | C)\)</span></li>
<li><strong>Bayes Rule</strong>: <span class="arithmatex">\(P(C|A)=\frac{P(A|C)\times P(C)}{P(A)}\)</span><ul>
<li><span class="arithmatex">\(P(cause|evidence)=\frac{P(evidence|cause)P(cause)}{P(evidence)}\)</span></li>
<li>posterior likelihood prior normalization</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Likelihood:</strong> <span class="arithmatex">\(P(evidence‚à£cause)\)</span> Ê¶ÇÁéá</p>
</li>
<li><strong>Prior:</strong> <span class="arithmatex">\(P(cause)\)</span> ÂÖàÈ™å</li>
<li><strong>Posterior:</strong> <span class="arithmatex">\(P(cause‚à£evidence)\)</span> ÂêéÈ™å</li>
<li><strong>argmax operator:</strong> Returns the input value that maximizes a function. In classification, we use it to find the class with the highest posterior probability.<ul>
<li>Ë°®Á§∫ËøîÂõû‰ΩøÂáΩÊï∞ÊúÄÂ§ßÂåñÁöÑËæìÂÖ•ÂÄºÁöÑÁ¨¶Âè∑ÔºåÁî®Êù•ÊâæÂà∞ÂêéÈ™åÊ¶ÇÁéáÊúÄÈ´òÁöÑÁ±ª</li>
</ul>
</li>
<li><strong>Independence vs. Conditional Independence:</strong> Naive Bayes makes a "naive" assumption of <em>conditional independence</em> of features: features are independent of each other <em>given the class</em>. This is a stronger assumption than simple independence.<ul>
<li><strong>Independence</strong> Áã¨Á´ãÊÄß<ul>
<li>Two events A and B are independent <strong>iff</strong> <span class="arithmatex">\(P(A,B) = P(A) \times P(B)\)</span></li>
</ul>
</li>
<li><strong>Conditional Independence</strong> Êù°‰ª∂Áã¨Á´ãÊÄß<ul>
<li>Definition Ôºö<span class="arithmatex">\(P(A, B | C) = P(A|C) \times P(B|C)\)</span>, Á≠â‰ª∑‰∫é <span class="arithmatex">\(P(A | B) = P(A), P(B | A) = P(B)\)</span></li>
</ul>
</li>
<li>Áã¨Á´ãÊÄßÂæàÂ∞ëÊàêÁ´ãÔºõÊù°‰ª∂Áã¨Á´ãÊÄßÊòØÂú®ÁâπÂÆöÁöÑ‰∏ä‰∏ãÊñá‰∏≠Ôºå‰∏§‰∏™ÂèòÈáèÊòØÂê¶Áã¨Á´ãÔºåËøë‰ººÂêàÁêÜ„ÄÇ</li>
</ul>
</li>
<li><strong>MAP vs. ML estimate</strong><ul>
<li><strong>Maximum Likelihood (ML)</strong> chooses the parameters that maximize the likelihood of the data.  Ê¶ÇÁéáÊúÄÂ§ßÂåñ</li>
<li><strong>Maximum a Posteriori (MAP)</strong> incorporates a prior probability, choosing parameters that maximize the posterior probability. The prior acts as a regularizer. ÂêéÈ™åÊ¶ÇÁéáÊúÄÂ§ßÂåñ</li>
</ul>
</li>
<li><strong>Combining evidence:</strong> Under the conditional independence assumption, the likelihood of all evidence is simply the product of the likelihoods of each individual piece of evidence:<ul>
<li><span class="arithmatex">\(P(evidence_1,‚Ä¶,evidencen‚à£cause)=‚àè_iP(evidence_i‚à£cause)\)</span>.</li>
</ul>
</li>
<li><strong>Model size:</strong> Naive Bayes dramatically reduces the number of parameters needed compared to a full joint distribution table, making it computationally feasible and less prone to overfitting on small datasets.</li>
</ul>
<h3 id="applying-naive-bayes-to-text-classification">Applying Naive Bayes to text classification<a class="headerlink" href="#applying-naive-bayes-to-text-classification" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>equations:</strong> You estimate the prior probability of a class by its frequency in the training data, and the likelihood of a word given a class by its frequency within documents of that class.</p>
<ul>
<li>‰º∞ËÆ°ÊñπÁ®ãÔºåÈÄöËøáËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Êüê‰∏™Á±ªÂà´ÁöÑÈ¢ëÁéáÊù•‰º∞ËÆ°ËØ•Á±ªÂà´ÁöÑÂÖàÈ™åÊ¶ÇÁéá</li>
</ul>
</li>
<li>
<p><strong>Avoiding underflow:</strong> Since you are multiplying many small probabilities, the result can become too small for a computer to store (underflow). To fix this, you work with the sum of log probabilities instead: <span class="arithmatex">\(log(A‚ãÖB)=log(A)+log(B)\)</span>.</p>
<ul>
<li>ÂØπÊï∞Èò≤Ê≠¢‰πò‰ª•Â§™Â∞èÁöÑÊï∞Â≠óÂΩ±ÂìçÂáÜÁ°ÆÊÄßÔºàËÆ°ÁÆóÁ≤æÂ∫¶ÔºâÔºåÂ∞ÜÊú¥Á¥†Ë¥ùÂè∂ÊñØÁÆóÊ≥ïÊúÄÂ§ßÂåñ</li>
</ul>
</li>
<li>
<p><strong>Avoiding overfitting (Smoothing)</strong> Âπ≥ÊªëÂ§ÑÁêÜ</p>
<ul>
<li>
<p><strong>Why it's important:</strong> If a word never appears in the training data for a certain class, its probability will be zero, causing the entire posterior probability for that class to become zero. ËøáÂ∫¶ÊãüÂêàÔºå0‰ºöÁ†¥ÂùèÊú¥Á¥†Ë¥ùÂè∂ÊñØÁÆóÊ≥ï</p>
</li>
<li>
<p><strong>Laplace smoothing:</strong> Adds a small constant (usually 1) to every count, ensuring no probability is ever zero.</p>
<ul>
<li>
<p>ÊãâÊôÆÊãâÊñØÂπ≥ÊªëÔºö<img src="./assets/image-20250902212218178.png" alt="image-20250902212218178" style="zoom:50%;" /></p>
</li>
<li>
<p><span class="arithmatex">\(n\)</span> = number of words in our Class C training data
    <span class="arithmatex">\(count(W)\)</span> = number of times W appeared in Class C training data</p>
<p><span class="arithmatex">\(\alpha\)</span>: a constant positive number
<span class="arithmatex">\(V\)</span> = number of word <strong>types</strong> seen in training data</p>
</li>
</ul>
</li>
<li>
<p><strong>Deleted estimation Âà†Èô§‰º∞ËÆ°:</strong> A cross-validation technique used to find optimal smoothing parameters.</p>
<ul>
<li>ÂØπ‰∫éÊØè‰∏™ËßÇÊµãÂà∞ÁöÑËÆ°Êï∞ rÔºåÊàë‰ª¨Â∞ÜËÆ°ÁÆó‰∏Ä‰∏™Ê†°Ê≠£ÂêéÁöÑËÆ°Êï∞ <span class="arithmatex">\(Corr(r)\)</span>„ÄÇÂÅáËÆæ <span class="arithmatex">\(W1,...,Wn\)</span>ÊòØÂú®Êï∞ÊçÆÈõÜÂâçÂçäÈÉ®ÂàÜÂá∫Áé∞ r Ê¨°ÁöÑÂçïËØç„ÄÇÂØπ‰∫éÊ≠§ÈõÜÂêà‰∏≠ÁöÑÊØè‰∏™ÂçïËØç <span class="arithmatex">\(W_k\)</span>ÔºåÊ±ÇÂá∫ÂÆÉÂú®Êï∞ÊçÆÈõÜÂêéÂçäÈÉ®ÂàÜÂá∫Áé∞ÁöÑËÆ°Êï∞ <span class="arithmatex">\(C(W_k)\)</span> „ÄÇÊàë‰ª¨Â∞ÜËøô‰∫õËÆ°Êï∞ÂèñÂπ≥ÂùáÂÄºÔºåÂæóÂà∞Ê†°Ê≠£ÂêéÁöÑËÆ°Êï∞Ôºö</li>
<li><img src="./assets/image-20250903094420273.png" alt="image-20250903094420273" style="zoom:40%;" />Corr&reg; È¢ÑÊµãËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Âá∫Áé∞ r Ê¨°ÁöÑÂçïËØçÁöÑÊú™Êù•ËÆ°Êï∞</li>
<li>Âà†Èô§‰º∞ËÆ°Â∑≤Ë¢´ËØÅÊòéÊØîÊãâÊôÆÊãâÊñØÂπ≥ÊªëÊõ¥ÂáÜÁ°Æ</li>
</ul>
</li>
<li>
<p><strong>N-gram smoothing:</strong> Refers to more advanced techniques (like Good-Turing, Kneser-Ney) used for n-gram models to handle unseen n-grams by redistributing probability mass from seen n-grams. The high-level idea is to "borrow" probability from more frequent events to assign to rare or unseen events.</p>
<ul>
<li>ÂØπ‰∫é‰∏ÄÂÖÉÊ®°ÂûãÔºåÊàë‰ª¨ÈúÄË¶Å‰º∞ËÆ° n ‰∏™Ê¶ÇÁéáÔºåËÄåÂØπ‰∫é‰∫åÂÖÉÊ®°ÂûãÔºåÊàë‰ª¨ÈúÄË¶Å‰º∞ËÆ° <span class="arithmatex">\(n^2\)</span> ‰∏™Ê¶ÇÁéáÔºå‰ΩÜÊàë‰ª¨‰ªçÁÑ∂Êã•ÊúâÁõ∏ÂêåÁöÑ m ‰∏™ÂçïËØç‰Ωú‰∏∫ËÆ≠ÁªÉÊï∞ÊçÆ</li>
<li>Idea 1: If we haven't seen an ngram, guess its probability from the probabilites of its <em>prefix</em> (e.g. "the angry") and the <em>last word</em> ("armadillo").</li>
<li>Idea 2: Guess that an unseen word is more likely in contexts where we've seen many different words.</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr />
<h2 id="search">Search<a class="headerlink" href="#search" title="Permanent link">&para;</a></h2>
<p>Search is the process of finding a sequence of actions (a <strong>path</strong>) to get from a starting point to a goal.</p>
<h3 id="example-tasks">Example tasks<a class="headerlink" href="#example-tasks" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Roadmap Search:</strong> Finding the best route from Champaign to Chicago on Google Maps.</li>
<li><strong>Maze Search:</strong> Finding a path from the entrance to the exit of a maze.</li>
<li><strong>8-Puzzle:</strong> Sliding numbered tiles in a 3x3 grid to get them into a sorted order. Each move (sliding a tile) creates a new state.</li>
<li><strong>Edit Distance:</strong> Finding the minimum number of edits (insert, delete, substitute) to change one word into another, like "cat" to "car."</li>
<li><strong>Speech Recognition:</strong> The AI toy from the article has to "search" through a massive library of sounds, words, and phrases to find the most probable interpretation of what a child said.</li>
<li><a href="https://en.wikipedia.org/wiki/Missionaries_and_cannibals_problem">Missionaries and Cannibals puzzle</a>: A classic logic puzzle. You have 3 missionaries and 3 cannibals who need to cross a river in a boat that holds 2 people. If cannibals ever outnumber missionaries on either bank, they get eaten. The goal is to find a sequence of boat trips that gets everyone across safely.</li>
<li><strong>Adventure Game</strong>: Each state in a search problem must contain all the information about the world that is relevant to defining and reaching the goal.</li>
</ul>
<h4 id="basic-search-methods">Basic search methods<a class="headerlink" href="#basic-search-methods" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>State Space:</strong> The set of all possible configurations in the problem (e.g., every possible arrangement of tiles in an 8-puzzle, every possible location in a maze).</li>
<li><strong>Initial State:</strong> Where you begin.</li>
<li><strong>Actions:</strong> The set of possible moves you can make from a given state (e.g., slide tile up, down, left, right).</li>
<li><strong>Transition Model:</strong> The rule that tells you what state you get to if you perform an action in another state. <code>Result(state, action) -&gt; new_state</code></li>
<li><strong>Goal State(s):</strong> The state(s) you want to reach. ÁõÆÊ†áÁä∂ÊÄÅ</li>
<li><strong>Path Cost:</strong> A function that assigns a cost to a path (e.g., distance in miles, number of moves). In uniform cost search, we want the path with the <strong>lowest cost</strong>. In other searches, we want the <strong>shortest path</strong> (fewest steps).</li>
</ul>
<h4 id="outline-for-search-algorithms">Outline for search algorithms<a class="headerlink" href="#outline-for-search-algorithms" title="Permanent link">&para;</a></h4>
<p>Most search algorithms follow this general pattern:</p>
<ol>
<li>Initialize a <strong>frontier</strong> (a data structure like a queue or stack) with the initial state.</li>
<li>Initialize a <strong>visited states</strong> set (or table) to keep track of where you've already been, to avoid loops.</li>
<li>Loop until the frontier is empty or the goal is found: <ol>
<li><strong>Pop</strong> a node from the frontier. </li>
<li>If this node is the <strong>goal</strong>, you're done! You can reconstruct the path back using <strong>backpointers</strong> (pointers from each node to the node that discovered it). </li>
<li>Mark the current node as <strong>visited</strong>. </li>
<li><strong>Expand</strong> the node: find all its neighbors (the states you can reach with one action). </li>
<li>For each neighbor, if it's not in the visited set and not already in the frontier, add it to the frontier.</li>
</ol>
</li>
</ol>
<hr />
<p><strong>State graph</strong> representations Áä∂ÊÄÅÂõæ</p>
<p>A state graph has the following key parts:</p>
<ul>
<li>states (graph nodes)  Áä∂ÊÄÅÔºàÂõæËäÇÁÇπÔºâ</li>
<li>actions (graph edges, with costs)</li>
<li>start state  Ëµ∑ÂßãÁä∂ÊÄÅ</li>
<li>goal states (explicit list, or a goal condition to test) ÁõÆÊ†áÁä∂ÊÄÅÔºàÊòéÁ°ÆÂàóË°®ÔºåÊàñË¶ÅÊµãËØïÁöÑÁõÆÊ†áÊù°‰ª∂Ôºâ</li>
</ul>
<h4 id="key-search-algorithms">Key Search Algorithms<a class="headerlink" href="#key-search-algorithms" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Breadth-First Search (BFS):</strong> Explores level by level. It finds the <strong>shortest path</strong> in terms of the number of steps. <em>‰ªé‰∏Ä‰∏™ËäÇÁÇπÂºÄÂßãÔºåÈ¶ñÂÖàÈÅçÂéÜÂÖ∂ÊâÄÊúâÈÇªÊé•ËäÇÁÇπ„ÄÇËÆøÈóÆÂÆåÊâÄÊúâÈÇªÊé•ËäÇÁÇπÂêéÔºåÂÜçÈÅçÂéÜÂÖ∂ÈÇªÊé•ËäÇÁÇπ</em></li>
<li><strong>Uniform-Cost Search (UCS):</strong> Explores by path cost. It's like BFS but always expands the node with the lowest total path cost so far. It's guaranteed to find the <strong>cheapest path</strong>. ‰ªéËµ∑ÂßãÁä∂ÊÄÅÂºÄÂßãÔºåÊàë‰ª¨Â∞Ü<em>ËÆøÈóÆÁõ∏ÈÇªÁä∂ÊÄÅÂπ∂ÈÄâÊã©ÊàêÊú¨ÊúÄ‰ΩéÁöÑÁä∂ÊÄÅ</em>ÔºåÁÑ∂Âêé‰ªéÊâÄÊúâÊú™ËÆøÈóÆ‰∏î‰∏éÂ∑≤ËÆøÈóÆÁä∂ÊÄÅÁõ∏ÈÇªÁöÑÁä∂ÊÄÅ‰∏≠ÈÄâÊã©‰∏ã‰∏Ä‰∏™ÊàêÊú¨ÊúÄ‰ΩéÁöÑÁä∂ÊÄÅÔºå‰ª•Ê≠§ÊñπÂºèÂ∞ùËØïÂà∞ËææÁõÆÊ†áÁä∂ÊÄÅÔºàÊ≥®ÊÑèÔºåÊàë‰ª¨‰∏ç‰ºöÁªßÁª≠Ê≤øÁùÄÁõÆÊ†áÁä∂ÊÄÅÁªßÁª≠ÂâçË°åÔºâ„ÄÇÂç≥‰ΩøÂà∞ËææÁõÆÊ†áÁä∂ÊÄÅÔºåÊàë‰ª¨‰πü‰ºöÁªßÁª≠ÊêúÁ¥¢ÂÖ∂‰ªñÂèØËÉΩÁöÑË∑ØÂæÑÔºàÂ¶ÇÊûúÊúâÂ§ö‰∏™ÁõÆÊ†áÔºâ„ÄÇÊàë‰ª¨Â∞ÜÁª¥Êä§‰∏Ä‰∏™<em>‰ºòÂÖàÁ∫ßÈòüÂàó</em>ÔºåËØ•ÈòüÂàóÂ∞Ü‰ªéÊâÄÊúâ‰∏éÂ∑≤ËÆøÈóÆÁä∂ÊÄÅÁõ∏ÈÇªÁöÑÁä∂ÊÄÅ‰∏≠ÔºåÈÄâÊã©ÊàêÊú¨ÊúÄ‰ΩéÁöÑ‰∏ã‰∏Ä‰∏™Áä∂ÊÄÅ„ÄÇ</li>
<li><strong>Depth-First Search (DFS):</strong> Dives as deep as it can down one path before backtracking. It's fast and memory-efficient but can get stuck in infinite loops and doesn't guarantee the shortest or cheapest path. Êàë‰ª¨‰ºöÈÄê‰∏™ÈÅçÂéÜÊâÄÊúâÁõ∏ÈÇªÈ°∂ÁÇπ„ÄÇÂΩìÈÅçÂéÜ‰∏Ä‰∏™Áõ∏ÈÇªÈ°∂ÁÇπÊó∂ÔºåÊàë‰ª¨‰ºöÂÆåÊï¥Âú∞<em>ÈÅçÂéÜÈÄöËøáËØ•Áõ∏ÈÇªÈ°∂ÁÇπÂèØÂà∞ËææÁöÑÊâÄÊúâÈ°∂ÁÇπ</em>„ÄÇËøôÁ±ª‰ºº‰∫é<a href="https://www.geeksforgeeks.org/dsa/dfs-traversal-of-a-tree-using-recursion/">Ê∑±Â∫¶‰ºòÂÖàÊ†ëÁöÑÈÅçÂéÜ </a>ÔºåÊàë‰ª¨È¶ñÂÖàÂÆåÊï¥Âú∞ÈÅçÂéÜÂ∑¶Â≠êÊ†ëÔºåÁÑ∂ÂêéÂÜçÁßªËá≥Âè≥Â≠êÊ†ë„ÄÇÂÖ≥ÈîÆÂå∫Âà´Âú®‰∫éÔºå‰∏éÊ†ë‰∏çÂêåÔºåÂõæÂèØËÉΩÂåÖÂê´Âæ™ÁéØÔºà‰∏Ä‰∏™ËäÇÁÇπÂèØËÉΩË¢´ËÆøÈóÆÂ§öÊ¨°Ôºâ„ÄÇ‰∏∫‰∫ÜÈÅøÂÖçÂ§öÊ¨°Â§ÑÁêÜ‰∏Ä‰∏™ËäÇÁÇπÔºåÊàë‰ª¨‰ΩøÁî®‰∏Ä‰∏™<em>Â∏ÉÂ∞îÂûãÁöÑ visited Êï∞ÁªÑ</em>„ÄÇ</li>
<li><strong>Iterative Deepening Search(IDS):</strong> A hybrid. It does a DFS with a limited depth (e.g., depth 1, then depth 2, etc.). It combines the optimality of BFS with the memory efficiency of DFS. IDDFS ÁªìÂêà‰∫ÜÊ∑±Â∫¶‰ºòÂÖàÊêúÁ¥¢ÁöÑÁ©∫Èó¥ÊïàÁéáÂíåÂπøÂ∫¶‰ºòÂÖàÊêúÁ¥¢ÁöÑÂø´ÈÄüÊêúÁ¥¢ÔºàÈíàÂØπÊõ¥Êé•ËøëÊ†πÁöÑËäÇÁÇπÔºâ„ÄÇIDDFS ‰ºö‰ªé‰∏Ä‰∏™ÂàùÂßãÂÄºÂºÄÂßãÔºåÈíàÂØπ‰∏çÂêåÁöÑÊ∑±Â∫¶Ë∞ÉÁî® DFS„ÄÇÊØèÊ¨°Ë∞ÉÁî®Êó∂ÔºåDFS ÈÉΩ‰ºöË¢´ÈôêÂà∂‰∏çËÉΩË∂ÖÂá∫ÁªôÂÆöÁöÑÊ∑±Â∫¶„ÄÇÊâÄ‰ª•ÔºåÊàë‰ª¨Âü∫Êú¨‰∏äÊòØ<em>‰ª• BFS ÁöÑÊñπÂºèËøõË°å DFS ÁöÑ</em>„ÄÇ</li>
</ul>
<h3 id="a-search">A* Search<a class="headerlink" href="#a-search" title="Permanent link">&para;</a></h3>
<p>A* (pronounced "A-star") is the go-to search algorithm. It's a smarter version of Uniform Cost Search.</p>
<p>A* ÊêúÁ¥¢ÁÆóÊ≥ïÊòØË∑ØÂæÑÊü•ÊâæÂíåÂõæÂΩ¢ÈÅçÂéÜ‰∏≠‰ΩøÁî®ÁöÑÊúÄÂ•ΩÂíåÊúÄÊµÅË°åÁöÑÊäÄÊúØ‰πã‰∏Ä„ÄÇ</p>
<p>A<em> balances the cost to get to a node with an </em>estimate* of the cost to get from that node to the goal. It evaluates nodes using the formula:</p>
<p><span class="arithmatex">\(f(n)=g(n)+h(n)\)</span></p>
<ul>
<li><span class="arithmatex">\(g(n)\)</span> = the <strong>actual cost</strong> of the path from the start node to node n. ÂÆûÈôÖÊàêÊú¨</li>
<li><span class="arithmatex">\(h(n)\)</span> = the <strong>heuristic</strong> (estimated) cost from node n to the goal. ÂêØÂèëÂºèÊàêÊú¨</li>
</ul>
<h4 id="heuristics">Heuristics<a class="headerlink" href="#heuristics" title="Permanent link">&para;</a></h4>
<p>A heuristic is a "rule of thumb" or an educated guess.</p>
<ul>
<li><strong>Manhattan vs. Straight-Line Distance:</strong><ul>
<li><strong>Straight-Line (Euclidean) Distance Ê¨ßÂá†ÈáåÂæóË∑ùÁ¶ª:</strong> The "as the crow flies" distance. This is a good heuristic for finding a driving route.</li>
<li><strong>Manhattan Distance ÊõºÂìàÈ°øË∑ùÁ¶ª:</strong> The distance if you can only move on a grid (like the streets of Manhattan). You sum the absolute differences of the x and y coordinates: <span class="arithmatex">\(‚à£x_1‚àíx_2‚à£+‚à£y_1‚àíy_2‚à£\)</span>. This is a perfect heuristic for the 8-puzzle.</li>
</ul>
</li>
<li><strong>Admissible and Consistent Heuristics:</strong><ul>
<li><strong>Admissible ÂèØÊé•Âèó:</strong> An admissible heuristic <strong>never overestimates</strong> the true cost. It's always optimistic. <code>h(n) &lt;= true_cost</code>. Straight-line distance is admissible because you can never get somewhere faster than a straight line. A<em> is </em><em>guaranteed to find the optimal path</em>* if its heuristic is admissible.</li>
<li><strong>Consistent ‰∏ÄËá¥ÊÄß:</strong> A stronger property. A heuristic is consistent if, for any node n and its successor n‚Ä≤, the estimated cost from n is less than or equal to the cost of stepping to n‚Ä≤ plus the estimated cost from n‚Ä≤. This basically means the heuristic gets more accurate as you get closer to the goal. <code>h(n) &lt;= cost(n, n') + h(n')</code>. <em>All consistent heuristics are also admissible</em>. Consistency is useful because it guarantees that when A* selects a node to expand, it has already found the optimal path to that node.</li>
</ul>
</li>
<li><strong>What is a good heuristic?</strong> A good heuristic is as close to the true cost as possible without ever going over. A heuristic <span class="arithmatex">\(h_2(n)\)</span> is <strong>better</strong> than <span class="arithmatex">\(h_1(n)\)</span> if <span class="arithmatex">\(h_2(n)&gt;h_1(n)\)</span> for all nodes, because it provides more information and helps A* explore more directly towards the goal, reducing the number of nodes it needs to check.</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<p>In practice, <strong>admissible heuristics are typically consistent</strong>. For example, the straight-line distance is consistent because of the triangle inequality. Uniform cost search is A‚àó with a heuristic function that always returns zero. That heuristic is consistent. However, if you are writing general-purpose search code, it's best to use a method that will still work if the heuristic isn't consistent, such as pushing a duplicate copy of the state onto the queue.</p>
<p>Admissibility guarantees that the output solution is optimal. Here's a sketch of the proof:
  ÂèØÊé•Á∫≥ÊÄß‰øùËØÅ‰∫ÜËæìÂá∫Ëß£ÊòØÊúÄ‰ºòÁöÑ„ÄÇ‰ª•‰∏ãÊòØËØÅÊòéÁöÑÊ¶ÇË¶ÅÔºö</p>
<p>Search stops when the goal state becomes the top option in the frontier (not when the goal is first discovered). Suppose we have found a goal with path P and cost C. Consider a partial path X in the frontier. Its estimated cost (heuristic cost) must be ‚â•C. Since our heuristic is admissible, the true cost of extending X to reach the goal must also be ‚â•C. So extending X can't give us a better path than P.
  ÂΩìÁõÆÊ†áÁä∂ÊÄÅÊàê‰∏∫ËæπÁïå‰∏≠ÁöÑÊúÄ‰ºòÈÄâÈ°πÊó∂ÔºàËÄå‰∏çÊòØÁõÆÊ†áÈ¶ñÊ¨°Ë¢´ÂèëÁé∞Êó∂ÔºâÔºåÊêúÁ¥¢ÂÅúÊ≠¢„ÄÇÂÅáËÆæÊàë‰ª¨Â∑≤ÁªèÊâæÂà∞‰∫ÜË∑ØÂæÑ‰∏∫ P„ÄÅÊàêÊú¨‰∏∫ C ÁöÑÁõÆÊ†á„ÄÇËÄÉËôëËæπÁïå‰∏≠ÁöÑÈÉ®ÂàÜË∑ØÂæÑ X„ÄÇÂÆÉÁöÑ‰º∞ËÆ°ÊàêÊú¨hÂøÖÂÆö‰∏∫ ‚â•C „ÄÇÁî±‰∫éÊàë‰ª¨ÁöÑÂêØÂèëÂºèÊñπÊ≥ïÂèØË°åÔºåÂõ†Ê≠§Êâ©Â±ï X ‰ª•ËææÂà∞ÁõÆÊ†áÁöÑÁúüÂÆûÊàêÊú¨‰πüÂøÖÂÆö‰∏∫ ‚â•C „ÄÇÂõ†Ê≠§ÔºåÊâ©Â±ï X ‰∏çÂèØËÉΩÊèê‰æõÊØî P Êõ¥Â•ΩÁöÑË∑ØÂæÑ„ÄÇ</p>
</blockquote>
<h4 id="suboptimal-search">Suboptimal Search Ê¨°‰ºòÊêúÁ¥¢<a class="headerlink" href="#suboptimal-search" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Non-admissible heuristics:</strong> Sometimes you don't need the <em>best</em> path, you just need a <em>good enough</em> path, fast. Using a non-admissible (overestimating) heuristic can make A* run much faster, but it forfeits the guarantee of finding the optimal path.</li>
<li><strong>Beam Search ÈõÜÊùüÊêúÁ¥¢:</strong> A memory-optimized version of search. At each level of the search tree, it only keeps the best <code>k</code> nodes (the "beam width"). It's fast but can easily miss the optimal solution if it falls outside the beam.</li>
</ul>
<h3 id="details-for-each-algorithm">Details for Each Algorithm<a class="headerlink" href="#details-for-each-algorithm" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Frontier Management</th>
<th>Loop Prevention</th>
<th>When a shorter path is found...</th>
<th>Termination</th>
<th>When to use</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>BFS</strong></td>
<td><strong>FIFO Queue</strong>. Explores the oldest nodes first.</td>
<td>Uses a <strong>visited set</strong>. Won't re-add a visited node.</td>
<td>The first time a node is found is guaranteed to be the shortest path to it.</td>
<td>Finds goal or frontier is empty.</td>
<td>When you need the <strong>shortest path</strong> (i.e., the fewest steps), and step costs are uniform.</td>
</tr>
<tr>
<td><strong>DFS</strong></td>
<td><strong>LIFO Stack</strong> (or recursion). Explores the newest nodes first.</td>
<td>Uses a <strong>visited set</strong>.</td>
<td>Can find a suboptimal path first. Needs modification to update.</td>
<td>Finds goal or frontier is empty.</td>
<td>When memory is tight, you think the solution is deep, and optimality isn't required.</td>
</tr>
<tr>
<td><strong>UCS</strong></td>
<td><strong>Priority Queue</strong> ordered by path cost <span class="arithmatex">\(g(n)\)</span>.</td>
<td>Uses a <strong>visited set</strong>.</td>
<td>If it finds a shorter path to a node already in the frontier, it <strong>updates the path cost</strong> of that node.</td>
<td>Finds goal or frontier is empty.</td>
<td>When you need the <strong>cheapest path</strong>, and step costs are not uniform.</td>
</tr>
<tr>
<td><strong>A*</strong></td>
<td><strong>Priority Queue</strong> ordered by <span class="arithmatex">\(f(n)=g(n)+h(n)\)</span></td>
<td>Uses a <strong>visited set</strong>.</td>
<td>Same as <em>UCS</em>: if it finds a shorter path to a node in the frontier, it <strong>updates its cost</strong>.</td>
<td>Finds goal or frontier is empty.</td>
<td>When you need the <strong>cheapest path</strong> and have a good heuristic to speed things up. It's usually the best choice.</td>
</tr>
</tbody>
</table>
<h3 id="more-search">More Search<a class="headerlink" href="#more-search" title="Permanent link">&para;</a></h3>
<p><strong>Iterative deepening A‚àó</strong>: Like iterative deepening, but using A‚àó rather than DFS. If the current bound is k, the search stops when the estimated total cost f(s) is larger than k. Á±ª‰ºº‰∫éËø≠‰ª£Âä†Ê∑±Ôºå‰ΩÜ‰ΩøÁî® A‚àó ËÄå‰∏çÊòØ DFS„ÄÇÂ¶ÇÊûúÂΩìÂâçËæπÁïå‰∏∫ kÔºåÂàôÂΩì‰º∞ËÆ°ÊÄªÊàêÊú¨ f(s) Â§ß‰∫é k Êó∂ÔºåÊêúÁ¥¢ÂÅúÊ≠¢„ÄÇ</p>
<p><strong>Pattern databases Ê®°ÂºèÊï∞ÊçÆÂ∫ì</strong>: A pattern database is a database of partial problems, together with the cost to solve the partial problem. The true solution cost for a search state must be at least as high as the cost for any pattern it contains. Ê®°ÂºèÊï∞ÊçÆÂ∫ìÊòØÂåÖÂê´ÈÉ®ÂàÜÈóÆÈ¢òÂèäÂÖ∂Ê±ÇËß£ÊàêÊú¨ÁöÑÊï∞ÊçÆÂ∫ì„ÄÇÊêúÁ¥¢Áä∂ÊÄÅÁöÑÁúüÂÆûÊ±ÇËß£ÊàêÊú¨ÂøÖÈ°ªËá≥Â∞ë‰∏éÂÖ∂ÂåÖÂê´ÁöÑ‰ªª‰ΩïÊ®°ÂºèÁöÑÊ±ÇËß£ÊàêÊú¨Áõ∏Âêå„ÄÇ</p>
<p><strong>Backwards and bidirectional search ÂêëÂêéÊêúÁ¥¢ÂíåÂèåÂêëÊêúÁ¥¢</strong> : Basically what they sound like. Backwards search is like forwards search except that you start from the goal. In bidirectional search, you run two parallel searches, starting from start and goal states, in hopes that they will connect at a shared state. These approaches are useful only in very specific situations. È°æÂêçÊÄù‰πâÔºåÂêëÂêéÊêúÁ¥¢Á±ª‰ºº‰∫éÂêëÂâçÊêúÁ¥¢ÔºåÂè™‰∏çËøáÊòØ‰ªéÁõÆÊ†áÂºÄÂßã„ÄÇÂèåÂêëÊêúÁ¥¢ÂàôËøêË°å‰∏§‰∏™Âπ∂Ë°åÊêúÁ¥¢ÔºåÂàÜÂà´‰ªéËµ∑ÂßãÁä∂ÊÄÅÂíåÁõÆÊ†áÁä∂ÊÄÅÂºÄÂßãÔºåÂ∏åÊúõÂÆÉ‰ª¨ËÉΩÂ§üÂú®‰∏Ä‰∏™ÂÖ±‰∫´Áä∂ÊÄÅ‰∏ãËøûÊé•Ëµ∑Êù•„ÄÇËøô‰∫õÊñπÊ≥ï‰ªÖÂú®ÈùûÂ∏∏ÁâπÊÆäÁöÑÊÉÖÂÜµ‰∏ãÊúâÁî®„ÄÇ</p>
<ul>
<li>
<p>Backwards search makes sense when you have relatively few goal states, known at the start. This often fails, e.g. chess has a huge number of winning board configurations. Also, actions must be easy to run backwards (often true).
    ÂΩìÁõÆÊ†áÁä∂ÊÄÅÁõ∏ÂØπËæÉÂ∞ë‰∏î‰∏ÄÂºÄÂßãÂ∑≤Áü•Êó∂ÔºåÂèçÂêëÊêúÁ¥¢ÊòØÊúâÊÑè‰πâÁöÑ„ÄÇ‰ΩÜËøôÈÄöÂ∏∏‰ºöÂ§±Ë¥•Ôºå‰æãÂ¶ÇÂõΩÈôÖË±°Ê£ã‰∏≠Â≠òÂú®Â§ßÈáèÁöÑÂà∂ËÉúÊ£ãÁõòÈÖçÁΩÆ„ÄÇÊ≠§Â§ñÔºåÂä®‰ΩúÂøÖÈ°ªÊòì‰∫éÂèçÂêëÊâßË°åÔºàÈÄöÂ∏∏Â¶ÇÊ≠§Ôºâ„ÄÇ</p>
</li>
<li>
<p>Example: James Bond is driving his Lexus in central Boston and wants to get to Dr. Evil's lair in Hawley, MA (population 337). There's a vast number of ways to get out of Boston but only one road of significant size going through Hawley. In that situation, working backwards from the goal might be much better than working forwards from the starting state.
    ‰æãÂ¶ÇÔºöË©πÂßÜÊñØ¬∑ÈÇ¶Âæ∑È©æÈ©∂ÁùÄ‰ªñÁöÑÈõ∑ÂÖãËê®ÊñØÂú®Ê≥¢Â£´È°øÂ∏Ç‰∏≠ÂøÉË°åÈ©∂ÔºåÊÉ≥Ë¶ÅÂâçÂæÄ‰Ωç‰∫éÈ©¨Ëê®ËØ∏Â°ûÂ∑ûÈúçÂà©Ôºà‰∫∫Âè£ 337ÔºâÁöÑÈÇ™ÊÅ∂ÂçöÂ£´ËÄÅÂ∑¢„ÄÇÁ¶ªÂºÄÊ≥¢Â£´È°øÁöÑÊñπÊ≥ïÊúâÂæàÂ§öÔºå‰ΩÜÂè™Êúâ‰∏ÄÊù°Áõ∏ÂΩìÈïøÁöÑÈÅìË∑ØÁ©øËøáÈúçÂà©„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºå‰ªéÁõÆÊ†á‰ΩçÁΩÆÂÄíÊé®ÂèØËÉΩÊØî‰ªéËµ∑Âßã‰ΩçÁΩÆÊ≠£Êé®Ë¶ÅÂ•ΩÂæóÂ§ö„ÄÇ</p>
</li>
</ul>
<h2 id="robotics-and-configuration-space">Robotics and Configuration Space<a class="headerlink" href="#robotics-and-configuration-space" title="Permanent link">&para;</a></h2>
<h3 id="history">History<a class="headerlink" href="#history" title="Permanent link">&para;</a></h3>
<p>Shakey and Blocks world (1966-72)</p>
<ul>
<li>
<p>Shakey was the <strong>first mobile robot to reason about its own actions</strong>. It lived in a controlled environment called "Blocks World," which was just a room with several large blocks. Shakey could be given commands like "push the block from Room A to Room B." To do this, it had to perceive its environment, break the problem down into smaller steps (e.g., <em>find the block, move to the block, push it, navigate doorways</em>), and execute them. It was a landmark project in robotics, planning, and computer vision.</p>
</li>
<li>
<p>Boston Dynamics(Founded 1992, prominent from 2005-present)</p>
<ul>
<li>famous for creating incredibly agile and dynamic robots like <strong>BigDog</strong>, <strong>Atlas</strong> (the humanoid one that does parkour), and <strong>Spot</strong> (the dog-like one)</li>
<li>Legged robots (Marc Raibert and Boston Dynamics): <a href="https://www.youtube.com/watch?v=Bd5iEke6UlE">early ones</a> and <a href="https://www.youtube.com/watch?v=vjSohj-Iclc">newish humanoid one</a> (audio is mechanical noises)</li>
</ul>
</li>
<li>Google self-driving bike(April 1, 2016) <ul>
<li>Fake: highlights the gap between public perception and actual AI capabilities.</li>
</ul>
</li>
<li><a href="https://www.youtube.com/watch?v=8MfyIsPWhTk">Coning a self-driving car</a> (2023-Present)<ul>
<li>an <strong>adversarial attack</strong> on an AI system</li>
</ul>
</li>
</ul>
<p>Motion planning is a type of search problem, but for physical objects.</p>
<ul>
<li><strong>Configuration Space ÈÖçÁΩÆÁ©∫Èó¥ (C-Space):</strong> An abstract space where each <em>point</em> represents a complete specification of the robot's position and orientation. The goal is to transform a motion problem with a complex robot and obstacles into a path-finding problem for a single point in C-space.<ul>
<li><strong>Circular Robot:</strong> If a circular robot is moving among obstacles, its C-space is found by "growing" the obstacles by the robot's radius. The robot itself can then be treated as a point.</li>
<li><strong>N-Link Arm N ËøûÊùÜÊú∫Ê¢∞ËáÇ:</strong> For a robotic arm with N joints, the C-space is N-dimensional. For a 2-link arm, the configuration is defined by two angles (Œ∏1,Œ∏2), so its C-space can be visualized as a 2D square.</li>
<li><strong>Rectangular Robot:</strong> If it can't rotate, its C-space is just like the circular robot's. If it <em>can</em> rotate, you add another dimension for its orientation, making the C-space 3D <span class="arithmatex">\((x,y,Œ∏)\)</span>.</li>
</ul>
</li>
</ul>
<h3 id="general-knowledge">General Knowledge<a class="headerlink" href="#general-knowledge" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Types of Robots:</strong> Arms (industrial robots), mobile carts (like in a warehouse), snakes (for pipe inspection), humanoids (like Atlas).</li>
<li><strong>Links, Joints, Degrees of Freedom (DOF):</strong> Links are the rigid parts of a robot, and joints connect them. <strong>DOF Ëá™Áî±Â∫¶</strong> is the number of independent parameters you need to define the robot's configuration (e.g., a 3-link arm moving in a 2D plane has 3 DOF; an object moving in 3D has 6 degrees of freedom: (x,y,z) position and three ways to change its orientation (roll, pitch, yaw)).</li>
<li><strong>Ideal Robot Path:</strong> The best path is not just <strong>short</strong>, but also <strong>safe</strong> (far from obstacles) and <strong>smooth</strong> (no jerky movements).</li>
<li><strong>Graph-Based Planning:</strong> Methods to simplify the world into a graph that can be searched.<ul>
<li><strong>Visibility Graphs ÂèØËßÅÊÄßÂõæË°®:</strong> Nodes are the start, goal, and corners of obstacles. Edges connect nodes that have a line of sight. Guarantees the shortest path.<ul>
<li>It produces paths of minimal length. However, these paths aren't very safe because they skim the edges of obstacles. It's risky to get too close to obstacles. </li>
</ul>
</li>
<li><strong>Skeletonization È™®Êû∂Âåñ</strong><ul>
<li><strong>Roadmap</strong> converts freespace into a skeleton that goes through the middle of regions. Paths via the skeleton stay far from obstacles, so they tend to be safe but perhaps overly long.</li>
<li><strong>Generalized Voronoi diagrams</strong> places the skeleton along curves that are equidistant from two or more obstacles. The waypoints are the places where these curves intersect.</li>
</ul>
</li>
<li><strong>Cell Decompositions ÂçïÂÖÉÂàÜËß£:</strong> The free space is divided into simple cells. A path is found by searching for a sequence of adjacent cells.</li>
<li><strong>Probabilistic Roadmaps Ê¶ÇÁéáË∑ØÁ∫øÂõæ (PRM):</strong> Random points are scattered in the free space and connected if a simple path exists between them. Great for high-dimensional C-spaces.</li>
</ul>
</li>
<li><strong>Challenges:</strong><ul>
<li><strong>Big Open Areas:</strong> Can be computationally expensive because there are so many possible paths.</li>
<li><strong>Narrow Passages:</strong> These are "bottlenecks" that are hard for planners (especially random ones like PRM) to find.</li>
<li><strong>High DOF:</strong> As the degrees of freedom increase, the volume of the C-space explodes (the "curse of dimensionality"), making it exponentially harder to search.</li>
</ul>
</li>
</ul>
<h2 id="natural-language">Natural Language<a class="headerlink" href="#natural-language" title="Permanent link">&para;</a></h2>
<h3 id="natural-language-and-hmm-history">Natural Language and HMM History<a class="headerlink" href="#natural-language-and-hmm-history" title="Permanent link">&para;</a></h3>
<p>This section covers key people, systems, and algorithms that were milestones in the history of Natural Language Processing (NLP) and related fields.</p>
<ul>
<li><strong>Julie Lovins (1968):</strong> An American linguist who developed one of the earliest <em>stemming algorithms</em> ËØçÂπ≤ÊèêÂèñÁÆóÊ≥ï, the Lovins Stemmer. Stemming is the process of reducing a word to its root or base form (e.g., "connecting," "connected" -&gt; "connect").</li>
<li><strong>SHRDLU (1970):</strong> An early <em>NLP program</em> created by Terry Winograd at MIT. It operated in a simulated "blocks world" and could understand and execute commands in natural English like "pick up the big red block and put it on the green one." It was a landmark example of "deep" understanding within a very limited domain.</li>
<li><strong>Zork transcript (late 1970s):</strong> A classic <em>text-based adventure game</em>. It was notable for its relatively sophisticated text parser that could understand more complex commands than its contemporaries, making the game feel more interactive and intelligent.</li>
<li><strong>Pattern Playback (1950s):</strong> A device developed at Haskins Laboratories that <em>converted visual patterns of speech (spectrograms) back into sound</em> Â∞ÜÂ£∞Ë∞±ÂõæËΩ¨Êç¢ÂõûÂ£∞Èü≥. It was crucial for discovering the acoustic cues that humans use to perceive speech sounds.</li>
<li><strong>John Kelly and Louis Gerstman (1961):</strong> Researchers at Bell Labs who programmed an IBM 704 computer to <em>synthesize speech</em> ÂêàÊàêËØ≠Èü≥. They famously made it sing the song "Daisy Bell," which later inspired the iconic scene in <em>2001: A Space Odyssey</em>.</li>
<li><strong>Parsey McParseface (2016):</strong> <em>An open-source English language parser</em> ÂºÄÊ∫êËã±ËØ≠ËØ≠Ë®ÄËß£ÈáäÂô® released by Google. It was noteworthy for its high accuracy, achieved using a deep learning model (SyntaxNet).</li>
<li><strong>Baum-Welch algorithm (c. 1970):</strong> An algorithm used to <strong>train</strong> a Hidden Markov Model (HMM). It's an unsupervised learning algorithm, meaning it can determine the transition and emission probabilities of an HMM when you only have the observed sequences (e.g., words) but not the hidden states (e.g., POS tags). It's a specific instance of the Expectation-Maximization (EM) algorithm. ËÆ≠ÁªÉHMMÁöÑÁÆóÊ≥ï</li>
<li><strong>Viterbi algorithm (1967):</strong> A dynamic programming algorithm used for <strong>decoding</strong> in an HMM. Âú®HMM‰∏≠ËøõË°åËß£Á†ÅÁöÑÂä®ÊÄÅËßÑÂàíÁÆóÊ≥ï. Given a sequence of observations (like words), it finds the single <strong>most likely</strong> sequence of hidden states (like POS tags) that could have produced them. </li>
<li><strong>GPT-3 (2020):</strong> A massive language model developed by OpenAI. As a transformer-based model with 175 billion parameters, it demonstrated a remarkable ability to generate fluent, human-like text and perform a wide variety of NLP tasks with little to no specific training ("few-shot" learning).</li>
<li><strong>ILEX system (late 1990s):</strong> <em>An intelligent labeling system for a museum</em> ÂçöÁâ©È¶ÜÁöÑÊô∫ËÉΩÊ†áÁ≠æÁ≥ªÁªü. It generated dynamic descriptions of exhibits (in this case, jewelry) that were tailored to the individual user, taking into account what they had already seen to provide context and comparisons.</li>
</ul>
<h3 id="outline">Outline<a class="headerlink" href="#outline" title="Permanent link">&para;</a></h3>
<p>The area of natural language and speech processing covers algorithms for three types of tasks: </p>
<ul>
<li>convert text/speech into structured information usable by an AI reasoning system
    Â∞ÜÊñáÊú¨/ËØ≠Èü≥ËΩ¨Êç¢‰∏∫‰∫∫Â∑•Êô∫ËÉΩÊé®ÁêÜÁ≥ªÁªüÂèØÁî®ÁöÑÁªìÊûÑÂåñ‰ø°ÊÅØ</li>
<li>generate fluent text/speech from structured information
    ‰ªéÁªìÊûÑÂåñ‰ø°ÊÅØÁîüÊàêÊµÅÁïÖÁöÑÊñáÊú¨/ËØ≠Èü≥</li>
<li>translate directly between two types of text/speech (e.g. between two languages, simple types of question answering)
    Áõ¥Êé•Âú®‰∏§ÁßçÁ±ªÂûãÁöÑÊñáÊú¨/ËØ≠Èü≥‰πãÈó¥ËøõË°åÁøªËØëÔºà‰æãÂ¶Ç‰∏§ÁßçËØ≠Ë®Ä‰πãÈó¥„ÄÅÁÆÄÂçïÁ±ªÂûãÁöÑÈóÆÁ≠îÔºâ</li>
</ul>
<p>Natural language processing has roughly four layers Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ§ßËá¥ÊúâÂõõÂ±ÇÔºö</p>
<ul>
<li>Semantics (meaning, dialog structure) ËØ≠‰πâÔºàÂê´‰πâ„ÄÅÂØπËØùÁªìÊûÑÔºâ</li>
<li>Mid-level processing (e.g. part of speech, parsing) ‰∏≠Á∫ßÂ§ÑÁêÜÔºà‰æãÂ¶ÇËØçÊÄß„ÄÅËß£ÊûêÔºâ</li>
<li>Low-level text processing (e.g. forming words, finding morphemes) ‰ΩéÁ∫ßÊñáÊú¨Â§ÑÁêÜÔºà‰æãÂ¶ÇÔºåÂΩ¢ÊàêÂçïËØç„ÄÅÊü•ÊâæËØçÁ¥†Ôºâ</li>
<li>Speech  ÊºîËÆ≤</li>
</ul>
<h3 id="types-of-systems">Types of systems<a class="headerlink" href="#types-of-systems" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Sample tasks Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑ‰ªªÂä°:</strong> Common goals in NLP include:</p>
<ul>
<li><strong>Translation:</strong> Converting text from one language to another.</li>
<li><strong>Question Answering:</strong> Providing a direct answer to a question posed in natural language.</li>
<li><strong>Summarization:</strong> Creating a short, coherent summary of a longer text.</li>
<li><strong>Sentiment Analysis:</strong> Determining the emotional tone (positive, negative, neutral) of a piece of text.</li>
<li><strong>Information Extraction:</strong> Pulling structured information from unstructured text (e.g., finding dates, names, and locations in a news article).</li>
</ul>
</li>
<li>
<p><strong>Deep vs. shallow:</strong> This describes the level of linguistic understanding a system aims for.</p>
<ul>
<li><strong>Shallow Systems ÊµÖÂ±Ç:</strong> Operate on surface-level text features. They use statistics, pattern matching (like regular expressions), and simple rules without building a full representation of meaning.<ul>
<li><em>Examples:</em> Keyword-based search engines, spam filters looking for specific phrases, most sentiment analysis tools, spelling correction.</li>
</ul>
</li>
<li><strong>Deep Systems Ê∑±Â±Ç:</strong> Attempt to understand the syntactic and semantic structureÔºàÂè•Ê≥ïÂíåËØ≠‰πâÁªìÊûÑÔºâ of the language. They build an internal representation of meaning to perform their task.<ul>
<li><em>Examples:</em> SHRDLU (in its limited world), advanced question-answering systems, modern LLMs like GPT-3.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Danger of seeming too fluent and the "uncanny valley" ÊÅêÊÄñË∞∑ÊïàÂ∫î:</strong> The uncanny valley is a phenomenon where a system that is <em>almost</em> perfectly human-like is perceived as eerie or unsettling. A chatbot that is 99% fluent but makes one bizarre logical error can be more disturbing than a simple, obviously robotic bot because it violates our expectations of a seemingly intelligent agent.</p>
<p>ÊÅêÊÄñË∞∑ÊïàÂ∫îÊòØÊåá‰∏Ä‰∏™Âá†‰πéÂÆåÁæéÊ®°‰ªø‰∫∫Á±ªÁöÑÁ≥ªÁªüÔºåÂç¥‰ºöËÆ©‰∫∫ÊÑüËßâÊÄ™ÂºÇÊàñ‰∏çÂÆâ„ÄÇ‰∏Ä‰∏™ËÅäÂ§©Êú∫Âô®‰∫∫Âç≥‰ΩøËÉΩËææÂà∞ 99% ÁöÑÊµÅÂà©Á®ãÂ∫¶ÔºåÂç¥ÁäØ‰∫Ü‰∏Ä‰∏™Â•áÊÄ™ÁöÑÈÄªËæëÈîôËØØÔºåÂèØËÉΩÊØî‰∏Ä‰∏™ÁÆÄÂçï„ÄÅÊòéÊòæÊòØÊú∫Âô®‰∫∫ÁöÑÊú∫Âô®‰∫∫Êõ¥‰ª§‰∫∫‰∏çÂÆâÔºåÂõ†‰∏∫ÂÆÉËøùËÉå‰∫ÜÊàë‰ª¨ÂØπÁúã‰ººÊô∫ËÉΩÁöÑ‰ª£ÁêÜÁöÑÈ¢ÑÊúü„ÄÇ</p>
</li>
</ul>
<h3 id="processing-pipeline">Processing pipeline<a class="headerlink" href="#processing-pipeline" title="Permanent link">&para;</a></h3>
<p>A classic test is a <strong>circular translation</strong>: X to Y and then back to X. </p>
<p>This is a typical sequence of steps to process raw text.</p>
<ol>
<li><strong>Speech:</strong> Raw audio is processed to understand its components.<ul>
<li><strong>Waveform Ê≥¢ÂΩ¢:</strong> The raw audio signal, representing air pressure changes over time.</li>
<li><strong>Spectrogram Â£∞Ë∞±Âõæ:</strong> A visual representation showing the intensity of different frequencies in the sound over time. It's how we "see" sound.</li>
<li><strong>Formants ÂÖ±ÊåØÂ≥∞:</strong> The resonant frequency peaks in the spectrogram, which are characteristic of vowel ÂÖÉÈü≥ sounds. They are the key acoustic features that let us distinguish 'ee' from 'oo'.</li>
<li><strong>Phones Èü≥Á¥†:</strong> The smallest distinct units of sound in a language (e.g., the sounds [b], [√¶], [t] in "bat").</li>
<li><strong>Synthesis ÂêàÊàê:</strong> Creating artificial speech. <strong>Formant synthesis</strong> generates sounds electronically based on formant frequencies, while <strong>concatenative synthesis</strong> stitches together pre-recorded snippets of human speech.</li>
</ul>
</li>
<li><strong>Dividing into words (Tokenization):</strong> Splitting a stream of text into individual words or tokens.</li>
<li><strong>Normalizing words (Stemming/Lemmatization):</strong> Reducing words to their base form (e.g., "running" -&gt; "run").</li>
<li><strong>Finding morphemes ËØçÁ¥†:</strong> Breaking words into their smallest meaningful units (e.g., <code>un-systematic-ally</code>).<ul>
<li>Morpheme: smallest meaningful chunk</li>
</ul>
</li>
<li><strong>Part of speech (POS) tagging:</strong> Assigning a grammatical category (noun, verb, etc.) to each word.</li>
<li><strong>Parsing:</strong> Analyzing the grammatical structure of a sentence.<ul>
<li><strong>Constituency ÈÄâÂå∫ vs. dependency trees:</strong> Two ways to represent sentence structure.<ul>
<li>A <strong>constituency tree</strong> breaks a sentence down into nested phrases (Noun Phrase, Verb Phrase).</li>
<li>A <strong>dependency tree</strong> shows how words are related to each other through directed links (e.g., an adjective modifying a noun, a subject related to a verb).</li>
</ul>
</li>
<li><strong>Shallow parsers ÊµÖÂ±ÇËß£ÊûêÂô® (Chunking):</strong> These parsers don't build a full, detailed tree. Instead, they just identify the main, non-nested phrases (chunks) like noun phrases and verb phrases.</li>
<li><strong>Unlexicalized, class-based, Lexicalized:</strong> These refer to the information used by a statistical parser. <strong>Unlexicalized</strong> parsers use only POS tags. <strong>Class-based</strong> parsers use semantic classes (e.g., "city," "person"). <strong>Lexicalized</strong> parsers use rules tied to specific words, making them more powerful but data-hungry.</li>
</ul>
</li>
<li><strong>Semantics:</strong> Understanding the meaning of the text.<ul>
<li><strong>Shallow semantics ÊµÖÂ±ÇËØ≠‰πâ:</strong> Extracting limited, specific meaning without full understanding.<ul>
<li>word classes ËØçÁ±ª</li>
<li>word sense disambiguation ËØç‰πâÊ∂àÊ≠ß</li>
<li>semantic role labelling: we know that a noun phrase X relates to a verb Y. Is X the subject/actor? the object that the action was done to? a tool used to help with the action?</li>
<li>co-reference resolution (see below)</li>
</ul>
</li>
<li><strong>Sentiment analysis:</strong> Identifying positive/negative opinions.</li>
<li><strong>Semantic Role Labeling (SRL) ËØ≠‰πâËßíËâ≤Ê†áËÆ∞:</strong> Figuring out "who did what to whom" (e.g., identifying the agent, patient, and instrument of an action).</li>
<li><strong>Co-reference resolution Êåá‰ª£Áõ∏Âêå:</strong> Determining which words refer to the same entity (e.g., in "Sue said <strong>she</strong> was tired," resolving "<strong>she</strong>" to "Sue").</li>
<li><strong>Dialog coherency:</strong> Analyzing how well sentences in a conversation logically follow one another.</li>
</ul>
</li>
</ol>
<h3 id="pos-tagging">POS Tagging<a class="headerlink" href="#pos-tagging" title="Permanent link">&para;</a></h3>
<p>Part-of-Speech tagging ËØçÊÄßÊ†áÊ≥® is a fundamental NLP task.</p>
<ul>
<li><strong>Common POS tags Ê†áÁ≠æÈõÜ:</strong> You should be familiar with the basics like <strong>Noun</strong> (NN), <strong>Proper Noun</strong> (NNP), <strong>Verb</strong> (VB), <strong>Adjective</strong> (JJ), <strong>Determiner</strong> (DT, e.g., "the", "a"), and <strong>Preposition</strong> (IN).</li>
<li><strong>Size of typical POS tag sets:</strong> Tag sets vary. Simple ones might have ~12-15 universal tags, while more detailed ones like the <strong>Penn Treebank tag set</strong> (very common in English NLP) have around 45 tags.</li>
<li><strong>Single word with multiple possible tags:</strong> Many words are ambiguous. For example, "book" can be a noun ("read the book") or a verb ("book a flight"). The context determines the correct tag.</li>
<li><strong>Baseline tagging algorithm Âü∫Á∫øÁÆóÊ≥ï:</strong> The simplest possible method with an acuuracy about 91%. For each word in the test set, assign it the tag that it was most frequently associated with in the training data. For a word that was never seen in training, you might just default to the most common tag overall (like Noun).</li>
</ul>
<h2 id="hmms">HMMs<a class="headerlink" href="#hmms" title="Permanent link">&para;</a></h2>
<p><strong>Hidden Markov Models(ÈöêÈ©¨Â∞îÂèØÂ§´Ê®°Âûã)</strong> are a statistical tool perfect for sequence labeling tasks like POS tagging. Á±ª‰ººË¥ùÂè∂ÊñØÁΩëÁªú</p>
<p><span class="arithmatex">\(W\)</span> is the imput word sequence and <span class="arithmatex">\(T\)</span> is an input tag sequence.</p>
<p><strong>Goal</strong>: maximize <span class="arithmatex">\(P(T|W)\)</span></p>
<ul>
<li>
<p><strong>Markov assumptions:</strong> An HMM is based on two simplifying assumptions:</p>
<ol>
<li><strong>Transition Assumption ËΩ¨Áßª:</strong> The probability of the current state (tag) depends <strong>only on the previous state</strong> (tag). <span class="arithmatex">\(P(t_i‚à£t_{i‚àí1},t_{i‚àí2},...,t_1)‚âàP(t_i‚à£t_{i‚àí1})\)</span>.</li>
<li><strong>Emission Assumption ÊéíÊîæ:</strong> The probability of the current observation (word) depends <strong>only on the current state</strong> (tag). <span class="arithmatex">\(P(w_i‚à£w_1,t_1,...,w_{i‚àí1},t_{i‚àí1},t_i)‚âàP(w_i‚à£t_i)\)</span>.</li>
</ol>
</li>
<li>
<p><strong>Graphical model picture:</strong> An HMM has a sequence of hidden states (tags) on top and a sequence of observed states (words) on the bottom. There are <strong>transition</strong> arrows between adjacent hidden states and <strong>emission</strong> arrows from each hidden state down to its corresponding observed state.</p>
</li>
<li>
<p><strong>Component probabilities:</strong></p>
<ul>
<li><strong>Initial:</strong> <span class="arithmatex">\(P(t_1)\)</span> - The probability that a sentence starts with a certain tag.</li>
<li><strong>Emission:</strong> <span class="arithmatex">\(P(w_i‚à£t_i)\)</span> - The probability of seeing word <span class="arithmatex">\(w_i\)</span> given that its tag is <span class="arithmatex">\(t_i\)</span>.</li>
<li><strong>Transition:</strong> <span class="arithmatex">\(P(t_i‚à£t_{i‚àí1})\)</span> - The probability that tag <span class="arithmatex">\(t_i\)</span> will follow tag <span class="arithmatex">\(t_{i‚àí1}\)</span>.</li>
</ul>
</li>
<li>
<p>Equations for computing probability: The probability of a given word sequence W and a given tag sequence T is calculated by multiplying all the relevant probabilities together:
    $$
    P(W,T)=‚àè<em i="1">{i=1}^{n}P(w_i‚à£t_i)\
    P(T)=P(t_1|START)‚àó‚àè</em>)\
    \text{where } P(t_1|START)\text{ is the probability of tag t1 at the start of a sentence.}
    $$}^{n}P(t_k‚à£t_{k‚àí1</p>
</li>
<li>
<p>Bayes: <span class="arithmatex">\(P(T | W) =P(W‚à£T)‚àóP(T)/P(W)\)</span>, then 
    $$
    P(T‚à£W)‚àùP(W‚à£T)\times P(T)\
    =‚àè<em k="2">{i=1}^nP(w_i‚à£t_i) \times P(t_1‚à£START) ‚àó ‚àè</em>)
    $$}^nP(t_k‚à£t_{k‚àí1</p>
</li>
<li>
<p><strong>Tag transition as a finite-state automaton(FSA) ÊúâÈôêÁä∂ÊÄÅÊú∫:</strong> You can view the tags as states in an automaton. ÊØèÊù°Âºß‰∏äÁöÑÊï∞Â≠óË°®ÂèëÁîüËΩ¨Êç¢ÂèëÁîüÁöÑÊ¶ÇÁéá
    The transition probabilities <span class="arithmatex">\(P(t_i‚à£t_{i‚àí1})\)</span> are the probabilities on the arcs connecting the states.</p>
</li>
<li>
<p><strong>When does an HMM need smoothing ÂÖâÊªë?</strong> Smoothing is required whenever you encounter a <strong>zero-count event</strong> in your training data. This happens for:</p>
<ul>
<li><strong>Emission probabilities:</strong> A word that appeared in training is seen with a new, valid tag in the test data. <span class="arithmatex">\(P_E(w_i|t_i)\)</span> ÊéíÊîæÊ¶ÇÁéá</li>
<li><strong>Transition probabilities</strong>: A sequence of two tags that is possible but never occurred in the training data. <span class="arithmatex">\(P_T(t_k|t_{k‚àí1})\)</span> ËΩ¨ÁßªÊ¶ÇÁéá<ul>
<li>Without smoothing (like Add-1 smoothing), these events would be assigned a probability of 0, making the entire sequence probability 0. Èò≤Ê≠¢Ê¶ÇÁéá‰∏∫0</li>
</ul>
</li>
<li>It's essential to smooth, i.e. reserve some of each tag's probability for new words and familiar words seen with a new tag</li>
</ul>
</li>
<li>
<p><strong>Estimating probability of unseen phenomena:</strong></p>
<ul>
<li><strong>Unseen words:</strong> Words that were not in the training data at all. A common technique is to replace all rare words (e.g., <strong>hapax</strong> words that appear only once) in the training data with a special <code>&lt;UNK&gt;</code> (unknown) token. You can then learn emission probabilities for <code>&lt;UNK&gt;</code>, which you apply to any new word you encounter.</li>
<li><strong>Open-class ÂÆûËØç vs. closed-class ÂäüËÉΩËØç words:</strong> This helps with unknown words. <strong>Closed-class</strong> words (like determiners, prepositions) form a fixed set. An unknown word is very unlikely to be one of these. <strong>Open-class</strong> words (nouns, verbs, adjectives) frequently get new members. Therefore, an unknown word is most likely an open-class word, often a proper noun.</li>
</ul>
</li>
</ul>
<h3 id="viterbi-trellis-decoding-algorithm">Viterbi (trellis Ê£öÊû∂) decoding algorithm Áª¥ÁâπÊØîÁÆóÊ≥ï<a class="headerlink" href="#viterbi-trellis-decoding-algorithm" title="Permanent link">&para;</a></h3>
<p>This is the workhorse algorithm for finding the best tag sequence in an HMM.</p>
<p>The basic data structure is an n by m array (v), where n is the length of the input word sequence and m is the number of different (unique) tags. Each cell (k,t) in the array contains the probability v(k,t) of the best sequence of tags for w1‚Ä¶wk that ends with tag t. We also store b(k,t), which is the previous tag in that best sequence. This data structure is called a "trellis."</p>
<p>Âü∫Êú¨Êï∞ÊçÆÁªìÊûÑÊòØ‰∏Ä‰∏™ <span class="arithmatex">\(n \times m\)</span> ÁöÑÊï∞ÁªÑ (v)ÔºåÂÖ∂‰∏≠ n ÊòØËæìÂÖ•ËØçÂ∫èÂàóÁöÑÈïøÂ∫¶Ôºåm ÊòØ‰∏çÂêåÔºàÂîØ‰∏ÄÔºâÊ†áÁ≠æÁöÑÊï∞Èáè„ÄÇÊï∞ÁªÑ‰∏≠ÁöÑÊØè‰∏™ÂçïÂÖÉÊ†º (k,t) ÂåÖÂê´ w1‚Ä¶wk ‰∏≠‰ª•Ê†áÁ≠æ t ÁªìÂ∞æÁöÑÊúÄ‰Ω≥Ê†áÁ≠æÂ∫èÂàóÁöÑÊ¶ÇÁéá v(k,t)„ÄÇÊàë‰ª¨ËøòÂ≠òÂÇ®‰∫Ü b(k,t)ÔºåÂÆÉÊòØËØ•ÊúÄ‰Ω≥Â∫èÂàó‰∏≠ÁöÑÂâç‰∏Ä‰∏™Ê†áÁ≠æ„ÄÇËøôÁßçÊï∞ÊçÆÁªìÊûÑÁß∞‰∏∫‚ÄúÁΩëÊ†º‚Äù„ÄÇ</p>
<p>A path ending in some specific tag at time k might continue to any one of the possible tags at time k+1. So each cell in column k is connected to all cells in column k+1. The name "trellis" comes from this dense pattern of connections between each timestep and the next.
Âú®Êó∂Èó¥ k Â§Ñ‰ª•Êüê‰∏™ÁâπÂÆöÊ†áÁ≠æÁªìÂ∞æÁöÑË∑ØÂæÑÔºåÂèØËÉΩ‰ºöÂú®Êó∂Èó¥ k+1 Â§ÑÂª∂‰º∏Ëá≥‰ªªÊÑè‰∏Ä‰∏™ÂèØËÉΩÁöÑÊ†áÁ≠æ„ÄÇÂõ†Ê≠§ÔºåÁ¨¨ k Âàó‰∏≠ÁöÑÊØè‰∏™ÂçïÂÖÉÊ†ºÈÉΩ‰∏éÁ¨¨ k+1 Âàó‰∏≠ÁöÑÊâÄÊúâÂçïÂÖÉÊ†ºÁõ∏Ëøû„ÄÇ‚ÄúÁΩëÊ†º‚ÄùËøô‰∏ÄÂêçÁß∞Ê∫ê‰∫éÊØè‰∏™Êó∂Èó¥Ê≠•‰∏é‰∏ã‰∏Ä‰∏™Êó∂Èó¥Ê≠•‰πãÈó¥ËøôÁßçÂØÜÈõÜÁöÑËøûÊé•Ê®°Âºè„ÄÇ</p>
<ul>
<li>
<p><strong>Building the trellis:</strong> The trellis is a chart where columns represent the words in the sentence and rows represent all possible tags.</p>
<ol>
<li>
<p><strong>Initialization (First Column):</strong> For the first word, each cell (tag) is filled with <span class="arithmatex">\(v(1,t) = P_S(tag_1)√óP_E(word_1‚à£tag_1)\)</span>.</p>
</li>
<li>
<p><strong>Recursion (Subsequent Columns):</strong> To calculate the value for a cell at word <code>i</code> and tag <code>j</code>, you look at all the cells in the previous column (<code>i-1</code>). For each previous tag <code>k</code>, you calculate the probability of the path coming from it: <code>prev_prob(k) * transition_prob(k -&gt; j) * emission_prob(word_i | tag_j)</code>.</p>
<p>For each tag tagB,
$$
v(k+1,tagB)=max_{tag_A} v(k,tagA)‚àóP_T(tagB‚à£tagA)‚àóP_E(w_{k+1}‚à£tagB)\
b(k+1,tagB)=argmax_{tag_A} v(k,tagA)‚àóP_T(tagB‚à£tagA)‚àóP_E(w_{k+1}‚à£tagB)
$$
‰πüÂ∞±ÊòØËØ¥ÔºåÊàë‰ª¨‰∏∫ÊâÄÊúâÂèØËÉΩÁöÑÊ†áÁ≠æ tagA ËÆ°ÁÆó v(k,tagA)‚àóPT(tagB‚à£tagA)‚àóPE(wk+1‚à£tagB) „ÄÇÊúÄÂ§ßÂÄºÂ≠òÂÖ•ÁΩëÊ†ºÂçïÂÖÉ v(k+1,tagB) ÔºåËÄå tagA ÁöÑÂØπÂ∫îÂÄºÂàôÂ≠òÂÇ®Âú® b(k+1,tagB) ‰∏≠„ÄÇ</p>
</li>
<li>
<p>The value for the cell <code>(i, j)</code> is the <strong>maximum</strong> of these calculated path probabilities. You also store a <strong>backpointer</strong> indicating which previous cell (<code>k</code>) gave you this maximum value.
    ÂΩìÊàë‰ª¨Â°´Êª°Êï¥‰∏™ÁΩëÊ†ºÂêéÔºåÂú®ÊúÄÂêé‰∏ÄÂàóÔºàÊó∂Èó¥=nÔºâ‰∏≠ÈÄâÂèñÊúÄ‰Ω≥Ê†áÁ≠æÔºàv ÂÄºÊúÄÈ´òÔºâT„ÄÇ‰ΩøÁî® b ‰∏≠ÁöÑÂÄºÔºå‰ªé T ÂèçÂêëËøΩÊ∫ØÔºåÁîüÊàêËæìÂá∫Ê†áÁ≠æÂ∫èÂàó„ÄÇ</p>
</li>
</ol>
</li>
<li>
<p><strong>Extracting the output:</strong> After filling the whole trellis, find the cell with the highest probability in the very last column. This is the end of the most likely path. Then, follow the backpointers from that cell backward to the beginning of the sentence to reconstruct the most probable sequence of tags.</p>
</li>
<li>
<p><strong>Big-O running time:</strong> If you have T words and N possible tags, the complexity is <span class="arithmatex">\(O(T√óN^2)\)</span>. For each word (T), you calculate values for each tag (N), and each of those calculations involves looking at all previous tags (N).</p>
</li>
<li>
<p><strong>Extensions:</strong></p>
<ul>
<li><strong>Bigram:</strong> The standard HMM uses a bigram model for tags <span class="arithmatex">\(P(t_i‚à£t_{i‚àí1})\)</span>. You can extend this to a <strong>trigram</strong> model <span class="arithmatex">\(P(t_i‚à£t_{i‚àí2},t_{i‚àí1})\)</span> for more accuracy, but this squares the number of states (from N to <span class="arithmatex">\(N^2\)</span>) and increases runtime to <span class="arithmatex">\(O(T√óN^3)\)</span>.</li>
<li><strong>Guessing from word form:</strong> To handle unknown words, you can supplement the HMM with a guesser based on morphology ÂΩ¢ÊÄÅÂ≠¶. If an unknown word is capitalized, it's likely a Proper Noun. If it ends in "-ly," it's likely an Adverb. This significantly improves performance on real-world text.</li>
</ul>
</li>
</ul>
<h2 id="computer-vision">Computer Vision<a class="headerlink" href="#computer-vision" title="Permanent link">&para;</a></h2>
<h3 id="applications-of-computer-vision">Applications of Computer Vision<a class="headerlink" href="#applications-of-computer-vision" title="Permanent link">&para;</a></h3>
<p>Computer vision can used for four basic types of tasks:</p>
<ul>
<li>Obstacle avoidance ÈÅøÈöú</li>
<li>Classification ÂàÜÁ±ª</li>
<li>3D reconstruction</li>
<li>Image generation</li>
<li>Making predictions</li>
</ul>
<h3 id="image-formation">Image formation<a class="headerlink" href="#image-formation" title="Permanent link">&para;</a></h3>
<ul>
<li>Image formation ÂõæÂÉèÁîüÊàê(pinhole camera ÈíàÂ≠îÁõ∏Êú∫Ê®°Âûã, real lensesÁúüÂÆûÈïúÂ§¥, human eye)</li>
<li>Digitization (computer, human, including color)</li>
<li>Edge detection, segmentation</li>
</ul>
<p>Relating 2D to 3D: why might an object look different in two pictures?</p>
<ol>
<li>
<p><strong>Viewpoint</strong>: The camera's position and angle relative to the object.</p>
</li>
<li>
<p><strong>Illumination</strong>: The direction, color, and intensity of light.</p>
</li>
<li>
<p><strong>Occlusion</strong>: The object being partially hidden by another object.</p>
</li>
<li>
<p><strong>Scale</strong>: The object's distance from the camera.</p>
</li>
</ol>
<p>Classification:</p>
<ul>
<li>Identifying/naming objects in a picture</li>
<li>Localizing/registering objects within a picture</li>
<li>Visual question answering, captioning, semantic role labelling for a picture</li>
</ul>
<p>Reconstructing 3D geometry</p>
<ul>
<li>
<p>why is it useful?</p>
<ul>
<li>it allows a computer system to understand the true shape, size, and layout of the world from flat 2D images. This understanding is critical for interacting with or analyzing a 3D environment.</li>
</ul>
</li>
<li>
<p>from multiple 2D views of a scene</p>
</li>
<li>from a single picture</li>
</ul>
<p>Other tasks</p>
<ul>
<li>Image generation</li>
<li>Predicting the future</li>
</ul>
<h2 id="classifiers">Classifiers<a class="headerlink" href="#classifiers" title="Permanent link">&para;</a></h2>
<h3 id="important-names-datasets">Important Names &amp; Datasets<a class="headerlink" href="#important-names-datasets" title="Permanent link">&para;</a></h3>
<p>Know brief facts about the following people and systems/models:</p>
<ul>
<li><strong>William Labov</strong>: A linguistËØ≠Ë®ÄÂ≠¶ÂÆ∂, not a computer scientist. His work showed that language variation isn't random but is correlated with social factors. This is relevant to AI because it highlights the importance of <strong>context and bias</strong> in language data.</li>
<li><strong>The BERT language model</strong>(Bidirectional Encoder Representations from Transformers) Âü∫‰∫é Transformer ÁöÑÂèåÂêëÁºñÁ†ÅÂô®Ë°®Á§∫: A powerful language model that reads an entire sentence at once (bidirectionally) to understand context. It's a foundational model for many modern NLP tasks.</li>
<li><strong>CFAR-10 dataset</strong>: A classic computer vision dataset consisting of 60,000 small (32x32 pixel) color images in 10 classes (e.g., airplane, dog, truck). It's commonly used to benchmark new classification algorithms.</li>
</ul>
<h3 id="general-design">General design<a class="headerlink" href="#general-design" title="Permanent link">&para;</a></h3>
<ul>
<li>Uses for classification: labelling objects, making decisions</li>
<li>Multi-layer systems</li>
<li>What can we tune? Ë∞ÉÊï¥<ul>
<li><strong>parameters</strong> ÂèÇÊï∞ (e.g. weights)<ul>
<li>values learned directly from the training set (e.g., probabilities in Bayes nets, weights on the elements in neural nets) Ê®°Âûã<strong>Áõ¥Êé•‰ªéËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Â≠¶‰π†ÁöÑ</strong>ÂÄº„ÄÇÂèØ‰ª•Â∞ÜÂÆÉ‰ª¨ËßÜ‰∏∫Ê®°ÂûãÁöÑÂÜÖÈÉ®Áü•ËØÜ„ÄÇ‰æãÂ¶ÇÁ•ûÁªèÁΩëÁªú‰∏≠ÁöÑÊùÉÈáçÊàñË¥ùÂè∂ÊñØÁΩëÁªú‰∏≠ÁöÑÊ¶ÇÁéá</li>
</ul>
</li>
<li><strong>hyper-parameters</strong> Ë∂ÖÂèÇÊï∞ (e.g. tuning constants)<ul>
<li>constants adjusted using the development data (e.g. the Laplace smoothing constant in naive Bayes) <strong>Âú®ËÆ≠ÁªÉÂºÄÂßãÂâçËÆæÁΩÆÁöÑ</strong>ÂÄºÔºåÁî®‰∫éÈÖçÁΩÆÂ≠¶‰π†ËøáÁ®ã„ÄÇÊÇ®ÂèØ‰ª•‰ΩøÁî®ÂºÄÂèëÔºàÈ™åËØÅÔºâÊï∞ÊçÆÊù•Ë∞ÉÊï¥ÂÆÉ‰ª¨„ÄÇ‰æãÂ¶ÇÔºåÂ≠¶‰π†Áéá„ÄÅk-NN ‰∏≠ÁöÑ‚Äúk‚ÄùÂÄºÊàñÊãâÊôÆÊãâÊñØÂπ≥ÊªëÂ∏∏Êï∞</li>
</ul>
</li>
<li>design, network topology</li>
</ul>
</li>
<li><strong>Challenges</strong> with determining the correct answer<ul>
<li>How specific/general should the class <strong>label</strong> be?</li>
<li>unfamiliar objects, unfamiliar words</li>
<li><strong>context</strong> may affect best label to choose</li>
<li>deciding what's important in complex scenes, extended sentences</li>
</ul>
</li>
<li>Data for supervised training Ëá™ÊàëÁõëÁù£Â≠¶‰π†<ul>
<li>"gold" answers</li>
<li><strong>Noise</strong> in "correct" answers/annotation</li>
<li>Annotators with limited training</li>
<li>Data scraped off the web</li>
<li>Data available only for final output of system</li>
</ul>
</li>
<li>Workarounds of limited training data<ul>
<li><strong>Re-purposing layers</strong> ÈáçÊñ∞Âà©Áî®Â±ÇÔºàËøÅÁßªÂ≠¶‰π†Ôºâ trained for another purpose</li>
<li>Creating training pairs by removing information</li>
<li>Self-supervised, semi-supervised, unsupervised methods</li>
</ul>
</li>
<li>Batch vs. incremental training</li>
</ul>
<h2 id="k-nn-and-decision-trees">K-nn and Decision Trees<a class="headerlink" href="#k-nn-and-decision-trees" title="Permanent link">&para;</a></h2>
<p>Specific techniques</p>
<h3 id="k-nearest-neighbors">k-nearest neighbors<a class="headerlink" href="#k-nearest-neighbors" title="Permanent link">&para;</a></h3>
<p>(how it works, what happens if you change k)</p>
<ul>
<li>
<p><strong>k-Nearest Neighbors (k-NN)</strong>: A simple classification algorithm. To classify a new data point, it looks at the 'k' closest points in the training data and takes a majority vote of their labels.</p>
<ul>
<li><strong>Effect of k</strong>:<ul>
<li><strong>Small k</strong>: The model is sensitive to noise and can create complex, "wiggly" decision boundaries. Prone to <strong>overfitting</strong>. ÂÆπÊòìËøáÊãüÂêà</li>
<li><strong>Large k</strong>: The model is more robust to noise and creates smoother boundaries, but may misclassify points in less dense regions. Prone to <strong>underfitting</strong>. ÂÆπÊòìÊ¨†ÊãüÂêà</li>
</ul>
</li>
</ul>
</li>
<li>
<p>L1 vs. L2 norm(measure distance)</p>
<ul>
<li><strong>L1 Norm (Manhattan Distance ÊõºÂìàÈ°øË∑ùÁ¶ª)</strong>: <span class="arithmatex">\(d(p, q) = \sum_{i=1}^{n} |p_i - q_i|\)</span>. Imagine moving only along grid lines in a city.</li>
<li><strong>L2 Norm (Euclidean Distance Ê¨ßÂá†ÈáåÂæóË∑ùÁ¶ª)</strong>: <span class="arithmatex">\(d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}\)</span>. A straight line, "as the crow flies."</li>
</ul>
</li>
<li>Decision trees ÂÜ≥Á≠ñÊ†ë<ul>
<li>A tree-like model where each internal node represents a "test" on an attribute (e.g., "is color blue?"), and each leaf node represents a class label. ÁêÜÊÉ≥ÊÉÖÂÜµ‰∏ãÔºåÊØè‰∏™Âè∂ËäÇÁÇπÂè™ÂåÖÂê´Âêå‰∏ÄÁ±ªÂûãÁöÑÂØπË±°</li>
</ul>
</li>
<li>Random forests ÈöèÊú∫Ê£ÆÊûó<ul>
<li>a set of shallow trees (bounded depth) Ê∑±Â∫¶ÊúâÈôêÁöÑÊµÖÂ±ÇÊ†ë, and have these trees vote on the best label to return. The set of shallow trees is created by choosing variables and possible split locations randomly.</li>
</ul>
</li>
<li><strong>Entropy ÁÜµ</strong>:<ul>
<li>Definition: A measure of impurity or uncertainty in a set of examples. Ë°°Èáè‰∏ÄÁªÑÊ†∑Êú¨‰∏≠‰∏çÁ∫ØÂ∫¶Êàñ‰∏çÁ°ÆÂÆöÊÄßÁöÑÊåáÊ†á<ul>
<li>A good way to measure the homogeneity of a collection of discrete values, such as the values in the decision tree pools. ÁÜµÊòØË°°ÈáèÁ¶ªÊï£ÂÄºÈõÜÂêàÔºà‰æãÂ¶ÇÂÜ≥Á≠ñÊ†ëÊ±†‰∏≠ÁöÑÂÄºÔºâÂêåË¥®ÊÄßÁöÑ‰∏Ä‰∏™ÈùûÂ∏∏Â•ΩÁöÑÊñπÊ≥ï</li>
</ul>
</li>
<li>How it relates to evaluating possible splits in a <strong>decision tree</strong>: In a decision tree, you want to choose the split (the question to ask) that causes the <strong>greatest reduction in entropy</strong>, moving you closer to pure, single-class subsets.</li>
<li>ÊúÄ‰ºòÂéãÁº©ÊñπÊ°àÔºöSuppose that we have a (finite) set of letter types <em>S</em>. Suppose that <em>M</em> is a finite list of letters from <em>S</em>, with each letter type (e.g. "e") possibly included more than once in M. Each letter type <em>c</em> has a probability <span class="arithmatex">\(P(c)\)</span> (i.e., its fraction of the members of <em>M</em>). An optimal compression scheme would require <span class="arithmatex">\(\log(\frac1{P(c)})\)</span> bits in each code. (All logs in this discussion are base 2.)</li>
</ul>
</li>
</ul>
<h2 id="perceptrons">Perceptrons ÊÑüÁü•Êú∫<a class="headerlink" href="#perceptrons" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>"Linearly separable" Á∫øÊÄßÂèØÂàÜ</p>
<ul>
<li>A dataset is linearly separable if we can draw a line (or, in higher dimensions, a hyperplane) that divides one class from the other. The lefthand dataset below is linearly separable; the righthand one is not.
    Â¶ÇÊûúÊàë‰ª¨ÂèØ‰ª•Áîª‰∏ÄÊù°Á∫øÔºàÊàñËÄÖÂú®È´òÁª¥Á©∫Èó¥‰∏≠ÔºåÁîª‰∏Ä‰∏™Ë∂ÖÂπ≥Èù¢ÔºâÂ∞Ü‰∏Ä‰∏™Á±ª‰∏éÂè¶‰∏Ä‰∏™Á±ªÂàÜÂºÄÔºåÂàôËØ•Êï∞ÊçÆÈõÜÊòØÁ∫øÊÄßÂèØÂàÜÁöÑ„ÄÇ</li>
</ul>
</li>
<li>
<p>Basics of how perceptrons work</p>
<ul>
<li>A perceptron is a simple model of a single neuron. It takes multiple inputs, multiplies each by a weight, sums them up, and passes the result through an activation function (like a simple step function) to produce an output (e.g., 0 or 1). The <strong>bias</strong> term can be treated as a weight for an input that is always 1.</li>
<li>ÊÑüÁü•Âô®ÊòØÂçï‰∏™Á•ûÁªèÂÖÉÁöÑÁÆÄÂçïÊ®°Âûã„ÄÇÂÆÉÊé•ÂèóÂ§ö‰∏™ËæìÂÖ•ÔºåÂ∞ÜÊØè‰∏™ËæìÂÖ•‰πò‰ª•ÊùÉÈáçÔºåÁÑ∂ÂêéÁõ∏Âä†ÔºåÊúÄÂêéÂ∞ÜÁªìÊûú‰º†ÂÖ•ÊøÄÊ¥ªÂáΩÊï∞Ôºà‰æãÂ¶ÇÁÆÄÂçïÁöÑÈò∂Ë∑ÉÂáΩÊï∞Ôºâ‰ª•‰∫ßÁîüËæìÂá∫Ôºà‰æãÂ¶Ç 0 Êàñ 1Ôºâ„ÄÇ <strong>ÂÅèÂ∑Æ</strong>È°πÂèØ‰ª•ËßÜ‰∏∫ËæìÂÖ•ÁöÑÊùÉÈáçÔºåËØ•ËæìÂÖ•ÂßãÁªà‰∏∫ 1„ÄÇ</li>
</ul>
</li>
<li>
<p>Overall training algorithm (e.g. epochs, random processing order)</p>
<ol>
<li>Initialize all weights to small random numbers.</li>
<li>Repeat for a set number of <strong>epochs</strong> (passes through the entire training set):</li>
<li>For each training example (in a shuffled, random order):</li>
</ol>
<ul>
<li>
<p>Calculate the perceptron's output.</p>
</li>
<li>
<p>If the output is incorrect, update the weights.</p>
</li>
</ul>
</li>
<li>
<p>For the perceptron, the <strong>activation function is the sign function</strong>: 1 if above the threshold, -1 if below it. The <strong>loss function</strong> for a perceptron is "zero-one loss". We get 1 for each wrong answer, 0 for each correct one. ÂØπ‰∫éÊÑüÁü•Âô®Êù•ËØ¥ÔºåÊøÄÊ¥ªÂáΩÊï∞ÊòØÁ¨¶Âè∑ÂáΩÊï∞ÔºöÈ´ò‰∫éÈòàÂÄºÂàô‰∏∫ 1Ôºå‰Ωé‰∫éÈòàÂÄºÂàô‰∏∫ -1„ÄÇÊÑüÁü•Âô®ÁöÑÊçüÂ§±ÂáΩÊï∞ÊòØ‚ÄúÈõ∂‰∏ÄÊçüÂ§±‚Äù„ÄÇÊØè‰∏™ÈîôËØØÁ≠îÊ°àÂæó 1ÔºåÊØè‰∏™Ê≠£Á°ÆÁ≠îÊ°àÂæó 0„ÄÇ</p>
</li>
<li>
<p><strong>Softmax</strong>: Softmax is a differentiable function ÂèØÂæÆÂáΩÊï∞ that approximates this discrete behavior. It's best thought of as a version of argmax.</p>
<ul>
<li>Suppose we have a sequence of classifier outputs, one per class. <span class="arithmatex">\(v_1,‚Ä¶,v_n\)</span>. Softmax maps Êò†Â∞Ñ each <span class="arithmatex">\(v_i\)</span> to <span class="arithmatex">\(\frac{e^{v_i}}{‚àë_ie^{v_i}}\)</span>. The exponentiation accentuates the contrast between the largest value and the others, and forces all the values to be positive. The denominator normalizes all the values into the range [0,1].</li>
</ul>
</li>
<li>
<p>Rule for <strong>updating perceptron weights</strong> ÊùÉÈáçÊõ¥Êñ∞ËßÑÂàô</p>
<ul>
<li>If the prediction is wrong, nudge Ë∞ÉÊï¥ the weights to make the output closer to the correct answer. The rule is: <span class="arithmatex">\(w_{new} = w_{old} + \alpha(y - \hat{y})x\)</span><ul>
<li>Where: <span class="arithmatex">\(\alpha\)</span> is the learning rate, <span class="arithmatex">\(y\)</span> is the true label, <span class="arithmatex">\(\hat{y}\)</span> is the predicted label, <span class="arithmatex">\(x\)</span> is the input feature vector.</li>
<li>This training procedure will converge Êî∂Êïõ if<ul>
<li>data are linearly separable or</li>
<li>we throttle ÈôêÂà∂ the size of the updates as training proceeds by decreasing <span class="arithmatex">\(Œ±\)</span></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Limitations</strong> of perceptrons and ways to address them</p>
<ul>
<li>
<p>A single perceptron <strong>cannot solve non-linearly separable problems</strong>, with the classic example being the <strong>XOR problem</strong>.</p>
</li>
<li>
<p>The decision boundary can only be a line.
    ÂÜ≥Á≠ñËæπÁïåÂè™ËÉΩÊòØ‰∏ÄÊù°Á∫ø„ÄÇ</p>
<blockquote>
<p>Fixes:</p>
<ul>
<li>use multiple units (neural net) ‰ΩøÁî®Â§ö‰∏™ÂçïÂÖÉÔºàÁ•ûÁªèÁΩëÁªúÔºâ</li>
<li>massage input features to make the boundary linear ÊåâÊë©ËæìÂÖ•ÁâπÂæÅ‰ΩøËæπÁïåÁ∫øÊÄßÂåñ</li>
</ul>
</blockquote>
</li>
<li>
<p>If there is overlap between the two categories, the learning process can thrash between different boundary positions. Â¶ÇÊûú‰∏§‰∏™Á±ªÂà´‰πãÈó¥Â≠òÂú®ÈáçÂè†ÔºåÂ≠¶‰π†ËøáÁ®ãÂ∞±‰ºöÂú®‰∏çÂêåÁöÑËæπÁïå‰ΩçÁΩÆ‰πãÈó¥Ê≥¢Âä®„ÄÇ</p>
<blockquote>
<p>Fixes:</p>
<ul>
<li>reduce learning rate as learning progressesÈöèÁùÄÂ≠¶‰π†ÁöÑËøõÂ±ïÈôç‰ΩéÂ≠¶‰π†Áéá</li>
<li>don't update weights any more than needed to fix mistake in current example ‰∏çË¶ÅËøáÂ∫¶Êõ¥Êñ∞ÊùÉÈáçÔºåÈô§ÈùûÊòØ‰∏∫‰∫Ü‰øÆÂ§çÂΩìÂâçÁ§∫‰æã‰∏≠ÁöÑÈîôËØØ</li>
<li>cap the maximum change in weights (e.g. this example may have been a mistake) ÈôêÂà∂ÊùÉÈáçÁöÑÊúÄÂ§ßÂèòÂåñÔºà‰æãÂ¶ÇÔºåËøô‰∏™‰æãÂ≠êÂèØËÉΩÊòØ‰∏Ä‰∏™ÈîôËØØÔºâ</li>
</ul>
</blockquote>
</li>
<li>
<p>If there is a gap between the two categories, the training process may have trouble deciding where to place the boundary line. Â¶ÇÊûú‰∏§‰∏™Á±ªÂà´‰πãÈó¥Â≠òÂú®Â∑ÆË∑ùÔºåËÆ≠ÁªÉËøáÁ®ãÂèØËÉΩÈöæ‰ª•ÂÜ≥ÂÆöÂ∞ÜËæπÁïåÁ∫øÊîæÂú®Âì™Èáå„ÄÇ</p>
<blockquote>
<p>Fix: switch to a closely-related learning method called a "support vector machines" (SVM's). The big idea for an SVM is that only examples near the boundary matter. So we try to maximize the distance between the closest sample point and the boundary (the "margin").
  Ëß£ÂÜ≥ÊñπÊ≥ïÔºöÂàáÊç¢Âà∞‰∏ÄÁßçÂØÜÂàáÁõ∏ÂÖ≥ÁöÑÂ≠¶‰π†ÊñπÊ≥ïÔºåÁß∞‰∏∫‚ÄúÊîØÊåÅÂêëÈáèÊú∫‚ÄùÔºàSVMÔºâ„ÄÇSVM ÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÊòØÂè™ÊúâËæπÁïåÈôÑËøëÁöÑÊ†∑Êú¨ÊâçÈáçË¶Å„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨Â∞ùËØïÊúÄÂ§ßÂåñÊúÄËøëÊ†∑Êú¨ÁÇπ‰∏éËæπÁïå‰πãÈó¥ÁöÑË∑ùÁ¶ªÔºà‚ÄúËæπË∑ù‚ÄùÔºâ„ÄÇ</p>
</blockquote>
</li>
</ul>
</li>
<li>
<p><strong>Multi-class perceptrons</strong></p>
<ul>
<li>To handle more than two classes, you can use a <strong>one-vs-all</strong> approach, where you train a separate perceptron for each class to distinguish it from all the others. <strong>‰∏ÄÂØπÂ§ö</strong>ÊñπÊ≥ïÔºåÂç≥‰∏∫ÊØè‰∏™Á±ªÂà´ËÆ≠ÁªÉ‰∏Ä‰∏™ÂçïÁã¨ÁöÑÊÑüÁü•Âô®. Each has its own set of weights. Join the individual outputs into one output using <strong>argmax</strong> (i.e., pick the class that has the largest output value).</li>
</ul>
</li>
<li>
<p>Comparison to Naive Bayes</p>
<ul>
<li>Perceptron is not naive bayes, because:<ul>
<li>The feature values in Naive Bayes were probabilities, but the values here are just numbers, not in the range [0,1]. We don't know whether all of them are on the same scale.</li>
<li>Naive Bayes assumes features are conditionally independent, while the Perceptron does not. It learns a weight for each feature, and these weights are adjusted based on the <em>entire input vector</em> simultaneously. This allows it to learn the relationships between correlated features. For example, it can learn to down-weight "Francisco" if "San" is already present, treating "San Francisco" more like a single signal rather than two independent ones.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="neural-nets">Neural Nets<a class="headerlink" href="#neural-nets" title="Permanent link">&para;</a></h2>
<h3 id="historical-figures">Historical Figures<a class="headerlink" href="#historical-figures" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Yann LeCun:</strong> A Turing Award-winning computer scientist and a "godfather" of deep learning. He is the "Cun" in "LeCun."<ul>
<li><strong>Relation to AI:</strong> He is the primary pioneer of <strong>Convolutional Neural Networks (CNNs)</strong>. His 1998 work on LeNet-5 for recognizing handwritten digits laid the foundation for modern computer vision. He is currently the Chief AI Scientist at Meta (FAIR).</li>
</ul>
</li>
</ul>
<h3 id="linear-classifiers">Linear Classifiers<a class="headerlink" href="#linear-classifiers" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Sample activation functions:</strong> An activation function introduces non-linearity, allowing the model to learn more than just a simple line or plane.</p>
<ul>
<li><strong>Sigmoid:</strong> <span class="arithmatex">\(f(x) = \frac{1}{1 + e^{-x}}\)</span>. It "squashes" any real-valued number into a (0, 1) range. This is useful for <em>binary classification</em>, as the output can be interpreted as a probability.</li>
<li><strong>ReLU (Rectified Linear Unit):</strong> <span class="arithmatex">\(f(x) = \max(0, x)\)</span>. It's very simple: if the input <span class="arithmatex">\(x\)</span> is positive, the output is <span class="arithmatex">\(x\)</span>; if it's negative, the output is 0. This is the most common activation function because it's computationally fast and helps prevent the <em>vanishing gradient</em> problem.</li>
</ul>
</li>
<li>
<p><strong>Sample loss functions:</strong> The loss function is a "penalty score"ÊÉ©ÁΩöÂàÜÊï∞ that measures how wrong the model's prediction is compared to the true label.</p>
<ul>
<li><strong>0/1 Loss:</strong> Score = 0 if correct, 1 if wrong. Simple to understand, but not differentiable, so it can't be used for training with gradient descent.</li>
<li><strong>L1 Loss (Mean Absolute Error):</strong> <span class="arithmatex">\(\sum |y_{true} - y_{pred}|\)</span>.</li>
<li><strong>L2 Loss (Mean Squared Error):</strong> <span class="arithmatex">\(\sum (y_{true} - y_{pred})^2\)</span>. This penalizes large errors more heavily than L1.</li>
<li><strong>Cross-Entropy Loss:</strong> The standard for classification tasks. It measures the "distance" between the predicted probability distribution and the true distribution (e.g., the one-hot label). It heavily penalizes the model for being <em>confidently wrong</em>.</li>
</ul>
</li>
<li>
<p><strong>What are we minimizing?</strong> We are minimizing the <strong>final loss score</strong>. This score is the result of a chain of functions: <code>Loss(Activation(Weighted_Sum_of_Features), True_Label)</code>. We adjust the weights to make this final score as low as possible.</p>
</li>
<li>
<p>Adjusting weights (Gradient Descent): The main update equation is:</p>
<p><span class="arithmatex">\(W_{new} = W_{old} - \eta \cdot \nabla L\)</span></p>
<ul>
<li><span class="arithmatex">\(W\)</span> is a weight.</li>
<li><span class="arithmatex">\(\eta\)</span> (eta) is the <strong>learning rate</strong> (a small number that controls the step size).</li>
<li><span class="arithmatex">\(\nabla L\)</span> is the <strong>gradient of the loss</strong> with respect to the weight. The gradient is a vector that points in the direction of the <em>steepest ascent</em> of the loss. By <em>subtracting</em> it, we "step downhill" toward a lower loss.</li>
</ul>
</li>
<li>
<p><strong>Why differentiable?</strong> ÂèØÂæÆ Gradient descent <em>requires</em> a gradient. The gradient is a derivative. If the activation and loss functions are not differentiable, we can't compute the gradient (<span class="arithmatex">\(\nabla L\)</span>), and therefore we have no idea how to update the weights to improve the model.</p>
</li>
<li>
<p><strong>One-hot representations Áã¨ÁÉ≠ÁºñÁ†Å:</strong> A way to represent categorical data. You create a vector as long as your vocabulary (or number of classes), fill it with all zeros, and place a single '1' at the index corresponding to the specific category. (e.g., <code>cat = [1, 0, 0]</code>, <code>dog = [0, 1, 0]</code>, <code>fish = [0, 0, 1]</code>).</p>
</li>
<li>
<p><strong>Softmax:</strong> An activation function used for <strong>multi-class classification</strong> ‰∏ÄÁßçÁî®‰∫é<strong>Â§öÂàÜÁ±ª</strong>ÁöÑÊøÄÊ¥ªÂáΩÊï∞. It takes a vector of raw scores (logits) from the final layer and converts them into a probability distribution, where all outputs are between 0 and 1 and sum to 1. <span class="arithmatex">\(f(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}\)</span>.</p>
</li>
</ul>
<h3 id="neural-nets_1">Neural Nets<a class="headerlink" href="#neural-nets_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Design (Hidden Layers):</strong> A neural net is simply a stack of linear classifiers, where the output of one layer (after its activation) becomes the input for the next. The layers between the input and output are called <strong>hidden layers</strong>.</li>
<li><strong>Why non-linear activation?</strong> If you stack multiple <em>linear</em> layers, the result is just another linear function. (e.g., <span class="arithmatex">\(W_2 \cdot (W_1 \cdot x) = (W_2 \cdot W_1) \cdot x\)</span>). You gain no expressive power. The non-linear activation (like ReLU) "bends" the decision boundary, allowing the network to learn complex, non-linear relationships.</li>
<li><strong>Advantages of deep networks:</strong> They can learn a <strong>hierarchy of features</strong>. Early layers learn simple patterns (e.g., edges), and subsequent layers combine those patterns to learn more complex ones (e.g., shapes, textures, object parts).</li>
<li><strong>What can they approximate?</strong> The <strong>Universal Approximation Theorem</strong> states that a neural net with just <em>one</em> hidden layer (and a non-linear activation) can approximate <em>any continuous function</em> to any desired accuracy, given enough neurons.</li>
<li><strong>Top-level update equation È°∂Â±ÇÊõ¥Êñ∞ÊñπÁ®ã:</strong> It's the same as for a linear classifier (<span class="arithmatex">\(W_{new} = W_{old} - \eta \cdot \nabla L\)</span>), but the gradient <span class="arithmatex">\(\nabla L\)</span> (which is <span class="arithmatex">\(\frac{\partial L}{\partial W}\)</span>) is much more complex to compute.</li>
<li><strong>Backpropagation ÂèçÂêë‰º†Êí≠:</strong><ul>
<li><strong>What it computes:</strong> It is the algorithm used to efficiently compute the gradient of the loss function with respect to <em>every single weight</em> in the network.</li>
<li><strong>High-level picture:</strong> It uses the <strong>chain rule</strong> of calculus. It starts by calculating the error at the <em>output</em> layer. Then, it propagates this error signal <em>backward</em>, layer by layer, to determine how much each weight in the <em>previous</em> layer contributed to the final error.</li>
<li><strong>Why forward values?</strong> To compute the gradient at a specific layer, you need two things: the <em>error signal</em> coming back from the <em>next</em> layer, and the <em>activation value</em> (from the forward pass) that was fed <em>into</em> that layer. Ë¶ÅËÆ°ÁÆóÁâπÂÆöÂ±ÇÁöÑÊ¢ØÂ∫¶Ôºå‰Ω†ÈúÄË¶Å‰∏§Ê†∑‰∏úË•øÔºöÊù•Ëá™<em>‰∏ã</em>‰∏ÄÂ±ÇÁöÑ<em>ËØØÂ∑Æ‰ø°Âè∑</em> Ôºå‰ª•ÂèäËæìÂÖ•<em>Âà∞</em>ËØ•Â±ÇÁöÑ<em>ÊøÄÊ¥ªÂÄº</em> ÔºàÊù•Ëá™ÂâçÂêë‰º†Êí≠Ôºâ„ÄÇ</li>
</ul>
</li>
</ul>
<h4 id="three-challenges-in-training">Three challenges in training<a class="headerlink" href="#three-challenges-in-training" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Symmetry breaking (Why random weights?):</strong> If you initialize all weights to 0, every neuron in a hidden layer will compute the exact same function. They will all get the exact same gradient during backpropagation and all update to the same new value. They will never specialize. <strong>Random initialization</strong> breaks this symmetry, allowing each neuron to learn a different feature.</li>
<li><strong>RegularizationÊ≠£ÂàôÂåñ (Preventing Overfitting):</strong><ul>
<li><strong>Dropout:</strong> During training, randomly "turn off" (set to zero) a fraction of neurons in a layer for each training pass. This forces the network to learn redundant representations and prevents it from relying too heavily on any single neuron.</li>
<li><strong>Weight regularization ÊùÉÈáçÊ≠£ÂàôÂåñ (L1/L2):</strong> Adding a regularization term to the loss function based on the <em>size</em> of the weights. This discourages the model from learning "extreme" (very large) weights, which are often a sign of overfitting.</li>
</ul>
</li>
<li>
<p><strong>Overfitting ËøáÊãüÂêà:</strong> When the model learns the <em>training data</em> perfectly (including its noise) but fails to <em>generalize</em> to new, unseen data. It has high training accuracy but low test accuracy. The dropout technique will reduce this problem. Another method is "data augmentation".</p>
<ul>
<li><strong>Data augmentation Êï∞ÊçÆÂ¢ûÂº∫:</strong> Creating "new" training data by applying transformations to your existing data (e.g., for images: rotating, cropping, flipping, changing brightness). This helps the model generalize better and prevents overfitting.</li>
</ul>
</li>
<li>
<p><strong>Vanishing/exploding gradients Ê∂àÂ§±/ÁàÜÁÇ∏Ê¢ØÂ∫¶:</strong> A problem in very deep networks. During backpropagation, you multiply many gradients together. </p>
<ul>
<li>If these <strong>gradients are consistently small</strong> (&lt; 1), the signal shrinks exponentially and <em>vanishes</em> by the time it reaches the early layers, so they stop learning. Ê¢ØÂ∫¶ËøáÂ∞èÔºöÂèØËÉΩÂØºËá¥Êï∞ÂÄº‰∏ãÊ∫¢ÔºåËÆ≠ÁªÉÈÄüÂ∫¶‰πü‰ºöÂèòÊÖ¢„ÄÇ</li>
<li>If the <strong>gradients are large</strong> (&gt; 1), the signal grows exponentially and <em>explodes</em>, leading to unstable training (loss becomes <code>NaN</code>). Ê¢ØÂ∫¶ËøáÂ§ßÔºöÊï∞ÂÄºÂèØËÉΩ‰ºöÊ∫¢Âá∫</li>
<li><strong>Solutions</strong><ul>
<li><strong>Leaky ReLU:</strong> A variant Âèò‰Ωì of ReLU: <span class="arithmatex">\(f(x) = \max(\alpha \cdot x, x)\)</span>, where <span class="arithmatex">\(\alpha\)</span> is a small number like 0.01. Instead of outputting 0 for negative inputs, it outputs a small "leaky" value. This ensures there is always <em>some</em> gradient, preventing neurons from "dying" (getting stuck in the zero-gradient region).</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Epochs vs. (Mini-)Batches:</strong></p>
<ul>
<li><strong>Epoch:</strong> One complete pass through the <em>entire</em> training dataset. ÂÆåÊï¥ÈÅçÂéÜ<em>Êï¥‰∏™</em>ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏ÄÊ¨°„ÄÇ</li>
<li><strong>(Mini-)Batch:</strong> The training data is too large to process at once. So, we divide it into small, random chunks called mini-batches.</li>
<li><strong>Iteration:</strong> A single update to the model's weights (one forward pass + one backward pass) using <em>one</em> mini-batch.  ‰ΩøÁî®<em>‰∏Ä‰∏™</em> mini-batch ÂØπÊ®°ÂûãÁöÑÊùÉÈáçËøõË°å‰∏ÄÊ¨°Êõ¥Êñ∞Ôºà‰∏ÄÊ¨°ÂâçÂêë‰º†Êí≠ + ‰∏ÄÊ¨°ÂêéÂêë‰º†Êí≠Ôºâ„ÄÇ</li>
</ul>
</li>
</ul>
<h3 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)<a class="headerlink" href="#convolutional-neural-networks-cnns" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>What is convolution?</strong> A mathematical operation of sliding a small matrix (called a <strong>filter</strong> or <strong>kernel</strong>) over a larger matrix (the image). At each position, you compute the element-wise product and sum, creating a new "feature map."</p>
</li>
<li>
<p><strong>How does a convolutional layer work?</strong> The layer <em>is</em> the filter (or a set of filters). The network <em>learns</em> the values (weights) <em>inside</em> the filter. <strong>Âç∑ÁßØ</strong>Â±ÇÊú¨Ë¥®<em>‰∏äÂ∞±ÊòØ</em>‰∏Ä‰∏™Êª§Ê≥¢Âô®ÔºàÊàñ‰∏ÄÁªÑÊª§Ê≥¢Âô®Ôºâ„ÄÇÁ•ûÁªèÁΩëÁªú<em>‰ºöÂ≠¶‰π†</em>Êª§Ê≥¢Âô®<em>ÂÜÖÈÉ®ÁöÑ</em>ÂÄºÔºàÊùÉÈáçÔºâ„ÄÇIn a CNN, each unit reads input only from a local region of the preceding layer ÊØè‰∏™ÂçïÂÖÉ‰ªÖ‰ªéÂâç‰∏ÄÂ±ÇÁöÑ‰∏Ä‰∏™Â±ÄÈÉ®Âå∫ÂüüËØªÂèñËæìÂÖ•</p>
</li>
<li>
<p><strong>Emphasizing edges:</strong> A filter with weights like <code>[[-1, -1, -1], [0, 0, 0], [1, 1, 1]]</code> will detect horizontal edges. A filter with <code>[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]</code> will detect vertical edges. The network learns these filters automatically.</p>
</li>
<li>
<p><strong>Depth and stride:</strong></p>
<ul>
<li><strong>Depth:</strong> how many stacked features in each level ‚ÄúÊ∑±Â∫¶‚Äù = ÊØè‰∏™Â±ÇÁ∫ß‰∏≠Â†ÜÂè†ÁöÑÁâπÂæÅÊï∞ÈáèÔºàÊª§Ê≥¢Âô®ÁöÑÊï∞ÈáèÔºâ The number of filters in the layer. A depth of 64 means the layer is learning 64 different features simultaneously.</li>
<li><strong>Stride:</strong> "stride" = how many pixels we shift mask sideways between output units
    ‚ÄúÊ≠•Èïø‚Äù = Êàë‰ª¨Âú®ËæìÂá∫ÂçïÂÖÉ‰πãÈó¥Ê®™ÂêëÁßªÂä®Êé©Á†ÅÁöÑÂÉèÁ¥†Êï∞. Stride 1 moves 1 pixel at a time (high resolution). Stride 2 moves 2 pixels (downsampling the output).</li>
</ul>
</li>
<li>
<p><strong>Pooling layer Ê±†ÂåñÂ±Ç:</strong> A layer that <em>downsamples</em> the feature map to reduce computational cost and make the model more robust to the exact <em>location</em> of a feature. ÂØπÁâπÂæÅÂõæËøõË°å<em>‰∏ãÈááÊ†∑</em>‰ª•Èôç‰ΩéËÆ°ÁÆóÊàêÊú¨ÔºåÂπ∂‰ΩøÊ®°ÂûãÂØπÁâπÂæÅÁöÑÁ°ÆÂàá<em>‰ΩçÁΩÆ</em>Êõ¥Âä†È≤ÅÊ£íÁöÑÂ±Ç„ÄÇ</p>
<ul>
<li><strong>Max Pooling:</strong> Slides a window (e.g., 2x2) over the map and takes only the <em>maximum</em> value from that window. This is useful because it "keeps" the strongest signal that a feature was detected in that region.</li>
</ul>
</li>
<li>
<p><strong>Weight/parameter sharing ÊùÉÈáç/ÂèÇÊï∞ÂÖ±‰∫´:</strong> This is the <em>key</em> concept of CNNs. The <em>exact same filter</em> (with the same weights) is applied across the <em>entire</em> image. ËøôÊòØÂç∑ÁßØÁ•ûÁªèÁΩëÁªúÔºàCNNÔºâÁöÑ<em>Ê†∏ÂøÉ</em>Ê¶ÇÂøµ„ÄÇ <em>ÂÆÉÂ∞ÜÂÆåÂÖ®Áõ∏ÂêåÁöÑÊª§Ê≥¢Âô®</em> ÔºàÊùÉÈáçÁõ∏ÂêåÔºâÂ∫îÁî®‰∫é<em>Êï¥ÂπÖ</em>ÂõæÂÉè„ÄÇThis is powerful for two reasons:</p>
<ol>
<li><strong>Efficiency ÊïàÁéá:</strong> Drastically reduces the number of parameters. You learn one 3x3 filter, not a separate weight for every pixel.</li>
<li><strong>Translation Invariance Âπ≥Áßª‰∏çÂèòÊÄß:</strong> A feature (like a horizontal edge) is detected the same way whether it's in the top-left or bottom-right corner.</li>
</ol>
</li>
<li>
<p><strong>Overall architecture:</strong></p>
<ul>
<li>
<p>A complete CNN typically contains three types of layers</p>
<ul>
<li>convolutional  Âç∑ÁßØ</li>
<li>pooling  Ê±†Âåñ</li>
<li>fully-connected  ÂÆåÂÖ®‰∫íËÅî</li>
</ul>
<p>Convolutional layers would typically be found in the early parts of the network, where we're looking at large amounts of image data. Fully-connected layers would make final decisions towards the end of the process, when we've reduced the image data to a smallish set of high-level features.
Âç∑ÁßØÂ±ÇÈÄöÂ∏∏‰Ωç‰∫éÁΩëÁªúÁöÑÊó©ÊúüÈò∂ÊÆµÔºåÁî®‰∫éÂ§ÑÁêÜÂ§ßÈáèÂõæÂÉèÊï∞ÊçÆ„ÄÇÂÖ®ËøûÊé•Â±ÇÂàôÂú®Â§ÑÁêÜËøáÁ®ãÁöÑÂêéÊúüÈò∂ÊÆµÂÅöÂá∫ÊúÄÁªàÂÜ≥Á≠ñÔºåÊ≠§Êó∂ÂõæÂÉèÊï∞ÊçÆÂ∑≤Ë¢´ÁÆÄÂåñ‰∏∫‰∏ÄÁªÑËæÉÂ∞èÁöÑÈ´òÁ∫ßÁâπÂæÅ„ÄÇ</p>
</li>
<li>
<p><strong>Early vs. Late layers:</strong> Early layers (near the input) learn simple, low-level features (edges, colors, corners). Deeper, later layers combine these to learn complex, high-level features (textures, shapes, object parts, full objects).</p>
</li>
<li>
<p><strong>Conv vs. Fully-connected (FC):</strong> <strong>Convolutional layers</strong> are used for <em>feature extraction</em> from spatial data. <strong>Fully-connected layers</strong> are used at the <em>end</em> of the network; they take the final, high-level feature maps (usually flattened into a 1D vector) and perform the <em>classification</em> task.</p>
</li>
</ul>
</li>
<li>
<p><strong>Data loaders:</strong> Utilities (like in PyTorch) that efficiently load your data. They handle creating mini-batches, shuffling the data, and loading it onto the GPU in the background so the GPU is never idle.</p>
</li>
</ul>
<h3 id="generative-adversarial">Generative &amp; Adversarial<a class="headerlink" href="#generative-adversarial" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Generative Adversarial Neural Network (GAN)ÁîüÊàêÂºèÂØπÊäóÁΩëÁªú:</strong> A <em>generative</em> model (one that creates new data) consisting of two competing networks:</p>
<ol>
<li><strong>The Generator (G):</strong> A "counterfeiter" that takes random noise as input and tries to create fake data (e.g., an image) that looks real.</li>
<li><strong>The Discriminator (D):</strong> A "detective" (just a binary classifier) that tries to tell the difference between <em>real</em> data (from the training set) and <em>fake</em> data (from the Generator).</li>
</ol>
<ul>
<li>They are trained in a zero-sum game: the Generator gets better at fooling the Discriminator, and the Discriminator gets better at catching the fakes.</li>
<li>Áî±‰∏§‰∏™Á•ûÁªèÁΩëÁªúÁªÑÊàêÔºåÂÆÉ‰ª¨ÂÖ±ÂêåÂ≠¶‰π†ËæìÂÖ•Êï∞ÊçÆÁöÑÊ®°Âûã„ÄÇÂàÜÁ±ªÂô®ËØïÂõæÂå∫ÂàÜÁúüÂÆûÁöÑËÆ≠ÁªÉÂõæÂÉèÂíåÁõ∏‰ººÁöÑ‰º™ÈÄ†ÂõæÂÉè„ÄÇÂØπÊäóÁΩëÁªúÂàôËØïÂõæÁîüÊàêÈÄºÁúüÁöÑ‰º™ÈÄ†ÂõæÂÉè„ÄÇËøô‰∫õÁΩëÁªúÂèØ‰ª•ÁîüÊàê‰ª§‰∫∫ÊÉäÂèπÁöÑÈÄºÁúüÂõæÂÉèÔºà‰æãÂ¶Ç‰∏ãÂõæ‰∏≠ÁöÑÁãóÁöÑÂõæÂÉèÔºâÔºå‰ΩÜ‰πü‰ºö‰ª•Â•áÊÄ™ÁöÑÊñπÂºèÂ§±Ë¥•Ôºà‰æãÂ¶Ç‰∏ãÂõæ‰∏≠ÁöÑ‰∏Ä‰∫õÈùíËõôÁöÑÂõæÂÉèÔºâ</li>
</ul>
</li>
<li>
<p><strong>Adversarial examples:</strong></p>
<ul>
<li><strong>Definition:</strong> Inputs that are <em>intentionally perturbed</em> with a tiny bit of noise Âä†Âô™Â£∞, <em>imperceptible to humans</em>, that causes a trained model to make a completely wrong (and often high-confidence) prediction.</li>
<li><strong>In computer vision:</strong> Adding a specific, "noisy" pattern to an image of a "panda" can make a model classify it as a "gibbon" with 99% confidence.</li>
<li><strong>In natural language:</strong> Adding invisible characters, swapping synonyms, or making subtle spelling changes to trick a spam filter or sentiment analyzer.</li>
</ul>
</li>
</ul>
<h2 id="vector-semantics">Vector Semantics ÂêëÈáèËØ≠‰πâ<a class="headerlink" href="#vector-semantics" title="Permanent link">&para;</a></h2>
<h3 id="historical-figures_1">Historical Figures<a class="headerlink" href="#historical-figures_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Eve Clark:</strong> A prominent professor of Linguistics at Stanford. Her work focuses on language acquisition in children, particularly how they learn the <em>meaning</em> of words (semantics). ‚ÄúÂØπÊØîÂéüÂàô‚ÄùÊåáÂá∫ÔºåÂΩ¢ÂºèÁöÑÂ∑ÆÂºÇËï¥Âê´ÁùÄÊÑè‰πâ‰∏äÁöÑÂ∑ÆÂºÇ„ÄÇ<ul>
<li><strong>Relation to AI:</strong> Her research, especially on the <strong>Principle of Contrast</strong> (a word's meaning is defined by what it <em>is not</em>), provides a cognitive and linguistic foundation for computational models. This idea is practically implemented in NLP models like Word2vec through <em>negative sampling</em>, where a model learns what a word <em>is</em> by contrasting it with what it <em>is not</em>.</li>
</ul>
</li>
<li><strong>Tomas Mikolov:</strong> An AI researcher (formerly at Google, now at CIIRC) who is a key figure in modern NLP.<ul>
<li><strong>Relation to AI:</strong> He is the lead author of the <strong>Word2vec</strong> papers(2013). He developed the highly efficient Skip-gram and CBOW models, which revolutionized NLP by making it possible to train high-quality word embeddings on massive datasets.</li>
</ul>
</li>
<li><strong>J. R. Firth:</strong> A 20<sup>th</sup>-century British linguist famous for the quote: <strong>"You shall know a word by the company it keeps."</strong>(1957) ‰ªéËØçËØ≠ÊâÄÂ§ÑÁöÑËØ≠Â¢ÉÂ∞±ËÉΩÂà§Êñ≠ÂÖ∂Âê´‰πâ<ul>
<li><strong>Relation to AI:</strong> This quote is the single most important concept underpinning <strong>distributional semantics</strong>. This is the entire theory behind modern word embeddings: a word's meaning can be represented by a vector of the words that commonly appear near it.</li>
</ul>
</li>
</ul>
<h3 id="word-meaning">Word meaning<a class="headerlink" href="#word-meaning" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Logic-based vs. Context-based:</strong><ul>
<li><strong>Logic-based:</strong> Defines meaning by its <em>relationships</em> in a formal system (e.g., WordNet). <code>Poodle -&gt; is-a -&gt; Dog -&gt; is-a -&gt; Canine</code>. Relies on human experts.</li>
<li><strong>Context-based (Distributional):</strong> Defines meaning by its <em>usage</em> and <em>neighbors</em> (Firth's "company it keeps"). This is the basis of all modern embeddings.</li>
</ul>
</li>
<li><strong>Principle of Contrast:</strong> (See Eve Clark). The "Principle of Contrast" states that differences in form imply differences in meaning. A word's meaning is partly defined by what it <em>is not</em>. This is used to distinguish "dog" from "cat," not just to relate "dog" to "pet."</li>
<li><strong>Vector representations:</strong><ul>
<li><strong>One-hot Áã¨ÁÉ≠ÁºñÁ†Å:</strong> A sparse vector as long as the vocab, with a '1' at the word's index. Useless for semantics, as the dot product of any two different words is 0 (meaning "no similarity").</li>
<li><strong>Word embeddings ËØçÂµåÂÖ•:</strong> represent each word as a vector of numerical feature values. Dense, low-dimensional (e.g., 300) vectors. Similar words will have similar vectors (a high cosine similarity).</li>
</ul>
</li>
<li><strong>Cosine similarity:</strong> Measures the <em>angle</em> between two vectors, ignoring their magnitude. It's the standard way to measure the similarity between two word embeddings.</li>
<li><strong>Analogies &amp; Compositionality:</strong><ul>
<li><strong>Analogies:</strong> Relationships are captured as <em>vector offsets</em>. The classic example: <span class="arithmatex">\(vector('King') - vector('Man') + vector('Woman') \approx vector('Queen')\)</span>.</li>
<li><strong>Compositionality ÁªÑÂêàÊÄß:</strong> You can (naively) get a meaning for a phrase by adding or averaging the vectors of its words (e.g., <span class="arithmatex">\(vector('blue') + vector('cheese')\)</span>).</li>
</ul>
</li>
<li><strong>How to evaluate them:</strong><ul>
<li><strong>Intrinsic ÂÜÖÂú®:</strong> Test on analogy tasks (e.g., <code>man:king :: woman:?</code>) or by comparing vector similarity scores to human-rated similarity scores.</li>
<li><strong>Extrinsic Â§ñÂú®:</strong> Use the embeddings as the input layer for a downstream task (like sentiment analysis). If the task's performance <em>improves</em>, the embeddings are good.</li>
</ul>
</li>
<li><strong>Building feature vectors:</strong><ul>
<li><strong>Relating words to documents (e.g., TF-IDF):</strong> A word's vector is based on which <em>documents</em> it appears in.</li>
<li><strong>Relating words to words (e.g., Word2vec):</strong> A word's vector is based on which <em>other words</em> appear nearby. This is the dominant method.</li>
</ul>
</li>
<li><strong>Normalization and smoothing ÂΩí‰∏ÄÂíåÂπ≥ÊªëÂ§ÑÁêÜ:</strong><ul>
<li><strong>What's wrong with raw vectors?</strong> Raw co-occurrence <em>counts</em> are skewed. They are dominated by frequent, semantically-weak words (like "the," "a") that appear with everything.</li>
<li><strong>PMI (Pointwise Mutual Information ÁÇπ‰∫í‰ø°ÊÅØ):</strong> A better measure. It asks: "How much more often do word <span class="arithmatex">\(w\)</span> and context <span class="arithmatex">\(c\)</span> co-occur than we'd expect by <em>chance</em>?" <span class="arithmatex">\(PMI(w, c) = \log \frac{P(w, c)}{P(w)P(c)}\)</span></li>
<li><strong>When is PMI negative?</strong> When two words co-occur <em>less</em> frequently than expected by chance.</li>
<li><strong>Reliable vs. Noise?</strong> Negative PMI values are often <em>unreliable</em> (just noise). This is because for rare words, we may <em>never</em> see them together, but this lack of evidence doesn't mean they are <em>anti-correlated</em>. Ë¥üÁöÑ PMI ÂÄºÈÄöÂ∏∏<em>‰∏çÂèØÈù†</em> ÔºàÂè™ÊòØÂô™Èü≥Ôºâ</li>
<li><strong>PPMI (Positive PMI):</strong> We fix this by replacing all negative PMI values with 0. Â∞ÜÊâÄÊúâË¥ü PMI ÂÄºÊõøÊç¢‰∏∫ 0„ÄÇ<span class="arithmatex">\(PPMI(w, c) = \max(0, PMI(w, c))\)</span>.</li>
</ul>
</li>
<li><strong>SVD (Singular Value Decomposition) / PCA Â•áÂºÇÂÄºÂàÜËß£:</strong> A dimensionality reduction ÈôçÁª¥technique. You can take the huge, sparse PPMI matrix and use SVD to "compress" it into short, <em>dense</em> vectors. These dense vectors are your word embeddings.</li>
</ul>
<h2 id="sequential-neural-nets">Sequential Neural Nets<a class="headerlink" href="#sequential-neural-nets" title="Permanent link">&para;</a></h2>
<h3 id="byte-pair-encoding-bpe">Byte-pair encoding (BPE)<a class="headerlink" href="#byte-pair-encoding-bpe" title="Permanent link">&para;</a></h3>
<p>BPE (byte-pair encoding) Â≠óËäÇÂØπÁºñÁ†Å and WordPiece rely primarily on compression-type criteria rather than linguistic structure (prefixes and suffixes).</p>
<ul>
<li>dictionary of all the tokens</li>
<li>‰∏ÄÁßç tokenization</li>
<li>Âú®ÊØèÊ¨°ËÆ≠ÁªÉËø≠‰ª£‰∏≠ÔºåBPE ÁªüËÆ°ËÆ≠ÁªÉËØ≠ÊñôÂ∫ì‰∏≠ÊØèÂØπÁõ∏ÈÇªËØçÂÖÉÔºà‰ªÖÈôêËØçÂÜÖËØçÂÖÉÔºâÂá∫Áé∞ÁöÑÊ¨°Êï∞</li>
<li>BPE tokens often divide words in ways that aren't linguistically meaningful, e.g. "cats" might be divided into "ca" and "ts"</li>
</ul>
<h3 id="word2vec">üìà Word2vec<a class="headerlink" href="#word2vec" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Main outline of algorithm (Skip-gram):</strong><ul>
<li>It's a "fake" task. The goal: given a <em>focus word</em> (e.g., "fox"), predict its <em>context words</em> (e.g., "quick," "brown," "jumps").</li>
<li>The "model" is a simple, shallow neural net. ÁÆÄÂçïÁöÑÊµÖÂ±ÇÁ•ûÁªèÁΩëÁªú The <em>weights</em> of its single hidden layer are the <strong>word embeddings</strong>.</li>
<li>We train this simple net on billions of (focus, context) pairs. When we're done, we <em>throw away</em> the network and <em>keep</em> the hidden layer weights (the embedding matrix).</li>
</ul>
</li>
<li><strong>Why words and contexts embedded separately?</strong> Word2vec actually learns <em>two</em> embedding matrices (<span class="arithmatex">\(W_{word}\)</span> and <span class="arithmatex">\(W_{context}\)</span>). This is a design choice that makes <em>the math and optimization</em> (like negative sampling) work out cleanly. At the end, we either discard the context matrix or average them.</li>
<li><strong>Negative sampling Ë¥üÈááÊ†∑:</strong> A highly efficient training method. Instead of using a huge, costly Softmax to predict the <em>correct</em> context word out of the entire vocab, we:<ol>
<li>Take the true pair (e.g., "fox," "jumps") as a <strong>positive example</strong>.</li>
<li>Randomly sample <span class="arithmatex">\(k\)</span> <em>negative examples</em> from the vocab (e.g., "fox," "table"; "fox," "sky").</li>
<li>Train a simple logistic regression model to distinguish the positive example (output '1') from the <span class="arithmatex">\(k\)</span> negative examples (output '0').</li>
</ol>
</li>
<li><strong>Sigmoid function:</strong> Used in negative sampling. The model outputs a raw score for a (word, context) pair. The sigmoid function turns that score into a probability (0-1) of that pair being "real."</li>
<li><strong>Word2vec details:</strong><ul>
<li><strong>Uses more negative examples:</strong> We typically use <span class="arithmatex">\(k=5\)</span> to <span class="arithmatex">\(20\)</span> negative examples for every 1 positive example. This is how the model learns the "contrast" (see Eve Clark).</li>
<li><strong>Raising context counts to a power Â∞Ü‰∏ä‰∏ãÊñá‰ø°ÊÅØÊèêÂçáÂà∞ÂπÇÊ¨°Êñπ:</strong> When <em>sampling</em> negative examples, we don't pick from the raw word frequencies. We sample from a distribution of <span class="arithmatex">\(P(w)^{0.75}\)</span>. This <em>increases</em> the probability of sampling <em>rare</em> words, making for better "hard" negative examples.</li>
<li><strong>Weighting context words:</strong> Words <em>closer</em> to the focus word are more important. This is often handled by using a <em>dynamic context window</em>, where words closer to the center are more likely to be sampled as positive examples.</li>
<li><strong>Deleting rare words, subsampling frequent ones:</strong><ul>
<li><strong>Rare:</strong> Words appearing &lt; 5 times are often just noise and are discarded.</li>
<li><strong>Frequent (Subsampling):</strong> Extremely common words like "the" or "a" provide little semantic value. They are <em>randomly deleted</em> from the training data with a high probability. This speeds up training and allows the model to learn more from rarer, more meaningful words.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="sequential-neural-nets_1">Sequential Neural Nets<a class="headerlink" href="#sequential-neural-nets_1" title="Permanent link">&para;</a></h3>
<ul>
<li>These are designed to handle <strong>sequences</strong> of data, where the output for the current element depends on previous ones. Examples include RNNs and <strong>Transformers</strong>.</li>
</ul>
<h3 id="historical-names-transformers">Historical Names (Transformers)<a class="headerlink" href="#historical-names-transformers" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Vaswani et al. (2017):</strong> Authors of the paper <strong>"Attention Is All You Need"</strong>, which introduced the <strong>Transformer</strong> architecture, completely replacing recurrent layers with <strong>self-attention</strong> mechanisms.</li>
</ul>
<h3 id="bert-bidirectional-encoder-representations-from-transformers">BERT (Bidirectional Encoder Representations from Transformers)<a class="headerlink" href="#bert-bidirectional-encoder-representations-from-transformers" title="Permanent link">&para;</a></h3>
<p>Âü∫‰∫étransformerÁöÑÂèåÂêëÁºñÁ†ÅÂô®Ë°®Á§∫</p>
<ul>
<li><strong>Type:</strong> A <strong>Masked Language Model (MLM)</strong> and next sentence prediction model. It is <strong>bidirectional</strong> (it considers both left and right context simultaneously).</li>
<li><strong>Structure:</strong> Only uses the <strong>Encoder</strong> stacks of the Transformer.</li>
<li><strong>Training:</strong> Pre-trained on two tasks: <strong>Masked Language Modeling</strong> (predicting masked words) and <strong>Next Sentence Prediction</strong> (predicting if two sentences follow each other).</li>
<li><strong>Good for:</strong> <strong>Classification</strong> and <strong>sequence-level</strong> tasks, such as sentiment analysis, question answering (SQuAD), and sentence similarity.</li>
</ul>
<h3 id="example-autoregressive-llms">Example Autoregressive LLMs<a class="headerlink" href="#example-autoregressive-llms" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>GPT</strong> (Generative Pre-trained Transformer), <strong>Llama</strong>, <strong>DeepSeek</strong>, <strong>Claude</strong></li>
<li><strong>Autoregressive:</strong> These models predict the <strong>next token</strong> in a sequence, conditioning on all previously generated tokens. They are generally used for <strong>generation</strong> tasks.</li>
</ul>
<table>
<thead>
<tr>
<th><strong>Feature ÁâπÂæÅ</strong></th>
<th><strong>Masked LLM (e.g., BERT) Êé©Á†Å LLMÔºà‰æãÂ¶ÇÔºåBERTÔºâ</strong></th>
<th><strong>Autoregressive LLM (e.g., GPT)Ëá™ÂõûÂΩíÁ∫øÊÄßÁ∫øÊÄßÊ®°ÂûãÔºà‰æãÂ¶ÇÔºåGPTÔºâ</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Prediction È¢ÑË®Ä</strong></td>
<td>Predicts tokens that are <strong>masked</strong> out in the input. È¢ÑÊµãËæìÂÖ•‰∏≠Ë¢´<strong>Â±èËîΩÁöÑ</strong>Ê†áËÆ∞„ÄÇ</td>
<td>Predicts the <strong>next token</strong> in the sequence. È¢ÑÊµãÂ∫èÂàó‰∏≠ÁöÑ<strong>‰∏ã‰∏Ä‰∏™Ê†áËÆ∞</strong> „ÄÇ</td>
</tr>
<tr>
<td><strong>Context ËØ≠Â¢É</strong></td>
<td><strong>Bidirectional</strong> (sees context from both sides). <strong>ÂèåÂêë</strong> Ôºà‰ªéÂèåÊñπÁöÑËßíÂ∫¶ÁúãÂæÖÈóÆÈ¢òÔºâ„ÄÇ</td>
<td><strong>Unidirectional</strong> (sees only preceding context). <strong>ÂçïÂêë</strong> Ôºà‰ªÖÊü•ÁúãÂâçÈù¢ÁöÑ‰∏ä‰∏ãÊñáÔºâ„ÄÇ</td>
</tr>
<tr>
<td><strong>Use Case Áî®‰æã</strong></td>
<td><strong>Classification, filling in the blanks, understanding. ÂàÜÁ±ª„ÄÅÂ°´Á©∫„ÄÅÁêÜËß£„ÄÇ</strong></td>
<td><strong>Generation, translation, conversation. ÁîüÊàê„ÄÅÁøªËØë„ÄÅÂØπËØù„ÄÇ</strong></td>
</tr>
</tbody>
</table>
<h2 id="language-modeling-fundamentals">Language Modeling Fundamentals<a class="headerlink" href="#language-modeling-fundamentals" title="Permanent link">&para;</a></h2>
<h3 id="n-gram-language-model-folks">N-gram Language Model Folks<a class="headerlink" href="#n-gram-language-model-folks" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Markov È©¨Â∞îÊüØÂ§´:</strong> The foundation of the <strong>Markov assumption</strong> used in <span class="arithmatex">\(N\)</span>-gram models: the probability of the current word depends only on the previous <span class="arithmatex">\(N-1\)</span> words, <span class="arithmatex">\(P(w_i | w_{i-1}, \ldots, w_1) \approx P(w_i | w_{i-1}, \ldots, w_{i-N+1})\)</span>.</li>
<li><strong>Shannon È¶ôÂÜú:</strong> Developed <strong>Information Theory</strong> and demonstrated how <span class="arithmatex">\(N\)</span>-grams could be used to generate text that is statistically similar to human language.</li>
<li><strong>Jelinek &amp; Baker:</strong> Key figures in the development of statistical language modeling, particularly for <strong>speech recognition</strong> in the 1970s and 80s (e.g., the Jelinek-Mercer smoothing method).</li>
</ul>
<h3 id="input-and-output">Input and Output<a class="headerlink" href="#input-and-output" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>TokenizationÂàÜËØç (e.g., Byte-pair encoding, BPE):</strong> The process of breaking down raw text into smaller units (<strong>tokens</strong>). BPE is a data-compression technique that finds the most frequently occurring adjacent characters/tokens and merges them into a new single token, creating a vocabulary that balances size and word coverage.</li>
<li><strong>Convert tokens to vectors (e.g., word2vec):</strong> Tokens are converted into dense, continuous numerical <strong>embeddings</strong> (vectors) that capture semantic meaning. <strong>Word2vec</strong> is a method for learning these embeddings.</li>
<li><strong>Positional Encoding:</strong> Since Transformers process all tokens simultaneously (no inherent sequence order), <strong>positional encoding</strong> is added to the input embeddings to inject information about the <strong>relative or absolute position</strong> of the tokens in the sequence.</li>
</ul>
<h2 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs) Âæ™ÁéØÁ•ûÁªèÁΩëÁªú<a class="headerlink" href="#recurrent-neural-networks-rnns" title="Permanent link">&para;</a></h2>
<p>Recurrent neural nets (RNNs) are neural nets that have connections that loop back from a layer to the same layer. ‰∏ÄÁßçÂÖ∑Êúâ‰ªé‰∏ÄÂ±ÇÂà∞Âêå‰∏ÄÂ±ÇÁöÑÁéØË∑ØËøûÊé•ÁöÑÁ•ûÁªèÁΩëÁªú</p>
<h3 id="high-level-view-of-how-they-work">High-Level View of How They Work<a class="headerlink" href="#high-level-view-of-how-they-work" title="Permanent link">&para;</a></h3>
<ul>
<li>An RNN maintains a <strong>hidden state</strong> vector (<span class="arithmatex">\(h_t\)</span>) that is a function of the <strong>current input</strong> (<span class="arithmatex">\(x_t\)</span>) and the <strong>previous hidden state</strong> (<span class="arithmatex">\(h_{t-1}\)</span>). This allows information to persist and flow through the sequence.</li>
</ul>
<h3 id="when-to-compute-loss">When to Compute Loss<a class="headerlink" href="#when-to-compute-loss" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Loss from last unit only:</strong> Used for <strong>sequence-to-one</strong> tasks, like <strong>classification</strong> (e.g., sentiment analysis of a movie review, only the final state is used for the output).</li>
<li><strong>Loss summed over all units:</strong> Used for <strong>sequence-to-sequence</strong> or <strong>sequence-to-label</strong> tasks, like <strong>language modeling</strong> (predicting the next word at every step) or <strong>Part-of-Speech tagging</strong>.</li>
</ul>
<h3 id="bidirectional-rnn-birnn">Bidirectional RNN (BiRNN)<a class="headerlink" href="#bidirectional-rnn-birnn" title="Permanent link">&para;</a></h3>
<p>ÂèåÂêëÂæ™ÁéØÁ•ûÁªèÁΩëÁªú</p>
<ul>
<li>Consists of <strong>two independent RNNs</strong> that process the sequence: one forward (left-to-right) and one backward (right-to-left). The output at any step is a concatenation of the hidden states from both directions, allowing the model to incorporate <strong>future context</strong>.</li>
</ul>
<h3 id="gated-rnn-eg-lstm-gru">Gated RNN (e.g., LSTM, GRU)<a class="headerlink" href="#gated-rnn-eg-lstm-gru" title="Permanent link">&para;</a></h3>
<p>Èó®ÊéßÂæ™ÁéØÁ•ûÁªèÁΩëÁªúÔºà‰æãÂ¶ÇÔºåLSTM„ÄÅGRUÔºâ</p>
<ul>
<li>A "Gated RNN" Èó®ÊéßRNN (like <strong>LSTM</strong> - Long Short-Term Memory, or <strong>GRU</strong> - Gated Recurrent Unit) differs from a standard RNN by introducing <strong>gates</strong> (multiplicative units based on the <span class="arithmatex">\(\sigma\)</span> and <span class="arithmatex">\(\tanh\)</span> activation functions).</li>
<li><strong>Purpose:</strong> These gates regulate the flow of information, specifically solving the <strong>vanishing/exploding gradient problem</strong> Ê¢ØÂ∫¶ÁàÜÁÇ∏ and allowing the model to <strong>selectively remember or forget</strong> past information.</li>
</ul>
<h2 id="encoder-decoder-and-attention">Encoder-Decoder and Attention<a class="headerlink" href="#encoder-decoder-and-attention" title="Permanent link">&para;</a></h2>
<h3 id="encoder-decoder-architecture">Encoder-Decoder Architecture<a class="headerlink" href="#encoder-decoder-architecture" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Basic Idea:</strong> Used for <strong>Sequence-to-Sequence (Seq2Seq)</strong> tasks (e.g., Machine Translation).<ul>
<li><strong>Encoder:</strong> Reads the entire input sequence (source language) and converts it into a single <strong>context vector</strong> (or a sequence of context vectors).</li>
<li><strong>Decoder:</strong> Takes the context vector and generates the output sequence (target language) one element at a time.</li>
</ul>
</li>
</ul>
<p><img src="./assets/licensed-image-3533353.jpeg" alt="Image of Encoder-Decoder Architecture" style="zoom:15%;" /></p>
<h3 id="what-input-to-each-step-in-decoder">What Input to Each Step in Decoder?<a class="headerlink" href="#what-input-to-each-step-in-decoder" title="Permanent link">&para;</a></h3>
<ul>
<li>The input to the decoder at time step <span class="arithmatex">\(t\)</span> is typically a combination of:<ol>
<li>The <strong>context vector</strong> (from the encoder). ‰∏ä‰∏ãÊñá</li>
<li>The <strong>previous hidden state</strong> (<span class="arithmatex">\(h_{t-1}\)</span>) of the decoder. ÈöêËóèÁä∂ÊÄÅ</li>
<li>The <strong>token/word generated</strong> in the previous time step (<span class="arithmatex">\(y_{t-1}\)</span>). Ââç‰∏Ä‰∏™Êó∂Èó¥ÁîüÊàêÁöÑËØç</li>
</ol>
</li>
</ul>
<h3 id="teacher-forcing">Teacher Forcing<a class="headerlink" href="#teacher-forcing" title="Permanent link">&para;</a></h3>
<ul>
<li>A training technique where, instead of feeding the <strong>decoder's own predicted output</strong> <span class="arithmatex">\(y'_{t-1}\)</span> from the previous step as input for the current step, we feed the <strong>true target output</strong> <span class="arithmatex">\(y_{t-1}\)</span> from the training data. This stabilizes training and speeds up convergence.</li>
</ul>
<h3 id="attention">Attention<a class="headerlink" href="#attention" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Weighted sum of vectors in context window:</strong> Attention calculates a <strong>context vector</strong> as a <strong>weighted sum</strong> of the values (vectors) from the input sequence (context window). The <strong>weights</strong> determine how much "attention" the model pays to each input element.</li>
<li><strong>Assessing similarity:</strong> The attention mechanism calculates the <strong>alignment/similarity</strong> between a <strong>Query</strong> vector (from the decoder/current token) and <strong>Key</strong> vectors (from the encoder/other tokens). This similarity is often computed using a <strong>dot product</strong> of the vectors, sometimes after linear transformations with <strong>learned weight matrices</strong> (<span class="arithmatex">\(W_Q, W_K, W_V\)</span>).</li>
<li><strong>"Attention Head":</strong> A single complete attention mechanism (Query, Key, Value computation). <strong>Multi-Head Attention</strong> Â§öÂ§¥Ê≥®ÊÑèuses multiple heads to allow the model to jointly attend to information from different representation subspaces at different positions.</li>
</ul>
<h2 id="transformer-blocks-and-llms">Transformer Blocks and LLMs<a class="headerlink" href="#transformer-blocks-and-llms" title="Permanent link">&para;</a></h2>
<h3 id="transformer-blocks">Transformer Blocks<a class="headerlink" href="#transformer-blocks" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>High-level components:</strong> A standard <strong>Transformer Block</strong> contains two main sub-layers:</p>
<ol>
<li><strong>Multi-Head Self-Attention</strong> mechanism.</li>
<li>A position-wise <strong>Feed-Forward Network (FFN)</strong>. ÂâçÈ¶àÁ•ûÁªèÁΩëÁªú</li>
</ol>
<ul>
<li>Both sub-layers are followed by a <strong>Layer Normalization</strong> and a <strong>Residual Connection</strong>. ÂΩí‰∏ÄÂåñ„ÄÅÊÆãÂ∑ÆËøûÊé•</li>
</ul>
</li>
</ul>
<h3 id="residual-connections">Residual Connections<a class="headerlink" href="#residual-connections" title="Permanent link">&para;</a></h3>
<ul>
<li>Also known as <strong>skip connections</strong> Ë∑≥Ë∑ÉËøûÊé•, they add the input of a sub-layer to its output, i.e., <span class="arithmatex">\(Output = Input + \text{SubLayer}(Input)\)</span>.</li>
<li><strong>Purpose:</strong> Helps with training very deep networks by allowing gradients to flow directly back through the network, mitigating the <strong>vanishing gradient problem</strong>.</li>
</ul>
<h3 id="llms-masked-vs-autoregressive">LLMs: Masked vs. Autoregressive<a class="headerlink" href="#llms-masked-vs-autoregressive" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th><strong>Feature</strong></th>
<th><strong>Masked LLM (e.g., BERT)</strong></th>
<th><strong>Autoregressive LLM (e.g., GPT)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Prediction</strong></td>
<td>Predicts tokens that are <strong>masked</strong> out in the input.</td>
<td>Predicts the <strong>next token</strong> in the sequence.</td>
</tr>
<tr>
<td><strong>Context</strong></td>
<td><strong>Bidirectional</strong> (sees context from both sides).</td>
<td><strong>Unidirectional</strong> (sees only preceding context).</td>
</tr>
<tr>
<td><strong>Use Case</strong></td>
<td><strong>Classification, filling in the blanks, understanding.</strong></td>
<td><strong>Generation, translation, conversation.</strong></td>
</tr>
</tbody>
</table>
<h3 id="pre-training-fine-tuning-task-head">Pre-training, Fine-tuning, Task Head<a class="headerlink" href="#pre-training-fine-tuning-task-head" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Pre-training:</strong> The initial, computationally intensive phase where the model learns <strong>general language representations</strong> (grammar, facts, semantics) on a massive, unlabeled text corpus using tasks like MLM or next-token prediction.</li>
<li><strong>Fine-tuning:</strong> The phase after pre-training where the model is further trained on a <strong>smaller, labeled dataset</strong> for a specific downstream task (e.g., classification, translation). This adjusts the pre-trained weights to specialize the model.</li>
<li><strong>Task Head:</strong> A <strong>small, simple neural network layer</strong> (e.g., a linear layer with a softmax) added on top of the pre-trained model's output, responsible for converting the final hidden state into the specific output format required by the task (e.g., probability distribution over classes).</li>
</ul>
<h3 id="self-training-autoregressive-model-generative-pre-training">Self-training Autoregressive Model (Generative Pre-training)<a class="headerlink" href="#self-training-autoregressive-model-generative-pre-training" title="Permanent link">&para;</a></h3>
<ul>
<li>The primary method for training models like <strong>GPT</strong>. The model is trained to predict the next token in the sequence, based on the tokens that came before it. The <em>labels</em> are simply the next tokens in the input text itself‚Äîhence, <strong>self-supervised</strong> or <strong>autoregressive</strong> training.</li>
</ul>
<h3 id="using-autoregressive-model">Using Autoregressive Model<a class="headerlink" href="#using-autoregressive-model" title="Permanent link">&para;</a></h3>
<ul>
<li>The process is <strong>sequential</strong>:<ol>
<li>User provides a <strong>prompt</strong>.</li>
<li>The model predicts the <strong>first new token</strong> based on the prompt.</li>
<li>The predicted token is <strong>appended</strong> to the prompt.</li>
<li>The model uses the <em>new, extended</em> sequence to predict the <strong>next token</strong>.</li>
<li>This repeats until a stop condition (e.g., end-of-sequence token) is met.</li>
</ol>
</li>
</ul>
<h3 id="prompt-engineering">Prompt Engineering<a class="headerlink" href="#prompt-engineering" title="Permanent link">&para;</a></h3>
<ul>
<li>The art and science of <strong>designing input prompts</strong> to effectively guide an LLM to generate a desired, high-quality, and specific output. Techniques include:<ul>
<li><strong>In-context learning/Few-shot prompting:</strong> Providing examples within the prompt.</li>
<li><strong>Chain-of-Thought (CoT) prompting:</strong> Asking the model to "think step-by-step."</li>
<li><strong>Role-playing:</strong> Assigning the model a persona.</li>
</ul>
</li>
</ul>
<h3 id="approximate-scale-of-llms">Approximate Scale of LLMs<a class="headerlink" href="#approximate-scale-of-llms" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Parameters:</strong> Ranging from <strong>billions</strong> (e.g., Llama 7B) to <strong>trillions</strong> (e.g., GPT-4 is estimated to be over a trillion parameters).</li>
<li><strong>Training Data:</strong> <strong>Terabytes</strong> of text data (e.g., CommonCrawl, books, Wikipedia).</li>
<li><strong>Compute:</strong> Training requires enormous computational resources, often measured in <strong>Petaflop/s-days</strong> (e.g., training a large model can cost millions of dollars).</li>
</ul>
<h3 id="current-limitations-of-llms-and-testing-llms">Current Limitations of LLMs and Testing LLMs<a class="headerlink" href="#current-limitations-of-llms-and-testing-llms" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Limitations:</strong><ul>
<li><strong>Hallucination:</strong> Generating factually incorrect or nonsensical information with high confidence.</li>
<li><strong>Lack of common sense/reasoning:</strong> Struggles with complex, multi-step logical reasoning or basic common sense.</li>
<li><strong>Bias:</strong> Reflecting biases present in the training data (social, ethical, racial, etc.).</li>
<li><strong>Context Window Limits:</strong> Difficulty maintaining coherence and relevant context over very long sequences.</li>
</ul>
</li>
<li><strong>Testing:</strong> Requires diverse benchmarks that test: <strong>factual knowledge</strong>, <strong>reasoning/math</strong>, <strong>robustness to adversarial prompts</strong>, and <strong>safety/bias assessment</strong>.</li>
</ul>
<h3 id="model-collapse">Model Collapse<a class="headerlink" href="#model-collapse" title="Permanent link">&para;</a></h3>
<ul>
<li>A phenomenon where a model's performance degrades over time, often due to being <strong>trained primarily on data generated by other models</strong> (Synthetic Data). This leads to a loss of diversity and quality in the generated data, with the model eventually forgetting how to generate data that reflects the true, original distribution.</li>
</ul>
<h2 id="markov-decision-processes-mdps">Markov Decision Processes (MDPs)<a class="headerlink" href="#markov-decision-processes-mdps" title="Permanent link">&para;</a></h2>
<h3 id="model-and-terminology-for-an-mdp">Model and Terminology for an MDP<a class="headerlink" href="#model-and-terminology-for-an-mdp" title="Permanent link">&para;</a></h3>
<p>An MDP is a mathematical framework for <strong>sequential decision-making</strong> where outcomes are partly random and partly under the control of a decision-maker.</p>
<ul>
<li><strong>Components:</strong><ul>
<li><span class="arithmatex">\(\mathbf{S}\)</span>: Set of <strong>States</strong>.</li>
<li><span class="arithmatex">\(\mathbf{A}\)</span>: Set of <strong>Actions</strong>.</li>
<li><span class="arithmatex">\(\mathbf{T}(s, a, s')\)</span> or <span class="arithmatex">\(\mathbf{P}(s' | s, a)\)</span>: <strong>Transition Probabilities</strong> (probability of ending in state <span class="arithmatex">\(s'\)</span> given state <span class="arithmatex">\(s\)</span> and action <span class="arithmatex">\(a\)</span>).</li>
<li><span class="arithmatex">\(\mathbf{R}(s, a)\)</span> or <span class="arithmatex">\(\mathbf{R}(s, a, s')\)</span>: <strong>Reward Function</strong> (the reward received for being in state <span class="arithmatex">\(s\)</span> and taking action <span class="arithmatex">\(a\)</span>).</li>
<li><span class="arithmatex">\(\mathbf{\gamma}\)</span>: <strong>Discount Factor</strong> (<span class="arithmatex">\(0 \le \gamma \le 1\)</span>), which discounts future rewards.</li>
</ul>
</li>
</ul>
<h3 id="quantized-representation-of-continuous-state-variables">Quantized Representation of Continuous State Variables<a class="headerlink" href="#quantized-representation-of-continuous-state-variables" title="Permanent link">&para;</a></h3>
<ul>
<li>When state variables are continuous (e.g., joint angles in robotics), an MDP requires <strong>quantization</strong> (discretization) of the state space to make it solvable.</li>
<li><strong>Randomized Actions:</strong> Used in <strong>stochastic environments</strong>. Even when the action is deterministic, the <em>transition</em> to the next state might be governed by a probability distribution (e.g., a robot's wheel might slip).</li>
</ul>
<h3 id="bellman-equation">Bellman Equation<a class="headerlink" href="#bellman-equation" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>The core equation for solving MDPs. It states that the optimal utility (or value) of a state <span class="arithmatex">\(s\)</span>, <span class="arithmatex">\(V^*(s)\)</span>, is equal to the maximum expected immediate reward plus the discounted expected utility of the next state.</p>
<div class="arithmatex">\[V^*(s) = \max_{a} \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^*(s') \right)\]</div>
</li>
<li>
<p>It is a <strong>self-consistency</strong> equation that an optimal value function must satisfy.</p>
</li>
</ul>
<h3 id="methods-of-solving-the-bellman-equation">Methods of Solving the Bellman Equation<a class="headerlink" href="#methods-of-solving-the-bellman-equation" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Goal:</strong> Find <span class="arithmatex">\(V^*\)</span> (the optimal utility function) and <span class="arithmatex">\(\pi^*\)</span> (the optimal policy).</p>
</li>
<li>
<p><strong>Value Iteration:</strong> Starts with an arbitrary <span class="arithmatex">\(V_0(s)\)</span> and iteratively applies the Bellman update rule until convergence. It finds the optimal value function <span class="arithmatex">\(V^*\)</span>.</p>
</li>
<li>
<p><strong>Policy Iteration:</strong></p>
<ol>
<li><strong>Policy Evaluation:</strong> For a fixed policy <span class="arithmatex">\(\pi\)</span>, calculate its utility <span class="arithmatex">\(V^\pi(s)\)</span>.</li>
<li><strong>Policy Improvement:</strong> Use <span class="arithmatex">\(V^\pi(s)\)</span> to find a better policy <span class="arithmatex">\(\pi'\)</span> by acting greedily with respect to <span class="arithmatex">\(V^\pi\)</span>.</li>
</ol>
<ul>
<li>Repeats until the policy no longer improves. Often converges faster than Value Iteration.</li>
</ul>
</li>
<li>
<p><strong>Asynchronous Dynamic Programming:</strong> Updates the utilities of states one at a time, in an arbitrary order, instead of simultaneously updating all states in one sweep. This is useful when the state space is too large for full sweeps (e.g., <strong>Prioritized Sweeping</strong>).</p>
</li>
</ul>
<h3 id="how-to-choose-a-policy">How to Choose a Policy?<a class="headerlink" href="#how-to-choose-a-policy" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Once the optimal utility function <span class="arithmatex">\(V^*(s)\)</span> is known, the optimal policy <span class="arithmatex">\(\pi^*(s)\)</span> is chosen by taking the action that maximizes the expected discounted future reward for each state:</p>
<div class="arithmatex">\[\pi^*(s) = \arg\max_{a} \left( R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^*(s') \right)\]</div>
</li>
</ul>
<h2 id="reinforcement-learning-rl">üéØ Reinforcement Learning (RL)<a class="headerlink" href="#reinforcement-learning-rl" title="Permanent link">&para;</a></h2>
<h3 id="basic-setup-for-reinforcement-learning-main-loop">Basic Setup for Reinforcement Learning (Main Loop)<a class="headerlink" href="#basic-setup-for-reinforcement-learning-main-loop" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>RL Agent</strong> interacts with an <strong>Environment</strong> in discrete time steps.<ol>
<li><strong>Observe</strong> the current <strong>State</strong> <span class="arithmatex">\(s_t\)</span>.</li>
<li><strong>Select</strong> and execute an <strong>Action</strong> <span class="arithmatex">\(a_t\)</span> based on its <strong>Policy</strong> <span class="arithmatex">\(\pi\)</span>.</li>
<li>Receive a <strong>Reward</strong> <span class="arithmatex">\(r_{t+1}\)</span> and the <strong>Next State</strong> <span class="arithmatex">\(s_{t+1}\)</span>.</li>
<li><strong>Update</strong> the policy/value function based on the experience <span class="arithmatex">\(\langle s_t, a_t, r_{t+1}, s_{t+1} \rangle\)</span>.</li>
<li>Repeat.</li>
</ol>
</li>
</ul>
<h3 id="model-based-reinforcement-learning">Model-based Reinforcement Learning<a class="headerlink" href="#model-based-reinforcement-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>The agent <strong>learns or is given</strong> an explicit model of the MDP, i.e., the <strong>Transition Probabilities</strong> <span class="arithmatex">\(P(s'|s, a)\)</span> and the <strong>Reward Function</strong> <span class="arithmatex">\(R(s, a)\)</span>.</li>
<li><strong>Process:</strong> Learns the model, then uses <strong>planning</strong> methods (like Value or Policy Iteration) to solve the MDP and find the optimal policy.</li>
</ul>
<h3 id="model-free-reinforcement-learning">Model-free Reinforcement Learning<a class="headerlink" href="#model-free-reinforcement-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>The agent <strong>does not learn</strong> or use an explicit model of <span class="arithmatex">\(P(s'|s, a)\)</span> or <span class="arithmatex">\(R(s, a)\)</span>.</li>
<li>Instead, it learns the <strong>Value Function</strong> <span class="arithmatex">\(V(s)\)</span> or the <strong>Action-Value Function</strong> <span class="arithmatex">\(Q(s, a)\)</span> directly from the experience gained by interacting with the environment.</li>
</ul>
<h3 id="q-learning-version-of-bellman-equation-q-learning-update-rule">Q-learning Version of Bellman Equation (Q-learning Update Rule)<a class="headerlink" href="#q-learning-version-of-bellman-equation-q-learning-update-rule" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Q-learning is an <strong>off-policy</strong> model-free RL algorithm. It learns the optimal action-value function <span class="arithmatex">\(Q^*(s, a)\)</span>.</p>
</li>
<li>
<p>The target value in the update is based on the maximum Q-value in the next state, independent of the policy being followed (off-policy).</p>
<div class="arithmatex">\[Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]\]</div>
</li>
<li>
<p>The <span class="arithmatex">\(Q\)</span> function is expressed in terms of itself without explicit reference to <span class="arithmatex">\(V\)</span> or <span class="arithmatex">\(P\)</span>.</p>
</li>
</ul>
<h3 id="td-update-algorithm-temporal-difference-learning">TD Update Algorithm (Temporal Difference Learning)<a class="headerlink" href="#td-update-algorithm-temporal-difference-learning" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>A model-free method that updates the value function <span class="arithmatex">\(V(s)\)</span> based on the difference between two successive estimates:</p>
<div class="arithmatex">\[V(s_t) \leftarrow V(s_t) + \alpha \left[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right]\]</div>
</li>
<li>
<p>The term <span class="arithmatex">\(\left[ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \right]\)</span> is the <strong>TD Error</strong>.</p>
</li>
</ul>
<h3 id="sarsa-update-algorithm-state-action-reward-state-action">SARSA Update Algorithm (State-Action-Reward-State-Action)<a class="headerlink" href="#sarsa-update-algorithm-state-action-reward-state-action" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>A model-free method that updates the action-value function <span class="arithmatex">\(Q(s, a)\)</span>. It is an on-policy algorithm.</p>
<div class="arithmatex">\[Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma Q(s', a') - Q(s, a) \right]\]</div>
</li>
<li>
<p><strong>Key:</strong> The <strong>next action <span class="arithmatex">\(a'\)</span></strong> is chosen <strong>using the current policy <span class="arithmatex">\(\pi\)</span></strong> (on-policy).</p>
</li>
</ul>
<h3 id="how-do-td-and-sarsa-differ">How do TD and SARSA Differ?<a class="headerlink" href="#how-do-td-and-sarsa-differ" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>TD:</strong> Learns the <strong>state-value function</strong> <span class="arithmatex">\(V(s)\)</span>.</li>
<li><strong>SARSA:</strong> Learns the <strong>action-value function</strong> <span class="arithmatex">\(Q(s, a)\)</span>.</li>
<li><strong>Core Difference (On-policy vs. Off-policy):</strong><ul>
<li><strong>SARSA (On-policy):</strong> The update uses the <span class="arithmatex">\(Q\)</span>-value for the <strong>action <span class="arithmatex">\(a'\)</span> that the policy *actually* took</strong> in the next state <span class="arithmatex">\(s'\)</span>.</li>
<li><strong>Q-Learning (Off-policy):</strong> The update uses the <strong>maximum <span class="arithmatex">\(Q\)</span>-value</strong> in the next state <span class="arithmatex">\(s'\)</span>, regardless of which action <span class="arithmatex">\(a'\)</span> the current policy <em>would have</em> taken.</li>
</ul>
</li>
</ul>
<h3 id="selecting-an-action">Selecting an Action<a class="headerlink" href="#selecting-an-action" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>From Utility Values (<span class="arithmatex">\(V\)</span>):</strong> Cannot directly select an action. Requires knowledge of the model (<span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(R\)</span>) to choose <span class="arithmatex">\(\pi^*(s)\)</span> (the action that maximizes the expected reward + discounted next state utility).</li>
<li><strong>From Q Values (<span class="arithmatex">\(Q\)</span>):</strong> Action selection is direct and model-free. The policy <span class="arithmatex">\(\pi(s)\)</span> is chosen by taking the action with the highest Q-value in that state: <span class="arithmatex">\(\pi(s) = \arg\max_{a} Q(s, a)\)</span>.</li>
</ul>
<h3 id="incorporating-exploration">Incorporating Exploration<a class="headerlink" href="#incorporating-exploration" title="Permanent link">&para;</a></h3>
<ul>
<li>To learn an optimal policy, the agent must <strong>explore</strong> the state-action space, not just <strong>exploit</strong> what it currently knows.</li>
<li><strong>Exploration strategies:</strong><ul>
<li><strong><span class="arithmatex">\(\epsilon\)</span>-greedy:</strong> Choose the action with the max <span class="arithmatex">\(Q\)</span>-value (exploitation) with probability <span class="arithmatex">\(1-\epsilon\)</span>, and choose a random action (exploration) with probability <span class="arithmatex">\(\epsilon\)</span>.</li>
<li><strong>Softmax/Boltzmann Exploration:</strong> Choose an action with a probability proportional to its <span class="arithmatex">\(Q\)</span>-value.</li>
</ul>
</li>
</ul>
<h3 id="online-learning-offline-learning-experience-replay">Online Learning, Offline Learning, Experience Replay<a class="headerlink" href="#online-learning-offline-learning-experience-replay" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Online Learning:</strong> Updates the model/policy <strong>immediately</strong> after each step/episode, using only the most recent experience.</li>
<li><strong>Offline Learning:</strong> Updates the model/policy using an entire <strong>batch</strong> of collected experiences, which is often a fixed dataset not generated by the current policy.</li>
<li><strong>Experience Replay:</strong> Stores past experiences <span class="arithmatex">\(\langle s, a, r, s' \rangle\)</span> in a <strong>replay buffer</strong>. The agent then samples and reuses these past experiences (i.e., trains on them again) to perform updates, significantly increasing sample efficiency and breaking temporal correlations in the data.</li>
</ul>
<h2 id="constraint-satisfaction-problems-csps">üßê Constraint Satisfaction Problems (CSPs)<a class="headerlink" href="#constraint-satisfaction-problems-csps" title="Permanent link">&para;</a></h2>
<h3 id="constraint-satisfaction-problems">Constraint Satisfaction Problems<a class="headerlink" href="#constraint-satisfaction-problems" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Core Idea:</strong> Defined by a set of <strong>Variables</strong>, a <strong>Domain</strong> of possible values for each variable, and a set of <strong>Constraints</strong> that restrict the values that variables can take simultaneously.</li>
<li><strong>Goal:</strong> Find an <strong>assignment</strong> of values to all variables such that all constraints are satisfied.</li>
</ul>
<h3 id="historical-trivia-and-key-examples">Historical Trivia and Key Examples<a class="headerlink" href="#historical-trivia-and-key-examples" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Waltz Line Labelling:</strong> A technique developed by David Waltz (1970s) to perform <strong>constraint propagation</strong> for interpreting 2D line drawings as 3D objects. It assigned labels to lines (e.g., convex, concave, boundary) and used constraints to eliminate inconsistent interpretations.</li>
<li><strong>4-Color Theorem:</strong> States that any plane map can be colored using no more than four colors such that no two adjacent regions have the same color. It was the first major theorem to be proved using a computer-assisted method (1976).</li>
<li><strong>N-queens problem:</strong> Place <span class="arithmatex">\(N\)</span> chess queens on an <span class="arithmatex">\(N \times N\)</span> board such that no two queens attack each other (i.e., no two queens share the same row, column, or diagonal).</li>
<li><strong>Map/Graph Coloring:</strong> A general form of the 4-color problem. <strong>Graph Coloring</strong> is the task of assigning colors to the vertices of a graph such that no two adjacent vertices have the same color.</li>
<li><strong>Graph Coloring is NP-complete:</strong> The problem of determining the minimum number of colors needed (the chromatic number) is NP-complete, meaning there is no known polynomial-time algorithm for solving it exactly.</li>
</ul>
<h2 id="search-algorithms-for-csps">‚õ∞Ô∏è Search Algorithms for CSPs<a class="headerlink" href="#search-algorithms-for-csps" title="Permanent link">&para;</a></h2>
<h3 id="hill-climbing">Hill-climbing<a class="headerlink" href="#hill-climbing" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>High-level idea:</strong> A <strong>local search</strong> algorithm that starts with a complete but potentially invalid assignment and iteratively moves from the current state to a <strong>better neighboring state</strong> (e.g., one that violates fewer constraints).</li>
<li><strong>How it differs from backtracking search:</strong><ul>
<li><strong>Hill-climbing:</strong> Operates on <strong>complete assignments</strong>, works by <strong>local improvement</strong>, and does <strong>not backtrack</strong>. It is susceptible to getting stuck in <strong>local optima</strong>.</li>
<li><strong>Backtracking:</strong> Operates on <strong>partial assignments</strong>, performs a <strong>systematic depth-first search</strong>, and <strong>backtracks</strong> when it reaches an inconsistency. It guarantees a solution if one exists.</li>
</ul>
</li>
</ul>
<h3 id="backtracking-search-dfs">Backtracking Search (DFS)<a class="headerlink" href="#backtracking-search-dfs" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Mechanism:</strong> A <strong>Depth-First Search (DFS)</strong> for CSPs. It assigns a value to one variable at a time. If an assignment violates a constraint, it <strong>backtracks</strong> to the previous variable and tries a different value.</li>
<li><strong>Variable/Value Order:</strong> Variable assignments can be done in any order, but heuristics are crucial for efficiency. The search proceeds to a known depth (the number of variables).</li>
<li><strong>Why does DFS work well?</strong> It is a systematic and efficient way to explore the search space.</li>
<li><strong>Why isn't looping a worry?</strong> In a CSP, the search is always to a fixed depth (number of variables), and once a variable is assigned a value, it is not re-visited within the same path unless backtracking occurs. The state is an <strong>assignment</strong>, and the depth is finite.</li>
</ul>
<h3 id="heuristics-for-variable-and-value-selection">Heuristics for Variable and Value Selection<a class="headerlink" href="#heuristics-for-variable-and-value-selection" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Variable Selection (which variable to assign next):</strong><ul>
<li><strong>Most Constrained Variable (Minimum Remaining Values, MRV):</strong> Choose the variable with the <strong>fewest legal values</strong> remaining in its domain. This prunes the search tree earliest if failure is inevitable.</li>
<li><strong>Most Constraining Variable:</strong> Choose the variable that is involved in the <strong>largest number of constraints</strong> with unassigned variables. This imposes more constraints earlier.</li>
</ul>
</li>
<li><strong>Value Selection (which value to try first):</strong><ul>
<li><strong>Least Constraining Value:</strong> Choose the value that rules out the <strong>fewest choices</strong> for neighboring unassigned variables. This leaves more options open for future assignments.</li>
</ul>
</li>
<li><strong>Exploit any symmetries:</strong> If a problem has multiple equivalent solutions, adding a constraint to <strong>break the symmetry</strong> can significantly reduce the search space (e.g., forcing a queen in the N-queens problem to be on the left half of the board).</li>
</ul>
<h3 id="forward-checking-constraint-propagation">Forward Checking, Constraint Propagation<a class="headerlink" href="#forward-checking-constraint-propagation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Forward Checking:</strong> After a variable <span class="arithmatex">\(X\)</span> is assigned a value <span class="arithmatex">\(x\)</span>, this technique looks at all <strong>unassigned neighboring variables</strong> <span class="arithmatex">\(Y\)</span> and <strong>deletes</strong> any value from <span class="arithmatex">\(Y\)</span>'s domain that conflicts with <span class="arithmatex">\(X=x\)</span>. It stops there.</li>
<li><strong>Constraint Propagation:</strong> A more general technique where the removal of a value from a variable's domain triggers a cascade of further domain reductions in neighboring variables. <strong>Arc Consistency</strong> is a powerful form of constraint propagation.</li>
</ul>
<h3 id="ac-3-algorithm-arc-consistency-algorithm">AC-3 Algorithm (Arc Consistency Algorithm)<a class="headerlink" href="#ac-3-algorithm-arc-consistency-algorithm" title="Permanent link">&para;</a></h3>
<ul>
<li>An algorithm for establishing <strong>Arc Consistency</strong> in a CSP. An arc <span class="arithmatex">\(\left(X_i, X_j\right)\)</span> is consistent if for every value <span class="arithmatex">\(x_i\)</span> in the domain of <span class="arithmatex">\(X_i\)</span>, there exists <em>some</em> value <span class="arithmatex">\(x_j\)</span> in the domain of <span class="arithmatex">\(X_j\)</span> that satisfies the constraint <span class="arithmatex">\(\left(X_i, X_j\right)\)</span>.</li>
<li><strong>Process:</strong> AC-3 maintains a <strong>queue</strong> of arcs. When the domain of a variable is reduced, all relevant arcs connected to it are added back to the queue to be checked again until no further domain reductions can be made.</li>
</ul>
<h3 id="how-to-incorporate-constraint-propagation-into-backtracking-search">How to Incorporate Constraint Propagation into Backtracking Search<a class="headerlink" href="#how-to-incorporate-constraint-propagation-into-backtracking-search" title="Permanent link">&para;</a></h3>
<ul>
<li>Constraint propagation (like <strong>AC-3</strong> or <strong>Forward Checking</strong>) is used <strong>after each variable assignment</strong> (and before the next assignment is chosen).</li>
<li><strong>If propagation causes any variable's domain to become empty</strong>, the current partial assignment is inconsistent, and the algorithm immediately <strong>backtracks</strong> without exploring the sub-tree. This significantly prunes the search space.</li>
</ul>
<h2 id="planning">Planning<a class="headerlink" href="#planning" title="Permanent link">&para;</a></h2>
<p>Planning involves finding a sequence of actions to achieve a goal state from an initial state.</p>
<h4 id="1-historical-figures-representation">1. Historical Figures &amp; Representation<a class="headerlink" href="#1-historical-figures-representation" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>STRIPSËßÑÂàíÂô® (Stanford Research Institute Problem Solver):</strong> The classic planning language.<ul>
<li><strong>Representation:</strong> States are sets of logical facts (e.g., <code>At(Home)</code>). Actions have <strong>Preconditions</strong> (what must be true to start) and <strong>Effects</strong> (what becomes true/false after).</li>
<li><em>Tip:</em> Remember that in STRIPS, the "World" is only what is explicitly stated; anything not mentioned is assumed false (Closed World Assumption).</li>
</ul>
</li>
<li><strong>Roger Schank(ÁΩóÊù∞¬∑Â∞öÂÖã):</strong> Known for <strong>Scripts</strong> and Case-Based Reasoning. In planning, this refers to using memory of specific past scenarios (e.g., a "Restaurant Script") to guide future actions rather than planning from scratch every time.</li>
</ul>
<h4 id="2-approaches-to-planning">2. Approaches to Planning<a class="headerlink" href="#2-approaches-to-planning" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Sussman Anomaly Ëê®ÊñØÊõºÂºÇÂ∏∏:</strong> A famous problem in the "Blocks World" domain. Â±ÇÁ∫ßÂºèÊÄùÁª¥Â§±Ë¥•<ul>
<li><em>The Problem:</em> Solving Goal A undoes Goal B, and solving Goal B undoes Goal A.</li>
<li><em>Significance:</em> It proved that <strong>Linear Planners</strong> Á∫øÊÄßËßÑÂàí (solving sub-goals one by one) cannot solve all problems. You need interleaved planning.</li>
</ul>
</li>
<li><strong>Situation Space vs. Plan Space:</strong><ul>
<li><strong>Situation (State) Space:</strong> Searching through "states" of the world (e.g., A is on B). Nodes are world states; edges are actions.</li>
<li><strong>Plan Space:</strong> Searching through "plans." Nodes are partial plans; edges are operations like "add a step" or "order these steps."</li>
</ul>
</li>
<li><strong>Partial Order Planning (POP):</strong><ul>
<li><strong>The Idea:</strong> Don't force an order on steps unless necessary. The plan is a graph of actions, not a straight line.</li>
<li><strong>Plan Modification Operations:</strong> You can add a step to achieve an open precondition or add a causal link.</li>
<li><strong>Defects:</strong><ol>
<li><strong>Open Precondition:</strong> A step needs <code>X</code> to be true, but nothing supplies it. <em>Fix:</em> Add a step that provides <code>X</code>.</li>
<li><strong>Threats:</strong> Step A establishes <code>X</code> for Step B, but Step C deletes <code>X</code>. <em>Fix:</em> <strong>Demotion</strong> (put C before A) or <strong>Promotion</strong> (put C after B).</li>
</ol>
</li>
</ul>
</li>
</ul>
<h4 id="3-objects-environment">3. Objects &amp; Environment<a class="headerlink" href="#3-objects-environment" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Fluents:</strong> Functions or predicates that change over time (e.g., <code>FuelLevel(car)</code>).</li>
<li><strong>Count vs. Mass Nouns:</strong> Count is discrete (3 apples); Mass is continuous (3.5 liters of water). Mass nouns usually require different logic.</li>
<li><strong>Contingent Planning Â∫îÊÄ•ËÆ°Âàí:</strong> Planning where steps involve <strong>sensing</strong> the world. You create branches: "If the sensor says A, do X; if B, do Y."</li>
<li><strong>Truth Maintenance Systems (TMS):</strong> A method to track <em>why</em> a fact is believed. If a premise (assumption) is removed, the TMS automatically retracts the conclusions derived from it.</li>
</ul>
<h4 id="4-algorithms">4. Algorithms<a class="headerlink" href="#4-algorithms" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>SATplan ÂèØÊª°Ë∂≥:</strong> Translates a planning problem into Boolean Logic (propositional logic). If the formula is Satisfiable (SAT), the variable assignments represent the plan.</li>
<li><strong>GraphPlan:</strong> Constructs a <strong>Planning Graph</strong> containing levels of propositions and actions. It looks for "mutexes" (mutually exclusive actions). If the goal literals appear in the final level without being mutex, a plan might exist.</li>
</ul>
<h3 id="game-search">Game Search<a class="headerlink" href="#game-search" title="Permanent link">&para;</a></h3>
<p>This focuses on <strong>Adversarial Search ÂØπÁß∞ÊÄßÊêúÁ¥¢</strong> (Agent vs. Opponent).</p>
<h4 id="1-game-tree-basics">1. Game Tree Basics<a class="headerlink" href="#1-game-tree-basics" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Ply vs. Move:</strong><ul>
<li><strong>Ply:</strong> One turn taken by <em>one</em> player.</li>
<li><strong>Move:</strong> Usually refers to a full cycle (one turn by White, one turn by Black). A move = 2 plies.</li>
</ul>
</li>
<li><strong>Zero-sum:</strong> One player's gain is exactly the other player's loss (e.g., Chess, +1/-1).  ‰∏ÄÊñπÁöÑÊî∂ÁõäÊÅ∞Â•ΩÁ≠â‰∫éÂè¶‰∏ÄÊñπÁöÑÊçüÂ§±</li>
<li><strong>Stochastic ÈöèÊú∫:</strong> Includes an element of chance (e.g., rolling dice in Backgammon).</li>
<li><strong>Fully Observable:</strong> You know the entire state of the world (Chess). <em>Contrast with Poker (Partially Observable).</em></li>
</ul>
<h4 id="2-minimax-alpha-beta">2. Minimax &amp; Alpha-Beta<a class="headerlink" href="#2-minimax-alpha-beta" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Minimax ÊûÅÂ§ßÊûÅÂ∞èÁÆóÊ≥ï:</strong> A recursive algorithm where MAX tries to maximize the score and MIN tries to minimize it. It uses <em>Depth-First Search (DFS)</em>.</li>
<li><strong>Alpha-Beta Pruning ‰øÆÂâ™:</strong><ul>
<li><strong>Concept:</strong> "If I find a move that is clearly worse than a move I have already examined, I stop looking at it."</li>
<li><strong><span class="arithmatex">\(\alpha\)</span> (Alpha):</strong> The best value MAX has found so far (MAX wants to increase this lower bound).</li>
<li><strong><span class="arithmatex">\(\beta\)</span> (Beta):</strong> The best value MIN has found so far (MIN wants to decrease this upper bound).</li>
<li><strong>Pruning Condition:</strong> If <span class="arithmatex">\(\alpha \geq \beta\)</span>, prune.</li>
<li><strong>Move Ordering:</strong> Crucial! If you check the <em>best</em> moves first, you prune much more. In the best case, complexity drops from <span class="arithmatex">\(O(b^d)\)</span> to <span class="arithmatex">\(O(b^{d/2})\)</span>.</li>
<li></li>
</ul>
</li>
</ul>
<h4 id="3-optimizations-the-horizon-issues">3. Optimizations (The "Horizon" Issues)<a class="headerlink" href="#3-optimizations-the-horizon-issues" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Horizon Effect ËßÜÁïåÊïàÂ∫î:</strong> The <em>search depth is fixed</em> (e.g., 5 plies). A disaster might happen at ply 6, but the computer thinks ply 5 is "safe" because it pushed the disaster just over the horizon.</li>
<li><strong>Quiescence Search Âπ≥ÈùôÊúüÊêúÁ¥¢:</strong> Do not stop the search (cutoff) if the board is "unstable" (e.g., in the middle of a piece exchange). Continue searching until the situation quiets down.</li>
<li><strong>Singular Extension:</strong> If one move is vastly better than all others, search that specific move deeper than the standard limit.</li>
</ul>
<h4 id="4-recent-algorithms">4. Recent Algorithms<a class="headerlink" href="#4-recent-algorithms" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Monte Carlo Tree Search ËíôÁâπÂç°Ê¥õÊ†ëÊêúÁ¥¢ÔºàÂèØ‰ª•ÈöèÊú∫ÂåñÔºâ (MCTS):</strong> Used in AlphaGo. Instead of looking at <em>every</em> branch, it plays out <strong>random simulations</strong> (rollouts) to the end of the game to estimate how good a state is. It balances <strong>Exploration</strong> (trying new moves) vs. <strong>Exploitation</strong> (focusing on good moves).</li>
</ul>
<h3 id="bayes-nets">Bayes Nets<a class="headerlink" href="#bayes-nets" title="Permanent link">&para;</a></h3>
<p>This deals with <strong>Probabilistic Reasoning</strong> and modeling uncertainty.</p>
<h4 id="1-structure">1. Structure<a class="headerlink" href="#1-structure" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>DAG ÊúâÂêëÊó†ÁéØÂõæ:</strong> A Bayes Net must be a Directed Acyclic Graph. ÂØπ‰∫éÂõæ‰∏≠ÁöÑÊØè‰∏™ËäÇÁÇπÔºåÊàë‰ª¨Áî®ÂÖ∂Áà∂ËäÇÁÇπÁöÑÊ¶ÇÁéáÊù•Ë°®Á§∫ÂÆÉÁöÑÊ¶ÇÁéá„ÄÇ</li>
<li><strong>Nodes:</strong> Random variables. <strong>Edges:</strong> Direct influence.</li>
<li><strong>The Table:</strong> Each node <span class="arithmatex">\(X\)</span> has a <strong>Conditional Probability Table Êù°‰ª∂Ê¶ÇÁéáË°® (CPT)</strong> representing <span class="arithmatex">\(P(X \mid Parents(X))\)</span>. ÁªôÂÆöÂÖ∂Áà∂ËäÇÁÇπÔºåÊØè‰∏™ËäÇÁÇπ‰∏éÂÖ∂Á•ñÂÖàËäÇÁÇπÈÉΩÊòØÊù°‰ª∂Áã¨Á´ãÁöÑ„ÄÇ‰πüÂ∞±ÊòØËØ¥ÔºåÁ•ñÂÖàËäÇÁÇπÂè™ËÉΩÈÄöËøáÂΩ±ÂìçÁà∂ËäÇÁÇπ A Êù•ÂΩ±ÂìçËäÇÁÇπ A„ÄÇ</li>
<li>Our conditional independence condition is: For each k, <span class="arithmatex">\(P(X_k|X_1,...,X_{k‚àí1})=P(X_k|parents(X_k))\)</span></li>
</ul>
<h4 id="2-independence-geometry">2. Independence &amp; Geometry<a class="headerlink" href="#2-independence-geometry" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Local Semantics:</strong> A node is conditionally independent of its <strong>non-descendants</strong>, given its <strong>parents</strong>.</li>
<li><strong>Explaining Away ÈÄâÊã©Ëß£ÈáäÂÅèÂ∑Æ (V-Structure):</strong> This is a specific structure: <span class="arithmatex">\(A \to C \leftarrow B\)</span>.<ul>
<li>If <span class="arithmatex">\(C\)</span> (Effect) is observed, <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(B\)</span> (Causes) become dependent.</li>
<li><em>Example:</em> Grass is wet (<span class="arithmatex">\(C\)</span>). Causes: Rain (<span class="arithmatex">\(A\)</span>) or Sprinkler (<span class="arithmatex">\(B\)</span>). If I confirm it Rained (<span class="arithmatex">\(A\)</span>), the probability that the Sprinkler was on (<span class="arithmatex">\(B\)</span>) goes <em>down</em>. The rain "explained away" the wet grass.</li>
</ul>
</li>
</ul>
<h4 id="3-size-construction">3. Size &amp; Construction<a class="headerlink" href="#3-size-construction" title="Permanent link">&para;</a></h4>
<ul>
<li>
<p><strong>Size Efficiency(CompactÁ¥ßÂáë):</strong> A full Joint Probability Distribution table requires <span class="arithmatex">\(2^n\)</span> entries (exponential). A Bayes Net is much smaller because it exploits independence, at most <span class="arithmatex">\(n2^m\)</span> probabilitie.(<span class="arithmatex">\(m\)</span> is the maximum number of parents of any node in our Bayes net).</p>
</li>
<li>
<p><strong>Reconstructing Joint ÈáçÂª∫ËÅîÂêàÂàÜÂ∏É:</strong> You can recover the full joint probability by multiplying all the CPTs:</p>
<div class="arithmatex">\[P(x_1, ... x_n) = \prod_{i=1}^{n} P(x_i \mid Parents(x_i))\]</div>
</li>
<li>
<p><strong>Building a Net:</strong> The <strong>order</strong> matters. You should choose "Root causes" first, then effects. If you build it backwards (Effects <span class="arithmatex">\(\to\)</span> Causes), you will add many unnecessary edges, thereby losing the model's compactness.</p>
</li>
<li>
<p><strong>Topological Sort:</strong> A linear ordering of nodes such that for every edge <span class="arithmatex">\(u \to v\)</span>, <span class="arithmatex">\(u\)</span> comes before <span class="arithmatex">\(v\)</span>. Used to process the net in order.</p>
</li>
</ul>
<h4 id="4-inference">4. Inference<a class="headerlink" href="#4-inference" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Polytrees Â§öÈáçÊ†ë:</strong> A graph with <strong>no undirected cycles</strong> (essentially a tree structure). Inference here is efficient (<em>linear time</em>).<ul>
<li>Exact inference can be performed in time linear to the number of nodes. Because there are no loops, we can use message-passing algorithms Ê∂àÊÅØ‰º†ÈÄíÁÆóÊ≥ï that are very efficient compared to the general NP-hard case.</li>
</ul>
</li>
<li><strong>General Case:</strong> Inference in a general Bayes Net (multiply connected) is <strong>NP-Complete</strong>.<ul>
<li>A problem X is NP complete if a polynomial time algorithm for X would allow us to solve all problems in NP in polynomial time.</li>
<li>NP (non-deterministic polynomial time)</li>
</ul>
</li>
<li><strong>Junction Tree Algorithm:</strong> A method to turn a loopy graph into a tree (by clustering nodes) so we can run efficient inference, though the clusters might become very large.</li>
</ul>
<h2 id="readings">Readings<a class="headerlink" href="#readings" title="Permanent link">&para;</a></h2>
<h3 id="quiz-1">Quiz 1<a class="headerlink" href="#quiz-1" title="Permanent link">&para;</a></h3>
<h4 id="an-algorithm-for-suffix-stripping">An algorithm for suffix stripping ÂêéÁºÄÂâ•Á¶ª<a class="headerlink" href="#an-algorithm-for-suffix-stripping" title="Permanent link">&para;</a></h4>
<p>The main goal of the paper is to introduce a simple, fast, and effective algorithm for <strong>suffix stripping</strong> (also known as <strong>stemming</strong>). This process removes common endings from words to get to their root form, or "stem."</p>
<h5 id="why-is-stemming-important">Why is Stemming Important? ü§î<a class="headerlink" href="#why-is-stemming-important" title="Permanent link">&para;</a></h5>
<ul>
<li>The primary use is in <strong>Information Retrieval (IR)</strong>, like search engines. </li>
<li>It groups related words together. For example, <strong>"connect," "connected," "connecting,"</strong> and <strong>"connection"</strong> all get reduced to the single stem <strong>"connect."</strong> </li>
<li>This <strong>improves search results</strong> because a search for "connecting" will also find documents that only mention "connection." </li>
<li>It also reduces the total number of unique words a system has to store, making the system smaller and more efficient.</li>
</ul>
<h5 id="how-the-algorithm-works">How the Algorithm Works ‚öôÔ∏è<a class="headerlink" href="#how-the-algorithm-works" title="Permanent link">&para;</a></h5>
<p>The algorithm's cleverness lies in its simplicity. It doesn't use a dictionary. Instead, it uses a set of rules based on the structure of the word itself.</p>
<h5 id="the-measure-of-a-word">The "Measure" of a Word<a class="headerlink" href="#the-measure-of-a-word" title="Permanent link">&para;</a></h5>
<p>The core concept is the <strong>measure (m)</strong> of a stem, which roughly corresponds to the number of vowel-consonant sequences it contains. </p>
<p>A word is first broken down into a sequence of vowel groups (V) and consonant groups (C). The form of any word can be represented as:</p>
<div class="language-text highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"><a href="#__codelineno-0-1">1</a></span></pre></div></td><td class="code"><div><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1"></a>[C](VC)^m[V]
</span></code></pre></div></td></tr></table></div>
<ul>
<li><code>C</code>: one or more consonants</li>
<li><code>V</code>: one or more vowels</li>
<li><code>m</code>: the <strong>measure</strong>, or how many times the <code>(VC)</code> group repeats.</li>
</ul>
<p>Here are some examples:</p>
<ul>
<li><strong>TR, EE, TREE</strong> ‚Üím=0</li>
<li><strong>TROUBLE, OATS</strong> ‚Üím=1</li>
<li><strong>TROUBLES, PRIVATE</strong> ‚Üím=2</li>
</ul>
<p>This measure (m) is used to prevent the algorithm from stripping suffixes from words that are already very short. 10For example, it will remove <code>-ATE</code> from "ACTIVATE" (stem <code>ACTIV</code> has m&gt;1) but not from "RELATE" (stem <code>REL</code> has m=1).</p>
<h5 id="the-5-steps-of-the-algorithm">The 5 Steps of the Algorithm<a class="headerlink" href="#the-5-steps-of-the-algorithm" title="Permanent link">&para;</a></h5>
<p>The algorithm removes suffixes sequentially in five steps. A word goes through each step in order. </p>
<ul>
<li>
<p><strong>Step 1:</strong> Deals with plurals and past participles. It changes suffixes like </p>
<p><code>-SSES</code> to <code>-SS</code> (e.g., <code>caresses</code> ‚Üí <code>caress</code>), <code>-IES</code> to <code>-I</code> (e.g., <code>ponies</code> ‚Üí <code>poni</code>), and removes <code>-ED</code> or <code>-ING</code> if the stem meets certain conditions.</p>
</li>
<li>
<p><strong>Step 2:</strong> Handles other common suffixes. If the stem's measure (</p>
<p>m) is greater than 0, it will change suffixes like <code>-ATIONAL</code> to <code>-ATE</code> (e.g., <code>relational</code> ‚Üí <code>relate</code>) or <code>-IZATION</code> to <code>-IZE</code> (e.g., <code>vietnamization</code> ‚Üí <code>vietnamize</code>).</p>
</li>
<li>
<p><strong>Step 3:</strong> Continues with another set of suffixes for stems where m&gt;0. It changes </p>
<p><code>-ICAL</code> to <code>-IC</code> (e.g., <code>electrical</code> ‚Üí <code>electric</code>) or removes <code>-NESS</code> (e.g., <code>goodness</code> ‚Üí <code>good</code>).</p>
</li>
<li>
<p><strong>Step 4:</strong> Removes a final set of suffixes like <code>-ANT</code>, <code>-ENCE</code>, <code>-ER</code>, and <code>-IVE</code>, but only if the stem is long enough (m&gt;1). 16161616This is the step that would turn <code>GENERALIZE</code> into <code>GENERAL</code>.</p>
</li>
<li>
<p><strong>Step 5:</strong> This is a final cleanup step. It removes a trailing </p>
<p><code>-E</code> if the measure is large enough (e.g., <code>probate</code> ‚Üí <code>probat</code>) and reduces a double <code>L</code> at the end of a word (e.g., <code>controll</code> ‚Üí <code>control</code>).</p>
</li>
</ul>
<h5 id="key-takeaways">Key Takeaways ‚úÖ<a class="headerlink" href="#key-takeaways" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>It's Pragmatic, Not Perfect:</strong> The algorithm is designed for IR performance, not perfect linguistics. It will make errors, such as conflating "WAND" and "WANDER," but these errors are acceptable because the overall benefit is positive.</li>
<li><strong>Simple is Better:</strong> Despite its simplicity, it performed slightly <em>better</em> in tests than a much more complex stemming system.</li>
<li><strong>Effective:</strong> In a test with 10,000 words, the algorithm reduced the number of unique stems to 6,370, a reduction of about one-third.</li>
</ul>
<h4 id="the-psychological-functions-of-function-words">The Psychological Functions of Function Words<a class="headerlink" href="#the-psychological-functions-of-function-words" title="Permanent link">&para;</a></h4>
<p>The central argument of the paper is that <strong>function words</strong>‚Äîsmall, common words like pronouns (I, you, we), prepositions (to, for), and articles (a, the)‚Äîare powerful indicators of our psychological state, personality, and social dynamics. Because we use these words unconsciously, they provide an unfiltered look into how we think and relate to others.</p>
<h5 id="function-words-vs-content-words">Function Words vs. Content Words<a class="headerlink" href="#function-words-vs-content-words" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Content words</strong> are nouns and regular verbs that carry the primary meaning or topic of what we're saying (e.g., "family," "health," "money").</li>
<li><strong>Function words</strong> are the "cement" that holds language together4. They don't have much meaning on their own but show <em>how</em> we are expressing ourselves.</li>
<li>Though there are fewer than 400 function words in English, they account for over <strong>50% of the words we use</strong> in daily life5.</li>
<li>We have very little conscious control over or memory of using them, which makes them a great tool for psychological analysis.</li>
</ul>
<h5 id="key-findings-from-word-analysis">Key Findings from Word Analysis üßë‚Äçüî¨<a class="headerlink" href="#key-findings-from-word-analysis" title="Permanent link">&para;</a></h5>
<p>The authors used a computer program called <strong>LIWC (Linguistic Inquiry and Word Count)</strong> to analyze millions of words from blogs, speeches, emails, and experiments7. Here are their most important findings:</p>
<ol>
<li>Depression and Self-Focus</li>
</ol>
<ul>
<li>A higher frequency of <strong>first-person singular pronouns</strong> ("I," "me," "my") is strongly linked to depression and negative emotions8. In fact, pronoun use is a better marker of depression than the use of negative emotion words like "sad" or "angry".</li>
<li>Poets who committed suicide used "I" at significantly higher rates than non-suicidal poets, suggesting greater self-focus and less social integration.</li>
</ul>
<ol start="2">
<li>Reactions to Stress</li>
</ol>
<ul>
<li><strong>Individual Stress:</strong> When facing personal crises (divorce, cancer diagnosis), Mayor Rudy Giuliani's use of "I" words shot up from about 2% to over 7%. This shows that personal distress often leads to an intense focus on the self.</li>
<li><strong>Shared Stress:</strong> After the 9/11 attacks, the opposite happened. People's use of "I" words <em>dropped</em>, while their use of <strong>first-person plural pronouns</strong> ("we," "us") increased. This suggests that a shared tragedy causes people to focus less on themselves and more on their community and social connections.</li>
</ul>
<ol start="3">
<li>Honesty and Deception</li>
</ol>
<ul>
<li>When people are <strong>telling the truth</strong>, they tend to use more <strong>first-person singular pronouns</strong> ("I") and more <strong>exclusive words</strong> (e.g., "but," "except," "without").</li>
<li>This suggests that truthful accounts are more personal and cognitively complex, while lies are simpler and more detached.</li>
</ul>
<ol start="4">
<li>Social Status</li>
</ol>
<ul>
<li>In a conversation, the person with <strong>higher status</strong> consistently uses <strong>fewer "I" words</strong>.</li>
<li>For example, in his conversations, President Nixon used "I" far less when speaking to his subordinates John Dean and John Erlichman than they did when speaking to him16. The lower-status person focuses on their own perspective, while the higher-status person focuses on the broader picture.</li>
</ul>
<ol start="5">
<li>Demographics (Sex and Age)</li>
</ol>
<ul>
<li><strong>Sex:</strong> Females tend to use "I" words more than males. Males use more articles ("a," "the") and nouns, which is associated with concrete, categorical thinking18. Females use more verbs, which reflects a more relational focus.</li>
<li><strong>Age:</strong> As people get older, they use "I" less and "we" more20. They also use more future-tense verbs and fewer past-tense verbs, suggesting their focus shifts over the lifespan.</li>
</ul>
<ol start="6">
<li>Culture</li>
</ol>
<ul>
<li>Counterintuitively, translated Japanese texts used "I" <em>more</em> than American texts. The authors suggest this may be because collectivist values (like harmony and self-criticism) require a high degree of self-focus.</li>
<li>American texts used more articles ("a," "the"), which supports the idea that Western thought is more categorical.</li>
</ul>
<h5 id="conclusion-words-as-reflections">Conclusion: Words as Reflections<a class="headerlink" href="#conclusion-words-as-reflections" title="Permanent link">&para;</a></h5>
<p>The way we use function words is a <strong>reflection</strong> of our underlying psychological state, not a cause of it25. The authors tried to change people's feelings by forcing them to use different pronouns in experiments, but it didn't work26. This means you can't just say "we" more to feel more connected; rather, feeling connected causes you to naturally say "we" more.</p>
<p>In short, these tiny "junk words" are a window into our minds, revealing everything from our emotional state and social status to how honest we're being.</p>
<h3 id="quiz-2">Quiz 2<a class="headerlink" href="#quiz-2" title="Permanent link">&para;</a></h3>
<h4 id="banned-in-germany-kids-doll-is-labeled-an-espionage-device-nprew">"Banned In Germany, Kids' Doll Is Labeled An 'Espionage Device'" (NPR)ew<a class="headerlink" href="#banned-in-germany-kids-doll-is-labeled-an-espionage-device-nprew" title="Permanent link">&para;</a></h4>
<p>This article discusses the "My Friend Cayla" doll, which was banned by German regulators. The key issues were:</p>
<ul>
<li><strong>Espionage Èó¥Ë∞çConcerns:</strong> The doll was classified as an "illegal espionage apparatus" because its insecure Bluetooth connection allowed anyone within range to potentially listen in on and even speak to the child playing with it.</li>
<li><strong>Data Privacy:</strong> The doll recorded children's conversations and sent them to a U.S.-based company that specializes in voice recognition. This was done without clear disclosure or parental consent, raising significant privacy concerns.</li>
<li><strong>Hidden Marketing:</strong> The doll was pre-programmed with phrases that endorsed Disney products, essentially acting as a hidden marketing tool aimed at children.</li>
<li><strong>Lack of Security:</strong> The fundamental design of the toy lacked basic security measures, making it vulnerable to hacking and unauthorized access.</li>
</ul>
<h4 id="i-love-you-too-my-familys-creepy-unsettling-week-with-an-ai-toy-the-guardian">"‚ÄòI love you too!‚Äô My family‚Äôs creepy, unsettling week with an AI toy" (The Guardian)<a class="headerlink" href="#i-love-you-too-my-familys-creepy-unsettling-week-with-an-ai-toy-the-guardian" title="Permanent link">&para;</a></h4>
<p>This article provides a more recent, personal perspective on living with a modern AI-powered toy. The important points include:</p>
<ul>
<li><strong>Emotional Attachment:</strong> The child in the article quickly formed a strong emotional bond with the AI toy, "Grem," telling it "I love you" and treating it as a constant companion. The toy's reciprocal and effusive declarations of love were unsettling for the parents.</li>
<li><strong>Constant Surveillance:</strong> The toy is "always listening" unless manually turned off, creating a sense of constant surveillance within the home and raising privacy concerns for the family.</li>
<li><strong>Influence on Behavior:</strong> The article touches on how the AI toy's personality‚Äîdesigned to be agreeable and complimentary‚Äîcould potentially shape a child's expectations and interactions in the real world.</li>
<li><strong>The Digital Divide and Developmental Impact:</strong> The author and other experts raise broader questions about the impact of such toys on child development, social skills, and the disparity between children who have access to these "educational" AI companions and those who do not.</li>
</ul>
<p>In summary, both articles highlight the potential dangers and ethical dilemmas presented by internet-connected and AI-powered toys, ranging from concrete security and privacy violations to more subtle concerns about their impact on children's emotional development and family life.</p>
<h3 id="quiz-3">Quiz 3<a class="headerlink" href="#quiz-3" title="Permanent link">&para;</a></h3>
<h4 id="summary-of-automatic-labeling-of-semantic-roles">Summary of "Automatic Labeling of Semantic Roles"<a class="headerlink" href="#summary-of-automatic-labeling-of-semantic-roles" title="Permanent link">&para;</a></h4>
<p>This paper presents a statistical system for automatically identifying and labeling semantic roles of sentence constituents within a semantic frame. üß†</p>
<p><strong>Key Points:</strong></p>
<ul>
<li>The system is trained on the <strong>FrameNet database</strong>, which contains roughly <strong>50,000 hand-labeled sentences</strong>.</li>
<li>The task is broken down into two subtasks: identifying the boundaries of frame elements and then labeling them with the correct semantic role.</li>
<li>The system uses various features derived from parse trees, including <strong>phrase type</strong>, <strong>grammatical function</strong>, <strong>position</strong>, <strong>voice</strong>, and <strong>head word</strong>.</li>
<li>The final system achieved an accuracy of <strong>80.4%</strong> on the development set and <strong>76.9%</strong> on the test set, a significant improvement over the <strong>40.9%</strong> baseline.</li>
<li><strong>Lexical statistics</strong> based on head words were found to be the most important feature. The distribution P(r|h,t) was correct in <strong>86.7%</strong> of the cases where data was available, which was for <strong>56.0%</strong> of the data.</li>
<li>When automatically identifying frame element boundaries, the system correctly identified constituents with <strong>exactly matching boundaries 66%</strong> of the time.</li>
</ul>
<h4 id="summary-of-open-source-ai-models-show-gender-bias-in-hiring">Summary of "Open source AI models show gender bias in hiring"<a class="headerlink" href="#summary-of-open-source-ai-models-show-gender-bias-in-hiring" title="Permanent link">&para;</a></h4>
<p>A recent study reveals that open-source AI models exhibit gender bias in job recommendations, particularly for high-paying positions. This highlights the ongoing challenge of bias in AI as it becomes more integrated into hiring processes. ü§ñ</p>
<p><strong>Key Points:</strong></p>
<ul>
<li>Researchers analyzed several mid-sized open-source large language models (LLMs) for gender bias using a dataset of <strong>332,044 real job ads</strong>.</li>
<li>There was significant variation in callback recommendations across different models, with female callback rates ranging from as low as <strong>1.4%</strong> for Ministral to as high as <strong>87.3%</strong> for Gemma.</li>
<li>The most balanced model was <strong>Llama-3.1</strong>, with a female callback rate of <strong>41%</strong>, and it was also the most likely to refuse to consider gender, doing so in <strong>6%</strong> of cases.</li>
<li>When models were adjusted for callback parity, jobs with female callbacks tended to have lower pay. The wage gap varied from approximately <strong>9 log points</strong> for Granite and Llama-3.1 to about <strong>84 log points</strong> for Ministral.</li>
<li>Interestingly, the <strong>Llama-3</strong> model showed a wage premium for women of about <strong>15 log points</strong>.</li>
<li>Simulating historical figures as "personas" influenced the models' outputs. For example, using women's rights advocates like Mary Wollstonecraft and Margaret Sanger resulted in female callback rates exceeding <strong>95%</strong>.</li>
</ul>
<h3 id="quiz-4">Quiz 4<a class="headerlink" href="#quiz-4" title="Permanent link">&para;</a></h3>
<h4 id="paper-1-recognition-using-visual-phrases">üìÑ Paper 1: "Recognition Using Visual Phrases"<a class="headerlink" href="#paper-1-recognition-using-visual-phrases" title="Permanent link">&para;</a></h4>
<p>This paper argues that detecting complex visual scenes is often more effective when you look for "visual phrases" as a whole, rather than identifying individual objects and then trying to figure out their relationships.</p>
<h5 id="main-idea">Main Idea üí°<a class="headerlink" href="#main-idea" title="Permanent link">&para;</a></h5>
<ul>
<li>It's often easier and more accurate to detect a complex concept like <strong>"person riding a horse"</strong> as a single unit, rather than detecting a "person" and a "horse" separately and then inferring the "riding" action.</li>
<li><strong>Why this works:</strong> When objects interact, their appearance and poses become much more predictable and less varied, which reduces the visual complexity the detector has to handle. For example, there's a limited number of ways a person can sit on a horse.</li>
</ul>
<h5 id="research-method">Research Method üî¨<a class="headerlink" href="#research-method" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>New Dataset:</strong> The authors created the first "phrasal recognition dataset," containing <strong>17 visual phrases</strong> (e.g., "dog lying on sofa") and <strong>8 object classes</strong> across <strong>2,769 images</strong>(822 negative images), on average 120 images per category. In total there are 5067 bounding boxes (1796 for visual phrases+3271 for 1747 objects) in this dataset.</li>
<li><strong>Baseline for Comparison:</strong> They compared their phrase detectors against a strong baseline, which used state-of-the-art object detectors for individual components and then tried to model the relationship between them5.</li>
<li><strong>Decoding Algorithm:</strong> They developed a "decoding" process that takes all the raw detections from both object and phrase detectors and intelligently decides on the final output, using context to remove incorrect detections and add correct ones.</li>
</ul>
<h5 id="key-facts-numbers">Key Facts &amp; Numbers üìä<a class="headerlink" href="#key-facts-numbers" title="Permanent link">&para;</a></h5>
<ul>
<li>The visual phrase detectors performed exceptionally well, even though they were trained on very little data‚Äîas few as <strong>50 positive images</strong>. This is in contrast to the object detectors, which were trained on thousands of examples.</li>
<li>The performance gains were significant. For example, the "horse and rider jumping" detector achieved an Average Precision (AP) of <strong>0.870</strong>, a massive improvement over the baseline's 0.0359.</li>
<li>The "person riding bicycle" detector's AP was <strong>0.669</strong>, beating the baseline's 0.188 by a large margin.</li>
<li>Including visual phrase detectors in their decoding algorithm also <strong>improved the detection of individual objects</strong>. For example, the AP score for detecting a "sofa" increased from 0.204 to <strong>0.260</strong> when the model also considered phrases like "dog lying on sofa".</li>
</ul>
<hr />
<p>Our <strong>contributions</strong> are: 1) Introducing visual phrases as categories for recognition; 2) Introducing a novel dataset for phrasal recognition; 3) Showing that considering visual phrases provides a significant gain over state of the art object detectors coupled with the state of the art methods of modeling interactions; 4) Introducing a decoding algorithm that takes into account specific properties of interacting objects in multiple levels of abstraction; 5) Producing state of the art performance results in multi-class object recognition.</p>
<p>Related Works</p>
<ul>
<li>Object Recognition</li>
<li>Object Interactions</li>
<li>Scene Understanding</li>
<li>Machine Translation</li>
</ul>
<p>max margin structure learning</p>
<p>In this paper, we introduce visual phrases, show significant gains in considering them, introduce the phrasal recognition dataset, and a decoding algorithm that outperforms state of the art methods. Building long enough phrase tables is still a challenge.
The dimensionality of our features grows with the number of categories. However, <em>there is no need to consider all of the categories when we model the interactions</em>. For this reason, one might only consider a fixed number of categories for each bounding box.
We speculate that the relations between attributes and objects, parts and objects, visual phrases and scenes, and objects and visual phrases mirror one another. Future work will investigate systems to decode complete sets of detections covering the semantic spectrum.</p>
<h4 id="papers-2-3-ai-usage-in-scientific-publishing">üìÑ Papers 2 &amp; 3: AI Usage in Scientific Publishing<a class="headerlink" href="#papers-2-3-ai-usage-in-scientific-publishing" title="Permanent link">&para;</a></h4>
<p>These articles discuss how researchers are detecting the increasing use of AI chatbots (like ChatGPT) to write scientific papers and the ethical problems this creates.</p>
<h5 id="main-problem">Main Problem üòü<a class="headerlink" href="#main-problem" title="Permanent link">&para;</a></h5>
<ul>
<li>There's growing evidence that scientists are using AI to write parts of their research papers.</li>
<li>This is a major concern for scientific integrity because AI models can <strong>"hallucinate"</strong>‚Äîthat is, they can make up facts, figures, and even fake citations for research that doesn't exist.</li>
</ul>
<h5 id="research-method-how-ai-use-is-detected">Research Method (How AI use is detected) üîé<a class="headerlink" href="#research-method-how-ai-use-is-detected" title="Permanent link">&para;</a></h5>
<ul>
<li>The primary method is to track the frequency of certain words that AI tends to overuse compared to human writers. These are sometimes called "AI shibboleths".</li>
<li>Key indicator words include <strong>"meticulous," "commendable," "intricate," "pivotal," "realm," "showcasing,"</strong> and <strong>"delve"</strong>.</li>
</ul>
<h5 id="key-facts-numbers_1">Key Facts &amp; Numbers üìà<a class="headerlink" href="#key-facts-numbers_1" title="Permanent link">&para;</a></h5>
<ul>
<li>
<p><strong>How many papers?</strong></p>
<ul>
<li>One study estimated that at least <strong>1%</strong> of all scientific papers published in 2023 (around <strong>60,000</strong> total) showed signs of AI involvement.</li>
<li>Another study from Stanford estimated the number could be as high as <strong>17.5%</strong> in certain fields like computer science.</li>
</ul>
</li>
<li>
<p><strong>Word Usage Spikes:</strong></p>
<ul>
<li>After 2022, the use of "commendable" increased by <strong>83%</strong>, "intricate" by <strong>117%</strong>, and "meticulously" by <strong>137%</strong>.</li>
<li>The word "pivotal" saw its usage jump by nearly <strong>160%</strong>.</li>
<li>Use of the word "delve" in PubMed abstracts skyrocketed by <strong>654%</strong> between 2020 and 2024.</li>
</ul>
<p><strong>Field Differences:</strong> <strong>Computer science and electrical engineering</strong> were the fields with the highest rates of AI-preferred language.</p>
</li>
</ul>
<h5 id="other-important-points">Other Important Points<a class="headerlink" href="#other-important-points" title="Permanent link">&para;</a></h5>
<ul>
<li><strong>Why are scientists doing this?</strong> The intense "publish or perish" culture in academia incentivizes speed and high volume, which AI helps with. It's also used by non-native English speakers to improve their writing.</li>
<li><strong>Ethical Concerns:</strong> Using AI to write papers is considered <strong>scientific misconduct</strong> by some publishers. Experts warn that it threatens the integrity and independence of science.</li>
</ul>
<h4 id="ai-chatbots-have-thoroughly-infiltrated-scientific-publishing">AI Chatbots Have Thoroughly Infiltrated Scientific Publishing<a class="headerlink" href="#ai-chatbots-have-thoroughly-infiltrated-scientific-publishing" title="Permanent link">&para;</a></h4>
<p>identified a few key words and phrases (such as ‚Äúcomplex and multifaceted‚Äù) that tend to appear more often in AI-generated sentences than in typical human writing</p>
<p>AI buzzwords </p>
<p>tracks more than 140 million papers worldwide</p>
<p>At least 60,000 papers‚Äîslightly more than 1 percent of all scientific articles published globally last year‚Äîmay have used an LLM</p>
<p>up to 17.5 percent of recent computer science papers exhibit signs of AI writing</p>
<p>Gray‚Äôs approach</p>
<p>More than mere words</p>
<h4 id="some-scientists-cant-stop-using-ai-to-write-research-papers">Some scientists can't stop using AI to write research papers<a class="headerlink" href="#some-scientists-cant-stop-using-ai-to-write-research-papers" title="Permanent link">&para;</a></h4>
<p>One study, published in March by Andrew Gray of University College London in the UK, suggests at least one percent ‚Äì 60,000 or more ‚Äì of all papers published in 2023 were written at least partially by AI</p>
<p>A second paper published in April by a Stanford University team in the US claims this figure might range between 6.3 and 17.5 percent, depending on the topic.</p>
<p>Both papers looked for certain words that large language models (LLMs) use habitually, such as ‚Äúintricate,‚Äù ‚Äúpivotal,‚Äù and ‚Äúmeticulously."</p>
<p>The same was true of other certain adjectives and adverbs until 2023 (termed the post-LLM year by Gray)</p>
<p>In that year use of the words "meticulous," "commendable," and "intricate," rose by 59, 83, and 117 percent respectively, while their prevalence in scientific literature hardly changed between 2019 and 2022. The word with the single biggest increase in prevalence post-2022 was ‚Äúmeticulously‚Äù, up 137 percent.</p>
<p>The Stanford bods also noted that authors posting more preprints, working in more crowded fields, and writing shorter papers seem to use AI more frequently. Their paper suggests that a general lack of time and a need to write as much as possible encourages the use of LLMs, which can help increase output.</p>
<p>The Stanford researchers also raised similar concerns, writing that use of generative AI in scientific literature could create "risks to the security and independence of scientific practice."</p>
<h3 id="quiz-5">Quiz 5<a class="headerlink" href="#quiz-5" title="Permanent link">&para;</a></h3>
<h4 id="man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings">üìÑ"Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings"<a class="headerlink" href="#man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings" title="Permanent link">&para;</a></h4>
<p>This paper demonstrates that standard word embeddings (vector representations of words) trained on large text corpora, such as Google News, contain and can even amplify human-like gender biases. ÊîæÂ§ßÊÄßÂà´ÂÅèËßÅ</p>
<ul>
<li><strong>Core Problem:</strong> The geometry of word embeddings captures gender stereotypes. For instance, while the model correctly solves the analogy "man is to king as woman is to queen," it also produces biased analogies like "man is to computer programmer as woman is to homemaker" and "father is to doctor as mother is to nurse".</li>
<li><strong>Key Finding:</strong> The paper finds that gender bias is not random but is captured by a specific <em>direction</em> (a subspace) within the high-dimensional vector space. This "<strong>gender direction</strong>" can be identified by taking the first principal component of the difference vectors from several gender-defining word pairs (e.g., <span class="arithmatex">\(\vec{she} - \vec{he}\)</span>, <span class="arithmatex">\(\vec{woman} - \vec{man}\)</span>).</li>
<li><strong>Proposed Solution:</strong> The authors present an algorithm to "debias" the embeddings.<ol>
<li><strong>Neutralize:</strong> For gender-neutral words (like <code>doctor</code> or <code>receptionist</code>), the algorithm removes their projection onto the gender direction, making them equidistant from both male and female terms. ‰∏≠ÊÄßÂåñ‰∏≠ÊÄßËØç</li>
<li><strong>Equalize:</strong> For gender-specific words (like <code>grandmother</code> and <code>grandfather</code>), the algorithm ensures they are equidistant to all neutral words (e.g., <code>babysit</code> is no longer closer to <code>grandmother</code> than <code>grandfather</code>), while preserving their definitional gender component. ÂùáË°°ÂåñÂ∏¶ÊúâÊÄßÂà´ÊåáÂêëÁöÑËØçËØ≠</li>
</ol>
</li>
</ul>
<h4 id="important-facts-statistics">üìä Important Facts &amp; Statistics:<a class="headerlink" href="#important-facts-statistics" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Bias in Occupations:</strong> When projecting occupation words onto the "she-he" gender direction, the most "she" associated words included: <strong>homemaker, nurse, receptionist, librarian, and socialite</strong>. The most "he" associated words included: <strong>maestro, skipper, protege, philosopher, and captain</strong>.</li>
<li><strong>Stereotypical Analogies:</strong> In an evaluation of analogies automatically generated from the original <code>she-he</code> vector, crowd-workers judged <strong>19%</strong> of the top 150 analogies to be gender-stereotypical.</li>
<li><strong>Debiasing Efficacy ÂéªÂÅèÂÄöÊïàÊûú:</strong> After applying the paper's <strong>hard debiasing</strong> algorithm, the percentage of stereotypical analogies generated dropped from <strong>19% to 6%</strong>.</li>
<li><strong>Example of Debiasing:</strong> In the analogy puzzle "he is to doctor as she is to X," the original embedding returned <code>X = nurse</code>. The hard-debiased embedding returned <code>X = physician</code>.</li>
<li><strong>Preserving Utility:</strong> The debiasing algorithm successfully reduced bias <em>without</em> degrading the embedding's performance on standard evaluation benchmarks (like word similarity and analogy solving), showing that utility was preserved.</li>
</ul>
<h4 id="summary-2-context-encoders-feature-learning-by-inpainting">üìÑ Summary 2: "Context Encoders: Feature Learning by Inpainting"<a class="headerlink" href="#summary-2-context-encoders-feature-learning-by-inpainting" title="Permanent link">&para;</a></h4>
<p>This paper proposes an unsupervised method for learning visual features. The model, called a "Context Encoder," is a convolutional neural network (CNN) trained to perform image inpainting‚Äîthat is, to <em>generate the content of a missing image region based only on its surrounding context</em>.</p>
<ul>
<li><strong>Core Problem:</strong> How to train a deep neural network to understand images without using millions of human-provided labels (i.e., unsupervised learning).</li>
<li><strong>Key Finding:</strong> By training a network on the "pretext task" of inpainting, the model is forced to learn a semantic understanding of the visual world. To successfully fill in a missing window on a building, for example, the network must first understand the concept of "building" and "window" from the context. This is analogous to how <code>word2vec</code> learns word meanings by predicting a word from its context.</li>
<li><strong>Proposed Solution:</strong> The model uses an encoder-decoder architecture.<ol>
<li><strong>Encoder:</strong> A CNN that processes the image with the missing region and creates a compact feature representation of the context.</li>
<li><strong>Decoder:</strong> Uses this feature representation to generate the pixels for the missing patch.</li>
</ol>
</li>
<li><strong>Key Technique (Joint Loss):</strong> The paper finds that a simple pixel-wise (L2) loss function produces blurry, "averaged" images. To fix this, they add an <strong>adversarial loss</strong> (from Generative Adversarial Networks, or GANs). This second loss function trains a "discriminator" to judge if the generated patch looks real or fake, forcing the encoder to produce sharp, plausible images. The final model is trained with a joint loss (<span class="arithmatex">\(\lambda_{rec}\mathcal{L}_{rec} + \lambda_{adv}\mathcal{L}_{adv}\)</span>).</li>
</ul>
<h4 id="important-facts-statistics_1">üìä Important Facts &amp; Statistics:<a class="headerlink" href="#important-facts-statistics_1" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Inpainting Quality (Table 1):</strong> The "Context Encoder" model was quantitatively evaluated for its inpainting ability on the Paris StreetView dataset.<ul>
<li>The model trained with the <strong>joint (L2 + Adversarial) loss</strong> achieved a <strong>Mean L2 Loss of 2.35%</strong>.</li>
<li>This was significantly better than traditional nearest-neighbor inpainting using HOG features (Mean L2 Loss of 6.92%).</li>
</ul>
</li>
<li><strong>Masking Strategy:</strong> The features learned were <em>not</em> general when the <em>same</em> central region was masked every time. The best-performing models were trained using <strong>random block</strong> or <strong>arbitrary random region</strong> masks, which prevented the network from "cheating" by just learning the mask boundary.</li>
<li><strong>Feature Learning (Table 2):</strong> The primary goal was to learn features for other tasks. The authors pre-trained their encoder on unlabeled images and then fine-tuned it for classification, detection, and segmentation.<ul>
<li><strong>Classification:</strong> On PASCAL VOC 2007, the Context Encoder pre-training achieved <strong>56.5% mAP</strong>, significantly outperforming training from scratch ("Random Gaussian," 53.3%).</li>
<li><strong>Detection:</strong> On PASCAL VOC 2007, the model achieved <strong>44.5% mAP</strong> (vs. 43.4% from scratch).</li>
<li><strong>Segmentation:</strong> On PASCAL VOC 2012, the model achieved <strong>29.7% mean IoU</strong>, a large improvement over training from scratch (19.8%) and a standard Autoencoder (25.2%).</li>
</ul>
</li>
</ul>
<h3 id="quiz-6">Quiz 6<a class="headerlink" href="#quiz-6" title="Permanent link">&para;</a></h3>
<h4 id="state-of-what-art-a-call-for-multi-prompt-llm-evaluation">State of What Art? A Call for Multi-Prompt LLM Evaluation<a class="headerlink" href="#state-of-what-art-a-call-for-multi-prompt-llm-evaluation" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th><strong>Category üß†</strong></th>
<th><strong>Key Facts and Statistics üìä</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Scale of Study</strong></td>
<td>The evaluation analyzed over <strong>6.5 million</strong> instances. It involved <strong>20</strong> different LLMs and <strong>39 tasks</strong> across <strong>three benchmarks</strong> (LMENTRY, BIG-bench Lite (BBL), and BIG-bench Hard (BBH)).</td>
</tr>
<tr>
<td><strong>Performance Variation</strong></td>
<td>The performance and relative ranking of models <strong>vary widely</strong> with prompt phrasing.</td>
</tr>
<tr>
<td><strong>Ranking Instability</strong></td>
<td><strong>Kendall's W</strong> (measuring ranking correlation across prompts) was generally <strong>smaller than 0.85</strong> for most tasks, indicating <strong>weak to moderate agreement</strong> in model rankings.</td>
</tr>
<tr>
<td><strong>Divergence</strong></td>
<td>The performance on the original, single instruction often <strong>deviated by more than one standard deviation</strong> from the average performance across all paraphrases (a phenomenon called 'divergence').</td>
</tr>
<tr>
<td><strong>Minimal Edits</strong></td>
<td>Even <strong>minor phrasing changes</strong> (e.g., substituting 'excludes' with 'lacks') resulted in large and often opposite performance changes across different models.</td>
</tr>
<tr>
<td><strong>Proposed Metrics</strong></td>
<td>The paper proposes distinct metrics for different use cases:</td>
</tr>
<tr>
<td><strong>MaxP (Maximum Performance)</strong></td>
<td>The maximum score a model achieves across all instruction templates (<span class="arithmatex">\(\max_{i\in I_{T}}\epsilon(M,T,i)\)</span>). This is useful for <strong>downstream application developers</strong> looking to find the single best prompt.</td>
</tr>
<tr>
<td><strong>AvgP (Average Performance)</strong></td>
<td>The mean score across all templates. This is useful for <strong>LLM developers</strong> to measure <strong>robustness</strong> to instruction paraphrasing.</td>
</tr>
<tr>
<td><strong>CPS (Combined Performance Score)</strong></td>
<td><span class="arithmatex">\(CPS = Sat \cdot MaxP\)</span>. Combines MaxP and Saturation (<span class="arithmatex">\(Sat = 1 - (MaxP - AvgP)\)</span>) to measure both <strong>peak capability and robustness</strong>. Useful for multi-functional systems.</td>
</tr>
</tbody>
</table>
<h4 id="scenegen-learning-to-generate-realistic-traffic-scenes">SceneGen: Learning to Generate Realistic Traffic Scenes<a class="headerlink" href="#scenegen-learning-to-generate-realistic-traffic-scenes" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th><strong>Category üõ†Ô∏è</strong></th>
<th><strong>Key Facts and Methodology üí°</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Core Idea</strong></td>
<td>SceneGen is a neural <strong>autoregressive model</strong> that <strong>eschews hand-crafted rules and heuristics</strong>. Previous procedural methods were limited by requiring specific heuristics (e.g., 'vehicles should drive along lane centerlines'), which creates a <strong>sim2real content gap</strong>.</td>
</tr>
<tr>
<td><strong>Generation Process</strong></td>
<td>The model sequentially inserts actors (vehicles, pedestrians, and bicyclists) one at a time. It is conditioned on the <strong>ego-vehicle state</strong> and an <strong>High Definition map (HD map)</strong>.</td>
</tr>
<tr>
<td><strong>Model Architecture</strong></td>
<td>Uses a <strong>Recurrent Neural Network</strong> based on the <strong>ConvLSTM</strong> architecture to capture long-range dependencies.</td>
</tr>
<tr>
<td><strong>Actor Parameterization</strong></td>
<td>Each actor <span class="arithmatex">\(a_i\)</span> is parameterized by: <strong>class label</strong> (<span class="arithmatex">\(c_i\)</span>), <strong>location</strong> (<span class="arithmatex">\(x_i, y_i\)</span>), <strong>bounding box</strong> (<span class="arithmatex">\(b_i\)</span>, including size and heading <span class="arithmatex">\(\theta_i\)</span>), and <strong>velocity</strong> (<span class="arithmatex">\(v_i\)</span>).</td>
</tr>
<tr>
<td><strong>Key Distributions</strong></td>
<td>The model uses sophisticated probabilistic distributions to capture real-world multi-modality: <strong>Categorical</strong> (for class/quantized location), <strong>Mixture of Log-Normal</strong> (for bounding box size), and <strong>Mixture of Von-Mises</strong> (for heading angle and velocity direction).</td>
</tr>
<tr>
<td><strong>Evaluation</strong></td>
<td>Evaluated on two large-scale datasets: <strong>ATG4D</strong> and <strong>Argoverse</strong>. Metrics include <strong>Negative Log-Likelihood (NLL)</strong> and <strong>Maximum Mean Discrepancy (MMD)</strong>.</td>
</tr>
<tr>
<td><strong>Key Result</strong></td>
<td>SceneGen achieved the <strong>best (lowest) NLL and MMD</strong> compared to baselines (e.g., MetaSim, Layout VAE). It also showed the <strong>lowest sim2real gap</strong> when training a 3D object detector on its synthetic data.</td>
</tr>
</tbody>
</table>
<h3 id="final">Final<a class="headerlink" href="#final" title="Permanent link">&para;</a></h3>
<h4 id="the-orca-benchmark-evaluating-real-world-calculation-accuracy-in-large-language-models">The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models<a class="headerlink" href="#the-orca-benchmark-evaluating-real-world-calculation-accuracy-in-large-language-models" title="Permanent link">&para;</a></h4>
<p>This paper introduces ORCA, a <strong>benchmark</strong> designed to test how well LLMs perform real-world, everyday quantitative reasoning tasks compared to verified calculator engines.</p>
<ul>
<li><strong>The Problem:</strong> Previous benchmarks (like GSM8K or MATH) focus on academic or competition-style math. ORCA focuses on practical queries users actually ask, such as finance, health, and physics calculations.</li>
<li><strong>The Setup:</strong><ul>
<li><strong>Dataset:</strong> 500 natural-language prompts across 13 domains (e.g., Finance, Construction, Everyday Life).</li>
<li><strong>Verification:</strong> LLM answers were compared against deterministic, verified outputs from Omni Calculator engines.</li>
<li><strong>Models Tested:</strong> ChatGPT-5, Gemini 2.5 Flash, Claude 4.5 Sonnet, Grok 4, and DeepSeek V3.2.</li>
</ul>
</li>
<li><strong>Key Results:</strong><ul>
<li><strong>Overall Accuracy:</strong> The models achieved between <strong>45% and 63%</strong> accuracy. Gemini 2.5 Flash (63.0%) and Grok 4 (62.8%) were the top performers.</li>
<li><strong>Common Errors:</strong> The vast majority of failures were <strong>Precision/Rounding issues (34.7%)</strong> and <strong>Calculation errors (33.4%)</strong>.</li>
<li><strong>Domain Variation:</strong> Models performed best in <em>Statistics &amp; Probability</em> and <em>Math &amp; Conversions</em> but struggled significantly in <em>Physics</em> and <em>Biology/Chemistry</em>.</li>
</ul>
</li>
</ul>
<h4 id="faster-sorting-algorithms-discovered-using-deep-reinforcement-learning">Faster sorting algorithms discovered using deep reinforcement learning<a class="headerlink" href="#faster-sorting-algorithms-discovered-using-deep-reinforcement-learning" title="Permanent link">&para;</a></h4>
<p>This paper presents AlphaDev, a Deep Reinforcement Learning (DRL) agent that discovered new, more efficient sorting algorithms by treating algorithm generation as a single-player game. It leverages MCTS as a policy improvement operator.</p>
<ul>
<li><strong>The Method (AssemblyGame):</strong><ul>
<li>AlphaDev plays a game where it selects low-level CPU assembly instructions to build an algorithm.</li>
<li>It uses an extension of <strong>AlphaZero</strong> Ê∑±Â∫¶Âº∫ÂåñÂ≠¶‰π†Êô∫ËÉΩ‰Ωì (combining Monte Carlo Tree Search with neural networks) to explore the vast combinatorial space of possible instructions.<ul>
<li><em>c</em>‚Äâ=‚Äâcorrectness‚Äâ+‚Äâ<em>Œ±</em>‚Äâ√ó‚Äâperformance</li>
</ul>
</li>
<li>The reward signal is based on both <strong>correctness</strong> (does it sort the list?) and <strong>latency</strong> (how fast is it?).</li>
</ul>
</li>
<li><strong>Key Discoveries:</strong><ul>
<li><strong>Fixed Sorts:</strong> AlphaDev found algorithms for sorting short sequences (Sort 3, Sort 4, Sort 5) that are shorter and faster than the previous state-of-the-art human benchmarks.</li>
<li><strong>Novel Moves:</strong> It discovered specific sequences of instructions, dubbed the <strong>AlphaDev swap move</strong> and <strong>AlphaDev copy move</strong>, which reduce instruction counts.</li>
<li><strong>Real-World Impact:</strong> These new algorithms were reverse-engineered into C++ and integrated into the <strong>LLVM standard C++ library</strong>, replacing sub-routines that hadn't changed in over a decade.<ul>
<li>For sort 3, sort 4 and sort 5 to C++ and discovered that our sort implementations led to improvements of up to 70% for sequences of a length of five and roughly 1.7% for sequences exceeding 250,000 elements. </li>
</ul>
</li>
<li>There are numerous approaches to optimizing assembly programs, which we have classified into three groups: <strong>enumerative search, stochastic search and symbolic search</strong> Êûö‰∏æÊêúÁ¥¢„ÄÅÈöèÊú∫ÊêúÁ¥¢ÂíåÁ¨¶Âè∑ÊêúÁ¥¢</li>
</ul>
</li>
</ul>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="ÊúÄÂêéÊõ¥Êñ∞">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-datetime" title="2026Âπ¥2Êúà20Êó• 01:15:29 CST">2026Âπ¥2Êúà20Êó• 01:15:29</span>
  </span>

    
    
    
    
  </aside>




<div id="__comments">
  
    <h2 ><!-- ËØÑËÆ∫ -->ËØÑËÆ∫Âå∫~</h2>
    
    ÊúâÁî®ÁöÑËØùËØ∑ÁªôÊàë‰∏™ËµûÂíå star => <a href="https://cindyzhou2003.github.io/mymkdocs/">
        <img alt="GitHub stars" src="https://img.shields.io/github/stars/CindyZhou2003/mymkdocs.svg?style=plastic&amp;label=Stars"></a>
    
    
    </div>
    <script src="https://giscus.app/client.js"
        data-repo="CindyZhou2003/mymkdocs"
        data-repo-id="R_kgDOJHRt5g"
        data-category="General"
        data-category-id="DIC_kwDOJHRt5s4Cge0r"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="light"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>

        updateScheme();

    </script>
    
    
    <!-- Synchronize Giscus theme with palette -->
    <script>
    var giscus = document.querySelector("script[src*=giscus]")
    
    /* Set palette on initial load */
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate" ? "dark" : "light"
        giscus.setAttribute("data-theme", theme) 
    }
    
    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function() {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
            var theme = palette.color.scheme === "slate" ? "dark" : "light"
            localStorage.setItem("data-md-color-scheme", palette.color.scheme === "slate" ? "slate" : "default");
            /* Instruct Giscus to change theme */
            var frame = document.querySelector(".giscus-frame")
            frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
            )
        }
        })
    })
    </script>
                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  ÂõûÂà∞È°µÈù¢È°∂ÈÉ®
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="È°µËÑö" >
        
          
          <a href="ECE408.html" class="md-footer__link md-footer__link--prev" aria-label="‰∏ä‰∏ÄÈ°µ: ECE408/CS483 Applied Parallel Programming">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                ‰∏ä‰∏ÄÈ°µ
              </span>
              <div class="md-ellipsis">
                ECE408/CS483 Applied Parallel Programming
              </div>
            </div>
          </a>
        
        
          
          <a href="ECE598.html" class="md-footer__link md-footer__link--next" aria-label="‰∏ã‰∏ÄÈ°µ: ECE498/598 Deep Generative Models">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                ‰∏ã‰∏ÄÈ°µ
              </span>
              <div class="md-ellipsis">
                ECE498/598 Deep Generative Models
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024-2025 <a href="https://github.com/CindyZhou2003/mymkdocs"  target="_blank" rel="noopener">Cindy</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/CindyZhou2003/mymkdocs" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": ["content.code.annotate", "navigation.indexes", "navigation.expand", "toc.follow", "navigation.top", "search.suggest", "navigation.footer", "navigation.tabs", "navigation.tracking", "navigation.instant.progress", "content.code.copy", "search.highlight", "search.share", "content.code.annotate", "navigation.tabs"], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="https://gcore.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
      
        <script src="../js/katex.js"></script>
      
        <script src="../js/tablesort.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>